# ==============================================================
# Experiment 020 Stage 2: 증강 OFF + 극저 LR로 bbox 미세조정
# ==============================================================
# 목적: 2단계 학습의 2단계 — bbox 정밀도 극대화
# 전략: 1단계에서 학습된 모델을 로드하고,
#       모든 증강을 끈 상태에서 극저 학습률로 bbox만 미세조정
#       → mAP@0.75:0.95 점수 극대화
#
# ⚠️ 반드시 1단계(exp020_stage1) 학습 완료 후 실행!
#
# 실행:
#   # 데이터 준비
#   python scripts/1_create_coco_format.py --run-name exp020_s2
#   python scripts/0_splitting.py --run-name exp020_s2
#   python scripts/2_prepare_yolo_dataset.py --run-name exp020_s2
#
#   # 2단계 학습 (1단계 best.pt를 --ckpt-from으로 로드)
#   python scripts/3_train.py --run-name exp020_s2 --ckpt-from runs/exp020_s1/checkpoints/best.pt --config configs/experiments/exp020_stage2.yaml
#
#   # 평가 + 제출
#   python scripts/4_evaluate.py --run-name exp020_s2 --config configs/experiments/exp020_stage2.yaml
#   python scripts/5_submission.py --run-name exp020_s2 --config configs/experiments/exp020_stage2.yaml --conf 0.20
#
#   # TTA 적용 제출 (추가 성능 향상)
#   python scripts/5_submission.py --run-name exp020_s2 --config configs/experiments/exp020_stage2.yaml --conf 0.20 --tta
# ==============================================================

_base_: "../base.yaml"

experiment:
  id: "exp020_s2"
  name: "yolo11s_1024_stage2_finetune"
  description: "2단계 학습 - 2단계: 증강 OFF + 극저LR로 bbox 미세조정"
  author: "@DM"
  created: "2026-02-06"

# --- base.yaml에서 변경하는 값만 ---

train:
  model_name: "yolo11s"   # yolov8s → yolo11s
  imgsz: 1024             # 768 → 1024
  epochs: 60              # 미세조정이므로 적은 에폭
  batch: 4
  optimizer: "AdamW"      # AdamW (안정적 fine-tuning)
  lr0: 0.00005            # 극저 학습률 (5e-5)
  lrf: 0.1
  weight_decay: 0.001
  warmup_epochs: 0        # 이미 학습된 모델이므로 warmup 불필요
  patience: 20            # 빠른 early stopping

  # 모든 증강 OFF (bbox 정밀도에 집중)
  mosaic: 0.0
  mixup: 0.0
  copy_paste: 0.0
  hsv_h: 0.0
  hsv_s: 0.0
  hsv_v: 0.0
  degrees: 0.0
  translate: 0.0
  scale: 0.0
  fliplr: 0.0
  flipud: 0.0

  # bbox 정밀도 강화 (Loss 가중치 조정)
  box: 10.0               # 7.5 → 10.0 (bbox 위치 정확도 중시)
  dfl: 2.0                # 1.5 → 2.0  (distribution focal loss 강화)

infer:
  conf_thr: 0.20          # 낮은 conf (mAP는 PR 곡선 전체 사용)

notes: |
  2단계 학습 전략의 2단계 (핵심!).
  증강을 모두 끄고 극저 학습률로 bbox 정밀도만 미세조정.

  왜 증강 OFF?
  - 증강은 학습 다양성에 좋지만, bbox를 왜곡시킴
  - 증강 없이 원본 이미지로 학습하면 bbox가 더 정확해짐
  - mAP@0.75:0.95는 tight bbox가 점수에 직결

  왜 box=10.0, dfl=2.0?
  - 분류(cls)보다 위치(box, dfl) 정확도가 점수에 중요
  - box/dfl 가중치를 높여 bbox regression에 집중
