{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f82a806",
   "metadata": {},
   "source": [
    "# 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899c87fc",
   "metadata": {},
   "source": [
    "## 목차(상세)\n",
    "\n",
    "### 0. 문서 개요\n",
    "\n",
    "0.1 프로젝트 목표와 성공 기준\n",
    "\n",
    "0.2 범위(학습/추론/제출)와 제외 범위\n",
    "\n",
    "0.3 변경 이력(실험/규칙 변경 기록)\n",
    "\n",
    "---\n",
    "\n",
    "### 1. 대회 이해와 설계 기준\n",
    "\n",
    "1.1 문제 정의: 이미지당 최대 4개 객체 검출\n",
    "\n",
    "1.2 평가 지표 해설: **mAP@[0.75:0.95]** 가 요구하는 “박스 정밀도”\n",
    "\n",
    "1.3 데이터 특성 요약: PNG + COCO(JSON), 이미지/라벨 구조\n",
    "\n",
    "1.4 클래스 구성 리스크: **Train에만 존재하는 클래스**가 있는 이유와 대응 전략\n",
    "\n",
    "1.5 제출 규격 요약: CSV 컬럼/형식, row=객체 1개 규칙, image_id 규칙\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 프로젝트 구조와 재현성 규칙\n",
    "\n",
    "2.1 디렉터리 구조(데이터/실험/모델/로그/제출물)\n",
    "\n",
    "2.2 환경 고정(패키지 버전, GPU/Seed, 실행 커맨드)\n",
    "\n",
    "2.3 실험 관리 규칙(RUN_NAME, config 저장, 체크포인트 정책)\n",
    "\n",
    "2.4 공통 로깅 항목(성능표, 하이퍼파라미터, 데이터 버전, 후처리 값)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 데이터 준비 파이프라인\n",
    "\n",
    "3.1 원천 파일 구조: train_images / train_annotations / test_images\n",
    "\n",
    "3.2 COCO JSON 핵심 필드 정리(images / annotations / categories)\n",
    "\n",
    "3.3 **QC Gate 1: 라벨 존재/유효성 검사**\n",
    "\n",
    "* bbox 키 존재 여부, 길이=4 여부\n",
    "* 음수/0 너비·높이, 이미지 경계 밖 bbox\n",
    "* area 분포 이상치 탐지\n",
    "  3.4 **QC Gate 2: 데이터 무결성 검사**\n",
    "* image_id ↔ file_name 매핑 일관성\n",
    "* 중복/누락 이미지 체크\n",
    "  3.5 데이터 EDA 리포트(필수 산출물)\n",
    "* 클래스 빈도, 이미지당 객체 수(1~4)\n",
    "* bbox 크기/종횡비/면적 분포(“작은 알약” 비중 확인)\n",
    "  3.6 클래스 매핑 정책\n",
    "* categories 정리, label map 고정\n",
    "* train-only 클래스 처리 원칙(억제/가중치/필터링 등)\n",
    "  3.7 스플릿 전략(안정성 중심)\n",
    "* 기본: seed 고정 + 이미지당 객체수 stratify\n",
    "* 고급: 멀티라벨 stratify 또는 K-fold(클래스 누락 방지)\n",
    "  3.8 학습 포맷 변환 및 검증\n",
    "* COCO → 학습 포맷 변환(예: YOLO 포맷)\n",
    "* 변환 검증: 샘플 시각화로 bbox가 정확히 올라가는지 확인\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 베이스라인 모델 설계\n",
    "\n",
    "4.1 후보 모델군 및 선택 기준(속도/정확도/작은 객체)\n",
    "\n",
    "4.2 입력 해상도 전략(해상도 스윕 계획 포함)\n",
    "\n",
    "4.3 기본 학습 레시피(epochs, batch, optimizer, lr schedule)\n",
    "\n",
    "4.4 증강 설계(작은 객체/겹침 상황 고려)\n",
    "\n",
    "* geometric / color / blur / cutout\n",
    "* mosaic/mixup 사용 여부와 리스크\n",
    "  4.5 체크포인트/early stopping/EMA 등 안정화 옵션\n",
    "\n",
    "---\n",
    "\n",
    "### 5. 학습 실험 계획(점수에 직결되는 순서로)\n",
    "\n",
    "5.1 실험 우선순위(가성비 트랙 → 고성능 트랙)\n",
    "\n",
    "5.2 해상도 스윕: 640 → 768 → 960(예시)\n",
    "\n",
    "5.3 증강 아블레이션(하나씩 추가/제거하며 영향 확인)\n",
    "\n",
    "5.4 하이퍼파라미터 미세 조정\n",
    "\n",
    "* lr, weight decay, warmup, augmentation strength\n",
    "  5.5 train-only 클래스 리스크 완화 실험\n",
    "* 클래스별 threshold/억제 규칙 설계\n",
    "* “오답 많이 내는 클래스” 방어 전략\n",
    "  5.6 실험 결과 기록 템플릿(표 형태로 고정)\n",
    "\n",
    "---\n",
    "\n",
    "### 6. 평가 프로토콜 & 박스 품질 진단(필수)\n",
    "\n",
    "6.1 공통 평가 규칙(동일 val, 동일 seed, 동일 후처리 기본값)\n",
    "\n",
    "6.2 **IoU 구간별 성능 분해**\n",
    "\n",
    "* mAP@0.75 / 0.85 / 0.95 관찰 포인트\n",
    "  6.3 **BBox 품질 리포트**\n",
    "* bbox 크기 bucket별(작은/중간/큰) 성능\n",
    "* 겹침/밀집 장면에서의 실패 유형\n",
    "  6.4 에러 분석 체계\n",
    "* FP/FN 분류(클래스 혼동, 위치 오차, 중복 검출, 누락)\n",
    "* worst-k 샘플 자동 추출 + 시각화\n",
    "\n",
    "---\n",
    "\n",
    "### 7. 추론 & 후처리(Top-4 제약 대응 핵심 파트)\n",
    "\n",
    "7.1 기본 추론 파이프라인(모델 로드, 이미지 전처리)\n",
    "\n",
    "7.2 NMS 전략 비교(hard NMS / soft NMS / WBF 선택지)\n",
    "\n",
    "7.3 **Top-4 제약 적용 로직**\n",
    "\n",
    "* 이미지별 score 정렬 → 최대 4개만 남기기\n",
    "* 중복/근접 박스 정리 규칙\n",
    "  7.4 클래스별 threshold / score calibration\n",
    "* “리스크 클래스” 억제 규칙(오답 방지)\n",
    "  7.5 좌표 후처리\n",
    "* 이미지 경계 clipping\n",
    "* bbox_w/h 0 방지, 타입/반올림 규칙\n",
    "\n",
    "---\n",
    "\n",
    "### 8. 제출 파일 생성 & 검증(실수 방지 게이트)\n",
    "\n",
    "8.1 제출 CSV 스펙(컬럼/자료형/의미)\n",
    "\n",
    "8.2 annotation_id 생성 규칙(로우 수만큼 고유값 보장)\n",
    "\n",
    "8.3 **Submission Validator(자동 점검)**\n",
    "\n",
    "* 누락 컬럼/NaN/음수 좌표\n",
    "* image_id 형식(파일명 숫자)\n",
    "* bbox 범위/폭높이 유효성\n",
    "  8.4 샘플 제출 시각 검증(랜덤 N장 렌더링)\n",
    "  8.5 최종 제출 체크리스트\n",
    "\n",
    "---\n",
    "\n",
    "### 9. 최종 모델 선택 & 리더보드 운영 전략\n",
    "\n",
    "9.1 선택 기준(성능 + 안정성 + 과적합 징후)\n",
    "\n",
    "9.2 Public/Private 차이를 고려한 판단 규칙\n",
    "\n",
    "9.3 (옵션) 앙상블/TTA 적용 기준과 비용 대비 효과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74cd18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] ROOT      : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\n",
      "[OK] RUN_NAME  : exp_20260202_230604\n",
      "[OK] RUN_DIR   : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\n",
      "[OK] ART_DIR   : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\n",
      "[OK] saved meta: C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\config\\paths_meta.json\n",
      "\n",
      "[INPUT COUNTS]\n",
      "- train_images      : 232 png\n",
      "- train_annotations : 763 json\n",
      "- test_images       : 842 png\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 2-1. 프로젝트 디렉터리/경로 고정 + 존재 여부 점검 + RUN 폴더 생성\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "\n",
    "# 1) Project root (현재 노트북 실행 위치를 project root로 가정)\n",
    "ROOT = Path(\".\").resolve()\n",
    "\n",
    "# 2) Input data paths (이미 존재한다고 한 폴더들)\n",
    "INPUT = {\n",
    "    \"TRAIN_IMAGES\": ROOT / \"train_images\",\n",
    "    \"TRAIN_ANN_DIR\": ROOT / \"train_annotations\",\n",
    "    \"TEST_IMAGES\": ROOT / \"test_images\",\n",
    "}\n",
    "\n",
    "# 3) Working dirs (이미 만들어둔 폴더들)\n",
    "WORK = {\n",
    "    \"DATA\": ROOT / \"data\",\n",
    "    \"RUNS\": ROOT / \"runs\",\n",
    "    \"ARTIFACTS\": ROOT / \"artifacts\",\n",
    "}\n",
    "\n",
    "# 4) 필수 폴더 존재 체크\n",
    "missing = [k for k, p in {**INPUT, **WORK}.items() if not p.exists()]\n",
    "if missing:\n",
    "    raise FileNotFoundError(\n",
    "        \"필수 폴더가 없습니다:\\n\" +\n",
    "        \"\\n\".join([f\"- {k}: {str(({**INPUT, **WORK})[k])}\" for k in missing])\n",
    "    )\n",
    "\n",
    "# 5) RUN 이름 (원하면 아래 RUN_NAME만 바꿔도 전체 경로가 함께 바뀜)\n",
    "RUN_NAME = os.environ.get(\"RUN_NAME\") or f\"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "RUN_DIR = WORK[\"RUNS\"] / RUN_NAME\n",
    "ART_DIR = WORK[\"ARTIFACTS\"] / RUN_NAME\n",
    "\n",
    "# 6) RUN 하위 구조 (실험/로그/체크포인트/제출물/시각화 등)\n",
    "DIRS = {\n",
    "    # run space\n",
    "    \"RUN_DIR\": RUN_DIR,\n",
    "    \"CKPT\": RUN_DIR / \"checkpoints\",\n",
    "    \"LOGS\": RUN_DIR / \"logs\",\n",
    "    \"CONFIG\": RUN_DIR / \"config\",\n",
    "    # artifacts space\n",
    "    \"ART_DIR\": ART_DIR,\n",
    "    \"SUBMISSIONS\": ART_DIR / \"submissions\",\n",
    "    \"PLOTS\": ART_DIR / \"plots\",\n",
    "    \"REPORTS\": ART_DIR / \"reports\",\n",
    "    # cached/processed data (optional)\n",
    "    \"CACHE\": WORK[\"DATA\"] / \"cache\" / RUN_NAME,\n",
    "}\n",
    "\n",
    "for p in DIRS.values():\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 7) 실행 설정 스냅샷 저장 (재현성)\n",
    "meta = {\n",
    "    \"run_name\": RUN_NAME,\n",
    "    \"root\": str(ROOT),\n",
    "    \"input\": {k: str(v) for k, v in INPUT.items()},\n",
    "    \"work\": {k: str(v) for k, v in WORK.items()},\n",
    "    \"dirs\": {k: str(v) for k, v in DIRS.items()},\n",
    "}\n",
    "\n",
    "meta_path = DIRS[\"CONFIG\"] / \"paths_meta.json\"\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# 8) 간단 출력\n",
    "print(f\"[OK] ROOT      : {ROOT}\")\n",
    "print(f\"[OK] RUN_NAME  : {RUN_NAME}\")\n",
    "print(f\"[OK] RUN_DIR   : {RUN_DIR}\")\n",
    "print(f\"[OK] ART_DIR   : {ART_DIR}\")\n",
    "print(f\"[OK] saved meta: {meta_path}\")\n",
    "\n",
    "# 9) 입력 폴더 파일 개수 확인 (대략적인 sanity check)\n",
    "def count_files(folder: Path, exts=None):\n",
    "    if exts is None:\n",
    "        return sum(1 for _ in folder.rglob(\"*\") if _.is_file())\n",
    "    exts = set(e.lower() for e in exts)\n",
    "    return sum(1 for _ in folder.rglob(\"*\") if _.is_file() and _.suffix.lower() in exts)\n",
    "\n",
    "print(\"\\n[INPUT COUNTS]\")\n",
    "print(f\"- train_images      : {count_files(INPUT['TRAIN_IMAGES'], exts=['.png'])} png\")\n",
    "print(f\"- train_annotations : {count_files(INPUT['TRAIN_ANN_DIR'], exts=['.json'])} json\")\n",
    "print(f\"- test_images       : {count_files(INPUT['TEST_IMAGES'], exts=['.png'])} png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdaf580d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] SEED=42, DETERMINISTIC=True\n",
      "[OK] saved env meta: C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\config\\env_meta.json\n",
      "\n",
      "[VERSIONS]\n",
      "- numpy: 2.4.2\n",
      "- pandas: 3.0.0\n",
      "- opencv-python: 4.13.0.90\n",
      "- albumentations: 2.0.8\n",
      "- pycocotools: None\n",
      "- torch: 2.5.1+cu121\n",
      "- torchvision: 0.20.1+cu121\n",
      "- ultralytics: 8.4.9\n",
      "\n",
      "[TORCH]\n",
      "- torch_version: 2.5.1+cu121\n",
      "- cuda_available: True\n",
      "- cuda_version: 12.1\n",
      "- cudnn_version: 90100\n",
      "- device_count: 1\n",
      "- device_name: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 2-2. 재현성 설정(Seed 고정) + 실행 환경/버전 스냅샷 저장\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# 1) Seed / Determinism 설정\n",
    "SEED = int(os.environ.get(\"SEED\", \"42\"))\n",
    "DETERMINISTIC = True  # 필요 시 False로 바꿔도 됨\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "# (CUDA/torch에서 determinism 강제 시 일부 연산에서 성능 저하/에러 가능)\n",
    "# cublas determinism(일부 환경에서만 유효)\n",
    "os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":4096:8\")\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "# numpy seed\n",
    "try:\n",
    "    import numpy as np\n",
    "    np.random.seed(SEED)\n",
    "except Exception as e:\n",
    "    np = None\n",
    "    print(f\"[WARN] numpy seed skip: {e}\")\n",
    "\n",
    "# torch seed + cudnn\n",
    "torch_info = {}\n",
    "try:\n",
    "    import torch\n",
    "\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    if DETERMINISTIC:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        # strict determinism (연산에 따라 에러가 날 수 있어 선택)\n",
    "        # torch.use_deterministic_algorithms(True)\n",
    "    else:\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    torch_info = {\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "        \"cuda_version\": getattr(torch.version, \"cuda\", None),\n",
    "        \"cudnn_version\": torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else None,\n",
    "        \"device_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "        \"device_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() and torch.cuda.device_count() > 0 else None,\n",
    "    }\n",
    "except Exception as e:\n",
    "    torch = None\n",
    "    print(f\"[WARN] torch seed skip: {e}\")\n",
    "\n",
    "# 2) 주요 라이브러리 버전 수집(없으면 None)\n",
    "def safe_version(pkg_name: str):\n",
    "    try:\n",
    "        from importlib.metadata import version\n",
    "        return version(pkg_name)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "pkgs = [\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"opencv-python\",\n",
    "    \"albumentations\",\n",
    "    \"pycocotools\",\n",
    "    \"torch\",\n",
    "    \"torchvision\",\n",
    "    \"ultralytics\",\n",
    "    \"timm\",\n",
    "    \"matplotlib\",\n",
    "    \"scikit-learn\",\n",
    "]\n",
    "\n",
    "pkg_versions = {p: safe_version(p) for p in pkgs}\n",
    "\n",
    "# 3) 환경 메타 저장\n",
    "env_meta = {\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"seed\": SEED,\n",
    "    \"deterministic\": DETERMINISTIC,\n",
    "    \"python\": {\n",
    "        \"version\": sys.version,\n",
    "        \"executable\": sys.executable,\n",
    "    },\n",
    "    \"platform\": {\n",
    "        \"system\": platform.system(),\n",
    "        \"release\": platform.release(),\n",
    "        \"version\": platform.version(),\n",
    "        \"machine\": platform.machine(),\n",
    "        \"processor\": platform.processor(),\n",
    "    },\n",
    "    \"packages\": pkg_versions,\n",
    "    \"torch\": torch_info,\n",
    "}\n",
    "\n",
    "env_meta_path = DIRS[\"CONFIG\"] / \"env_meta.json\"\n",
    "with open(env_meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(env_meta, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"[OK] SEED={SEED}, DETERMINISTIC={DETERMINISTIC}\")\n",
    "print(f\"[OK] saved env meta: {env_meta_path}\")\n",
    "\n",
    "# 4) 핵심만 출력\n",
    "print(\"\\n[VERSIONS]\")\n",
    "for k in [\"numpy\", \"pandas\", \"opencv-python\", \"albumentations\", \"pycocotools\", \"torch\", \"torchvision\", \"ultralytics\"]:\n",
    "    if k in pkg_versions:\n",
    "        print(f\"- {k}: {pkg_versions[k]}\")\n",
    "if torch_info:\n",
    "    print(\"\\n[TORCH]\")\n",
    "    for k, v in torch_info.items():\n",
    "        print(f\"- {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ae672d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] CFG_PATH      : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\config\\config.json\n",
      "[OK] MANIFEST_PATH : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\config\\run_manifest.json\n",
      "[OK] METRICS_JSONL  : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\logs\\metrics.jsonl\n",
      "[OK] EVENTS_JSONL   : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\logs\\events.jsonl\n",
      "[OK] REGISTRY       : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\_registry.csv\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 2-3. 실험 관리 규칙(RUN config/manifest) + 로깅/메트릭 저장 유틸\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Small utils\n",
    "# -----------------------------\n",
    "def _json_dump(path: Path, obj: dict):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def _json_load(path: Path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _append_jsonl(path: Path, obj: dict):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def _run_cmd(cmd):\n",
    "    try:\n",
    "        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT).decode(\"utf-8\", errors=\"ignore\").strip()\n",
    "        return out\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Git snapshot (optional)\n",
    "# -----------------------------\n",
    "git_meta = {\n",
    "    \"git_head\": _run_cmd([\"git\", \"rev-parse\", \"HEAD\"]),\n",
    "    \"git_branch\": _run_cmd([\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"]),\n",
    "    \"git_dirty\": _run_cmd([\"git\", \"status\", \"--porcelain\"]),\n",
    "}\n",
    "git_meta[\"git_dirty\"] = bool(git_meta[\"git_dirty\"]) if git_meta[\"git_dirty\"] is not None else None\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Base config (project-level)\n",
    "#    - 이후 셀에서 CFG만 업데이트하며 실험을 반복하기 위한 \"단일 소스\"\n",
    "# -----------------------------\n",
    "CFG_PATH = DIRS[\"CONFIG\"] / \"config.json\"\n",
    "\n",
    "DEFAULT_CFG = {\n",
    "    \"project\": {\n",
    "        \"name\": \"ai07_pill_od\",\n",
    "        \"run_name\": RUN_NAME,\n",
    "        \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    },\n",
    "    \"paths\": {\n",
    "        \"root\": str(ROOT),\n",
    "        \"train_images\": str(INPUT[\"TRAIN_IMAGES\"]),\n",
    "        \"train_ann_dir\": str(INPUT[\"TRAIN_ANN_DIR\"]),\n",
    "        \"test_images\": str(INPUT[\"TEST_IMAGES\"]),\n",
    "        \"run_dir\": str(DIRS[\"RUN_DIR\"]),\n",
    "        \"ckpt_dir\": str(DIRS[\"CKPT\"]),\n",
    "        \"logs_dir\": str(DIRS[\"LOGS\"]),\n",
    "        \"art_dir\": str(DIRS[\"ART_DIR\"]),\n",
    "        \"submissions_dir\": str(DIRS[\"SUBMISSIONS\"]),\n",
    "        \"reports_dir\": str(DIRS[\"REPORTS\"]),\n",
    "        \"plots_dir\": str(DIRS[\"PLOTS\"]),\n",
    "        \"cache_dir\": str(DIRS[\"CACHE\"]),\n",
    "    },\n",
    "    \"reproducibility\": {\n",
    "        \"seed\": SEED,\n",
    "        \"deterministic\": True,\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"format\": \"coco_json_multi\",\n",
    "        \"max_objects_per_image\": 4,\n",
    "        \"num_classes\": None,          # 추후 categories에서 자동 추출/확정\n",
    "        \"class_whitelist\": None,      # test에 존재하는 40개 클래스 id 리스트(추후 설정)\n",
    "    },\n",
    "    \"split\": {\n",
    "        \"strategy\": \"stratify_by_num_objects\",\n",
    "        \"seed\": SEED,\n",
    "        \"ratios\": {\"train\": 0.8, \"valid\": 0.2},\n",
    "        \"kfold\": {\"enabled\": False, \"n_splits\": 5, \"fold_idx\": 0},\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"framework\": \"ultralytics_yolo\",   # 또는 mmrotate/mmdet 등으로 변경 가능\n",
    "        \"model\": {\n",
    "            \"name\": \"yolov8s\",\n",
    "            \"imgsz\": 768,\n",
    "            \"pretrained\": True,\n",
    "        },\n",
    "        \"hyperparams\": {\n",
    "            \"epochs\": 80,\n",
    "            \"batch\": 8,\n",
    "            \"lr0\": None,\n",
    "            \"weight_decay\": None,\n",
    "            \"workers\": 4,\n",
    "        },\n",
    "        \"augment\": {\n",
    "            \"enabled\": True,\n",
    "            \"mosaic\": True,\n",
    "            \"mixup\": False,\n",
    "            \"hsv\": True,\n",
    "            \"flip\": True,\n",
    "        },\n",
    "        \"checkpoint_policy\": {\n",
    "            \"save_best_on\": \"val_map_75_95\",  # 우리가 보는 기준(툴이 제공하는 metric명에 맞춰 조정)\n",
    "            \"save_last\": True,\n",
    "            \"keep_top_k\": 3,\n",
    "        },\n",
    "    },\n",
    "    \"infer\": {\n",
    "        \"conf_thr\": 0.001,      # 후보는 넓게 뽑고 후처리에서 Top-4로 정리\n",
    "        \"nms_iou_thr\": 0.5,\n",
    "        \"max_det_per_image\": 4, # 대회 제약 고정\n",
    "        \"tta\": {\"enabled\": False},\n",
    "    },\n",
    "    \"postprocess\": {\n",
    "        \"strategy\": \"topk_by_score\",\n",
    "        \"topk\": 4,\n",
    "        \"classwise_threshold\": None,  # {class_id: thr}\n",
    "        \"clip_boxes\": True,\n",
    "    },\n",
    "    \"submission\": {\n",
    "        \"columns\": [\"annotation_id\", \"image_id\", \"category_id\", \"bbox_x\", \"bbox_y\", \"bbox_w\", \"bbox_h\", \"score\"],\n",
    "        \"image_id_rule\": \"file_stem_int\",\n",
    "        \"annotation_id_rule\": \"unique_row_id\",\n",
    "        \"bbox_format\": \"xywh_abs\",\n",
    "    },\n",
    "    \"notes\": \"\",\n",
    "}\n",
    "\n",
    "if CFG_PATH.exists():\n",
    "    CFG = _json_load(CFG_PATH)\n",
    "else:\n",
    "    CFG = DEFAULT_CFG\n",
    "    _json_dump(CFG_PATH, CFG)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Run manifest (run-level snapshot)\n",
    "# -----------------------------\n",
    "MANIFEST_PATH = DIRS[\"CONFIG\"] / \"run_manifest.json\"\n",
    "manifest = {\n",
    "    \"run_name\": RUN_NAME,\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"paths_meta\": str((DIRS[\"CONFIG\"] / \"paths_meta.json\").resolve()),\n",
    "    \"env_meta\": str((DIRS[\"CONFIG\"] / \"env_meta.json\").resolve()),\n",
    "    \"config\": str(CFG_PATH.resolve()),\n",
    "    \"git\": git_meta,\n",
    "}\n",
    "_json_dump(MANIFEST_PATH, manifest)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Logging helpers (jsonl)\n",
    "# -----------------------------\n",
    "METRICS_JSONL = DIRS[\"LOGS\"] / \"metrics.jsonl\"\n",
    "EVENTS_JSONL = DIRS[\"LOGS\"] / \"events.jsonl\"\n",
    "\n",
    "def log_event(name: str, payload: dict | None = None):\n",
    "    _append_jsonl(EVENTS_JSONL, {\n",
    "        \"ts\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"event\": name,\n",
    "        \"payload\": payload or {},\n",
    "        \"run_name\": RUN_NAME,\n",
    "    })\n",
    "\n",
    "def log_metrics(step: str, metrics: dict):\n",
    "    _append_jsonl(METRICS_JSONL, {\n",
    "        \"ts\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"step\": step,\n",
    "        \"metrics\": metrics,\n",
    "        \"run_name\": RUN_NAME,\n",
    "    })\n",
    "\n",
    "def save_cfg():\n",
    "    _json_dump(CFG_PATH, CFG)\n",
    "    log_event(\"config_saved\", {\"path\": str(CFG_PATH)})\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Simple run registry (runs/_registry.csv)\n",
    "# -----------------------------\n",
    "REG_PATH = WORK[\"RUNS\"] / \"_registry.csv\"\n",
    "is_new = not REG_PATH.exists()\n",
    "\n",
    "with open(REG_PATH, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    if is_new:\n",
    "        w.writerow([\"run_name\", \"created_at\", \"run_dir\", \"config_path\", \"git_head\"])\n",
    "    w.writerow([RUN_NAME, manifest[\"created_at\"], str(DIRS[\"RUN_DIR\"]), str(CFG_PATH), git_meta.get(\"git_head\")])\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Print summary\n",
    "# -----------------------------\n",
    "print(f\"[OK] CFG_PATH      : {CFG_PATH}\")\n",
    "print(f\"[OK] MANIFEST_PATH : {MANIFEST_PATH}\")\n",
    "print(f\"[OK] METRICS_JSONL  : {METRICS_JSONL}\")\n",
    "print(f\"[OK] EVENTS_JSONL   : {EVENTS_JSONL}\")\n",
    "print(f\"[OK] REGISTRY       : {REG_PATH}\")\n",
    "log_event(\"run_initialized\", {\"manifest\": str(MANIFEST_PATH)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9128bc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] RESULTS_CSV  : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\\reports\\results.csv\n",
      "[OK] RESULTS_JSONL: C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\\reports\\results.jsonl\n",
      "[OK] LATEST_MD    : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\\reports\\latest_summary.md\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 2-4. 공통 로깅 스키마 고정 + 결과표(results.csv) 기록 유틸 + 요약 리포트 생성\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# 필수 전역 변수 체크(이전 셀에서 생성된 것들)\n",
    "required_globals = [\"DIRS\", \"WORK\", \"RUN_NAME\", \"CFG\"]\n",
    "missing_g = [g for g in required_globals if g not in globals()]\n",
    "if missing_g:\n",
    "    raise RuntimeError(f\"이전 셀(2-1~2-3)을 먼저 실행해야 합니다. missing: {missing_g}\")\n",
    "\n",
    "RESULTS_CSV = Path(DIRS[\"REPORTS\"]) / \"results.csv\"\n",
    "RESULTS_JSONL = Path(DIRS[\"REPORTS\"]) / \"results.jsonl\"\n",
    "LATEST_MD = Path(DIRS[\"REPORTS\"]) / \"latest_summary.md\"\n",
    "\n",
    "def _flatten_dict(d, parent_key=\"\", sep=\".\"):\n",
    "    items = {}\n",
    "    if isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            new_key = f\"{parent_key}{sep}{k}\" if parent_key else str(k)\n",
    "            if isinstance(v, dict):\n",
    "                items.update(_flatten_dict(v, new_key, sep=sep))\n",
    "            else:\n",
    "                items[new_key] = v\n",
    "    return items\n",
    "\n",
    "def _safe_scalar(x):\n",
    "    # CSV/JSON에 넣기 안전한 형태로 변환\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, (int, float, str, bool)):\n",
    "        return x\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        return json.dumps(x, ensure_ascii=False)\n",
    "    if isinstance(x, dict):\n",
    "        return json.dumps(x, ensure_ascii=False)\n",
    "    return str(x)\n",
    "\n",
    "def _pick_cfg_fields(cfg: dict):\n",
    "    flat = _flatten_dict(cfg)\n",
    "\n",
    "    # 자주 비교할 핵심 키들(필요하면 여기만 늘리면 됨)\n",
    "    keys = [\n",
    "        \"reproducibility.seed\",\n",
    "        \"reproducibility.deterministic\",\n",
    "\n",
    "        \"split.strategy\",\n",
    "        \"split.ratios.train\",\n",
    "        \"split.ratios.valid\",\n",
    "        \"split.kfold.enabled\",\n",
    "        \"split.kfold.n_splits\",\n",
    "        \"split.kfold.fold_idx\",\n",
    "\n",
    "        \"train.framework\",\n",
    "        \"train.model.name\",\n",
    "        \"train.model.imgsz\",\n",
    "        \"train.model.pretrained\",\n",
    "        \"train.hyperparams.epochs\",\n",
    "        \"train.hyperparams.batch\",\n",
    "        \"train.hyperparams.lr0\",\n",
    "        \"train.hyperparams.weight_decay\",\n",
    "        \"train.hyperparams.workers\",\n",
    "\n",
    "        \"infer.conf_thr\",\n",
    "        \"infer.nms_iou_thr\",\n",
    "        \"infer.max_det_per_image\",\n",
    "\n",
    "        \"postprocess.strategy\",\n",
    "        \"postprocess.topk\",\n",
    "        \"postprocess.clip_boxes\",\n",
    "    ]\n",
    "\n",
    "    picked = {}\n",
    "    for k in keys:\n",
    "        picked[k] = _safe_scalar(flat.get(k, None))\n",
    "\n",
    "    # classwise threshold는 크면 요약만 (전체 dict는 jsonl에 저장)\n",
    "    cwt = cfg.get(\"postprocess\", {}).get(\"classwise_threshold\", None)\n",
    "    if isinstance(cwt, dict):\n",
    "        picked[\"postprocess.classwise_threshold.n\"] = len(cwt)\n",
    "    else:\n",
    "        picked[\"postprocess.classwise_threshold.n\"] = 0 if cwt is None else 1\n",
    "\n",
    "    return picked\n",
    "\n",
    "def init_results_table():\n",
    "    if RESULTS_CSV.exists():\n",
    "        return\n",
    "\n",
    "    base_cols = [\n",
    "        \"ts\",\n",
    "        \"run_name\",\n",
    "        \"result_name\",   # 예: \"baseline_v1\", \"imgsz960_augA\"\n",
    "        \"stage\",         # 예: \"val\", \"oof\", \"public_lb\", \"private_lb\"\n",
    "        \"notes\",\n",
    "    ]\n",
    "\n",
    "    cfg_cols = list(_pick_cfg_fields(CFG).keys())\n",
    "\n",
    "    metric_cols = [\n",
    "        # 공통으로 쓰기 좋은 메트릭 키들(실제로 없으면 빈 값)\n",
    "        \"mAP_75_95\",\n",
    "        \"mAP_50\",\n",
    "        \"mAP_75\",\n",
    "        \"mean_IoU_TP\",\n",
    "        \"precision\",\n",
    "        \"recall\",\n",
    "    ]\n",
    "\n",
    "    extra_cols = [\n",
    "        \"cfg_path\",\n",
    "        \"run_dir\",\n",
    "        \"ckpt_dir\",\n",
    "        \"submission_path\",\n",
    "    ]\n",
    "\n",
    "    header = base_cols + cfg_cols + metric_cols + extra_cols\n",
    "\n",
    "    RESULTS_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(RESULTS_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(header)\n",
    "\n",
    "init_results_table()\n",
    "\n",
    "def record_result(\n",
    "    result_name: str,\n",
    "    stage: str,\n",
    "    metrics: dict | None = None,\n",
    "    notes: str = \"\",\n",
    "    submission_path: str | Path | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    - result_name: 실험 별칭(짧게)\n",
    "    - stage: \"val\" / \"oof\" / \"public_lb\" / \"private_lb\" 등\n",
    "    - metrics: dict (키는 자유, results.csv에는 대표 키만 뽑아 기록)\n",
    "    - notes: 한 줄 메모\n",
    "    - submission_path: 제출 csv 경로(있으면 기록)\n",
    "    \"\"\"\n",
    "    ts = datetime.now().isoformat(timespec=\"seconds\")\n",
    "    metrics = metrics or {}\n",
    "    metrics = {str(k): _safe_scalar(v) for k, v in metrics.items()}\n",
    "\n",
    "    cfg_fields = _pick_cfg_fields(CFG)\n",
    "\n",
    "    # 대표 metric만 테이블에 박고, 전체는 jsonl로 별도 기록\n",
    "    def mget(key, default=None):\n",
    "        return metrics.get(key, default)\n",
    "\n",
    "    row = {\n",
    "        \"ts\": ts,\n",
    "        \"run_name\": RUN_NAME,\n",
    "        \"result_name\": result_name,\n",
    "        \"stage\": stage,\n",
    "        \"notes\": notes,\n",
    "        **cfg_fields,\n",
    "        \"mAP_75_95\": mget(\"mAP_75_95\", mget(\"mAP@[0.75:0.95]\", None)),\n",
    "        \"mAP_50\": mget(\"mAP_50\", mget(\"mAP@0.5\", None)),\n",
    "        \"mAP_75\": mget(\"mAP_75\", mget(\"mAP@0.75\", None)),\n",
    "        \"mean_IoU_TP\": mget(\"mean_IoU_TP\", None),\n",
    "        \"precision\": mget(\"precision\", None),\n",
    "        \"recall\": mget(\"recall\", None),\n",
    "        \"cfg_path\": str((Path(DIRS[\"CONFIG\"]) / \"config.json\").resolve()),\n",
    "        \"run_dir\": str(Path(DIRS[\"RUN_DIR\"]).resolve()),\n",
    "        \"ckpt_dir\": str(Path(DIRS[\"CKPT\"]).resolve()),\n",
    "        \"submission_path\": str(Path(submission_path).resolve()) if submission_path else \"\",\n",
    "    }\n",
    "\n",
    "    # results.csv에 append\n",
    "    with open(RESULTS_CSV, \"r\", encoding=\"utf-8\") as f:\n",
    "        header = next(csv.reader(f))\n",
    "\n",
    "    with open(RESULTS_CSV, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=header)\n",
    "        w.writerow({k: _safe_scalar(row.get(k, \"\")) for k in header})\n",
    "\n",
    "    # 전체 기록은 jsonl에 저장(메트릭/threshold dict 등 보존)\n",
    "    full = {\n",
    "        \"ts\": ts,\n",
    "        \"run_name\": RUN_NAME,\n",
    "        \"result_name\": result_name,\n",
    "        \"stage\": stage,\n",
    "        \"notes\": notes,\n",
    "        \"cfg\": CFG,\n",
    "        \"metrics\": metrics,\n",
    "        \"paths\": {\n",
    "            \"cfg_path\": row[\"cfg_path\"],\n",
    "            \"run_dir\": row[\"run_dir\"],\n",
    "            \"ckpt_dir\": row[\"ckpt_dir\"],\n",
    "            \"submission_path\": row[\"submission_path\"],\n",
    "        },\n",
    "    }\n",
    "    RESULTS_JSONL.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(RESULTS_JSONL, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(full, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # 최신 요약(md)\n",
    "    md = []\n",
    "    md.append(f\"# Latest Summary — {RUN_NAME}\")\n",
    "    md.append(f\"- ts: {ts}\")\n",
    "    md.append(f\"- result_name: {result_name}\")\n",
    "    md.append(f\"- stage: {stage}\")\n",
    "    if notes:\n",
    "        md.append(f\"- notes: {notes}\")\n",
    "\n",
    "    md.append(\"\\n## Key Config\")\n",
    "    for k, v in cfg_fields.items():\n",
    "        md.append(f\"- {k}: {v}\")\n",
    "\n",
    "    md.append(\"\\n## Metrics (raw)\")\n",
    "    for k, v in metrics.items():\n",
    "        md.append(f\"- {k}: {v}\")\n",
    "\n",
    "    if submission_path:\n",
    "        md.append(f\"\\n## Submission\\n- {row['submission_path']}\")\n",
    "\n",
    "    LATEST_MD.write_text(\"\\n\".join(md), encoding=\"utf-8\")\n",
    "\n",
    "    # events/logs에도 남기고 싶으면(2-3에서 정의된 함수가 있다면)\n",
    "    if \"log_event\" in globals():\n",
    "        log_event(\"result_recorded\", {\"result_name\": result_name, \"stage\": stage, \"results_csv\": str(RESULTS_CSV)})\n",
    "\n",
    "    print(f\"[OK] recorded -> {RESULTS_CSV.name} | {result_name} ({stage})\")\n",
    "    print(f\"[OK] jsonl    -> {RESULTS_JSONL.name}\")\n",
    "    print(f\"[OK] summary  -> {LATEST_MD.name}\")\n",
    "\n",
    "# 경로 출력\n",
    "print(f\"[OK] RESULTS_CSV  : {RESULTS_CSV}\")\n",
    "print(f\"[OK] RESULTS_JSONL: {RESULTS_JSONL}\")\n",
    "print(f\"[OK] LATEST_MD    : {LATEST_MD}\")\n",
    "\n",
    "# 사용 예시(실제 metric 생기면 호출)\n",
    "# record_result(\"baseline_v1\", \"val\", metrics={\"mAP_75_95\": 0.1234, \"mAP_50\": 0.4567}, notes=\"imgsz=768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aad9de91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAW DATA SUMMARY]\n",
      "- train_images      : 232 png\n",
      "- test_images       : 842 png\n",
      "- train_annotations : 763 json (subdirs=114)\n",
      "\n",
      "[SAMPLES]\n",
      "- train_images:\n",
      "  train_images\\K-001900-016548-019607-029451_0_2_0_2_70_000_200.png\n",
      "  train_images\\K-001900-016548-019607-029451_0_2_0_2_75_000_200.png\n",
      "  train_images\\K-001900-016548-019607-029451_0_2_0_2_90_000_200.png\n",
      "  train_images\\K-001900-016548-019607-033009_0_2_0_2_70_000_200.png\n",
      "  train_images\\K-001900-016548-019607-033009_0_2_0_2_75_000_200.png\n",
      "- test_images:\n",
      "  test_images\\1.png\n",
      "  test_images\\10.png\n",
      "  test_images\\100.png\n",
      "  test_images\\1003.png\n",
      "  test_images\\1004.png\n",
      "- train_annotations:\n",
      "  train_annotations\\K-001900-016548-019607-029451_json\\K-001900\\K-001900-016548-019607-029451_0_2_0_2_70_000_200.json\n",
      "  train_annotations\\K-001900-016548-019607-029451_json\\K-001900\\K-001900-016548-019607-029451_0_2_0_2_75_000_200.json\n",
      "  train_annotations\\K-001900-016548-019607-029451_json\\K-001900\\K-001900-016548-019607-029451_0_2_0_2_90_000_200.json\n",
      "  train_annotations\\K-001900-016548-019607-029451_json\\K-016548\\K-001900-016548-019607-029451_0_2_0_2_70_000_200.json\n",
      "  train_annotations\\K-001900-016548-019607-029451_json\\K-016548\\K-001900-016548-019607-029451_0_2_0_2_75_000_200.json\n",
      "\n",
      "[IMAGE_ID STEM CHECK]\n",
      "- train_images: parseable=0/232, bad=232, dup=0\n",
      "  - bad examples: ['K-001900-016548-019607-029451_0_2_0_2_70_000_200', 'K-001900-016548-019607-029451_0_2_0_2_75_000_200', 'K-001900-016548-019607-029451_0_2_0_2_90_000_200', 'K-001900-016548-019607-033009_0_2_0_2_70_000_200', 'K-001900-016548-019607-033009_0_2_0_2_75_000_200']\n",
      "- test_images: parseable=842/842, bad=0, dup=0\n",
      "\n",
      "[OK] saved inventory -> C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\\reports\\raw_data_inventory.json\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 3-1. 원천 데이터 구조 점검(폴더/파일 개수/샘플/파일명 규칙) + 인벤토리 저장\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# 이전 셀에서 만든 INPUT / DIRS가 있으면 사용, 없으면 현재 폴더 기준으로 fallback\n",
    "ROOT_ = globals().get(\"ROOT\", Path(\".\").resolve())\n",
    "INPUT_ = globals().get(\"INPUT\", {\n",
    "    \"TRAIN_IMAGES\": ROOT_ / \"train_images\",\n",
    "    \"TRAIN_ANN_DIR\": ROOT_ / \"train_annotations\",\n",
    "    \"TEST_IMAGES\": ROOT_ / \"test_images\",\n",
    "})\n",
    "REPORT_DIR = Path(globals().get(\"DIRS\", {}).get(\"REPORTS\", ROOT_ / \"artifacts\" / \"reports\"))\n",
    "\n",
    "def count_files(folder: Path, exts=None):\n",
    "    if not folder.exists():\n",
    "        return 0\n",
    "    if exts is None:\n",
    "        return sum(1 for p in folder.rglob(\"*\") if p.is_file())\n",
    "    exts = {e.lower() for e in exts}\n",
    "    return sum(1 for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts)\n",
    "\n",
    "def sample_files(folder: Path, exts=None, k=5):\n",
    "    if not folder.exists():\n",
    "        return []\n",
    "    if exts is None:\n",
    "        files = [p for p in folder.rglob(\"*\") if p.is_file()]\n",
    "    else:\n",
    "        exts = {e.lower() for e in exts}\n",
    "        files = [p for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts]\n",
    "    files = sorted(files)\n",
    "    return [str(p.relative_to(ROOT_)) for p in files[:k]]\n",
    "\n",
    "def parse_int_stems(folder: Path, exts=(\".png\",)):\n",
    "    \"\"\"파일 stem을 int로 파싱 가능 여부를 확인(제출 image_id 규칙 대비)\"\"\"\n",
    "    stems = []\n",
    "    bad = []\n",
    "    for p in sorted(folder.glob(\"*\")):\n",
    "        if p.is_file() and p.suffix.lower() in exts:\n",
    "            s = p.stem\n",
    "            try:\n",
    "                stems.append(int(s))\n",
    "            except Exception:\n",
    "                bad.append(s)\n",
    "    return stems, bad\n",
    "\n",
    "# 1) 기본 존재 여부 확인\n",
    "for k, p in INPUT_.items():\n",
    "    if not Path(p).exists():\n",
    "        raise FileNotFoundError(f\"필수 경로가 없습니다: {k} -> {p}\")\n",
    "\n",
    "train_img_dir = Path(INPUT_[\"TRAIN_IMAGES\"])\n",
    "test_img_dir = Path(INPUT_[\"TEST_IMAGES\"])\n",
    "train_ann_dir = Path(INPUT_[\"TRAIN_ANN_DIR\"])\n",
    "\n",
    "# 2) 파일 개수 요약\n",
    "summary = {\n",
    "    \"paths\": {k: str(Path(v).resolve()) for k, v in INPUT_.items()},\n",
    "    \"counts\": {\n",
    "        \"train_images_png\": count_files(train_img_dir, exts=[\".png\"]),\n",
    "        \"test_images_png\": count_files(test_img_dir, exts=[\".png\"]),\n",
    "        \"train_annotations_json\": count_files(train_ann_dir, exts=[\".json\"]),\n",
    "        \"train_annotations_dirs\": sum(1 for p in train_ann_dir.iterdir() if p.is_dir()),\n",
    "    },\n",
    "    \"samples\": {\n",
    "        \"train_images\": sample_files(train_img_dir, exts=[\".png\"], k=5),\n",
    "        \"test_images\": sample_files(test_img_dir, exts=[\".png\"], k=5),\n",
    "        \"train_annotations\": sample_files(train_ann_dir, exts=[\".json\"], k=5),\n",
    "    },\n",
    "}\n",
    "\n",
    "# 3) train_annotations 하위 폴더 분포(폴더별 json 개수)\n",
    "subdir_json_counts = {}\n",
    "for d in sorted([p for p in train_ann_dir.iterdir() if p.is_dir()]):\n",
    "    subdir_json_counts[d.name] = count_files(d, exts=[\".json\"])\n",
    "\n",
    "if subdir_json_counts:\n",
    "    vals = list(subdir_json_counts.values())\n",
    "    summary[\"train_annotations_subdir_stats\"] = {\n",
    "        \"n_subdirs\": len(vals),\n",
    "        \"min_json\": min(vals),\n",
    "        \"max_json\": max(vals),\n",
    "        \"mean_json\": sum(vals) / len(vals),\n",
    "        \"top5_subdirs_by_json\": sorted(subdir_json_counts.items(), key=lambda x: x[1], reverse=True)[:5],\n",
    "    }\n",
    "else:\n",
    "    summary[\"train_annotations_subdir_stats\"] = {\"n_subdirs\": 0}\n",
    "\n",
    "# 4) 이미지 파일명(stem) 숫자 파싱 가능 여부(제출 image_id 규칙 대비)\n",
    "train_stems, train_bad = parse_int_stems(train_img_dir, exts=(\".png\",))\n",
    "test_stems, test_bad = parse_int_stems(test_img_dir, exts=(\".png\",))\n",
    "\n",
    "def stem_stats(stems, bad):\n",
    "    dup = [s for s, c in Counter(stems).items() if c > 1]\n",
    "    return {\n",
    "        \"n_files\": len(stems) + len(bad),\n",
    "        \"n_parseable_int_stems\": len(stems),\n",
    "        \"n_unparseable_stems\": len(bad),\n",
    "        \"unparseable_examples\": bad[:10],\n",
    "        \"n_duplicate_int_stems\": len(dup),\n",
    "        \"duplicate_examples\": dup[:10],\n",
    "    }\n",
    "\n",
    "summary[\"image_id_stem_check\"] = {\n",
    "    \"train_images\": stem_stats(train_stems, train_bad),\n",
    "    \"test_images\": stem_stats(test_stems, test_bad),\n",
    "}\n",
    "\n",
    "# 5) 출력\n",
    "print(\"[RAW DATA SUMMARY]\")\n",
    "print(f\"- train_images      : {summary['counts']['train_images_png']} png\")\n",
    "print(f\"- test_images       : {summary['counts']['test_images_png']} png\")\n",
    "print(f\"- train_annotations : {summary['counts']['train_annotations_json']} json \"\n",
    "      f\"(subdirs={summary['counts']['train_annotations_dirs']})\")\n",
    "\n",
    "print(\"\\n[SAMPLES]\")\n",
    "print(\"- train_images:\", *summary[\"samples\"][\"train_images\"], sep=\"\\n  \")\n",
    "print(\"- test_images:\", *summary[\"samples\"][\"test_images\"], sep=\"\\n  \")\n",
    "print(\"- train_annotations:\", *summary[\"samples\"][\"train_annotations\"], sep=\"\\n  \")\n",
    "\n",
    "print(\"\\n[IMAGE_ID STEM CHECK]\")\n",
    "for split in [\"train_images\", \"test_images\"]:\n",
    "    st = summary[\"image_id_stem_check\"][split]\n",
    "    print(f\"- {split}: parseable={st['n_parseable_int_stems']}/{st['n_files']}, \"\n",
    "          f\"bad={st['n_unparseable_stems']}, dup={st['n_duplicate_int_stems']}\")\n",
    "    if st[\"n_unparseable_stems\"] > 0:\n",
    "        print(f\"  - bad examples: {st['unparseable_examples'][:5]}\")\n",
    "\n",
    "# 6) 인벤토리 저장\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "out_path = REPORT_DIR / \"raw_data_inventory.json\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n[OK] saved inventory -> {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fb0cda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COCO SCHEMA SUMMARY]\n",
      "- ann_root            : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\train_annotations\n",
      "- json total / scanned: 763 / 50 (limit=50)\n",
      "\n",
      "[PER-JSON COUNTS]\n",
      "- images/json      : min=1 max=1 mean=1.00 median=1.0\n",
      "- annotations/json : min=1 max=1 mean=1.00 median=1.0\n",
      "- categories/json  : min=1 max=1 mean=1.00 median=1.0\n",
      "\n",
      "[TOP-LEVEL KEYS (presence count over scanned jsons)]\n",
      "- images: 50\n",
      "- type: 50\n",
      "- annotations: 50\n",
      "- categories: 50\n",
      "\n",
      "[COMMON KEYS]\n",
      "- images keys (top10): ['file_name', 'width', 'height', 'imgfile', 'drug_N', 'drug_S', 'back_color', 'drug_dir', 'light_color', 'camera_la']\n",
      "- ann keys    (top10): ['area', 'iscrowd', 'bbox', 'category_id', 'ignore', 'segmentation', 'id', 'image_id']\n",
      "- cat keys    (top10): ['supercategory', 'id', 'name']\n",
      "\n",
      "[BBOX FORMAT CHECK]\n",
      "- bbox length distribution: {4: 50}\n",
      "- bbox examples:\n",
      "  - K-001900-016548-019607-029451_0_2_0_2_70_000_200.json: [644, 845, 189, 190]\n",
      "  - K-001900-016548-019607-029451_0_2_0_2_75_000_200.json: [148, 241, 181, 181]\n",
      "  - K-001900-016548-019607-029451_0_2_0_2_90_000_200.json: [167, 248, 184, 182]\n",
      "  - K-001900-016548-019607-029451_0_2_0_2_70_000_200.json: [144, 799, 239, 239]\n",
      "  - K-001900-016548-019607-029451_0_2_0_2_75_000_200.json: [578, 222, 233, 226]\n",
      "\n",
      "[CATEGORIES SAMPLE]\n",
      "- id=1900 name=보령부스파정 5mg\n",
      "- id=1900 name=보령부스파정 5mg\n",
      "- id=1900 name=보령부스파정 5mg\n",
      "- id=16548 name=가바토파정 100mg\n",
      "- id=16548 name=가바토파정 100mg\n",
      "- id=16548 name=가바토파정 100mg\n",
      "- id=19607 name=스토가정 10mg\n",
      "- id=19607 name=스토가정 10mg\n",
      "- id=19607 name=스토가정 10mg\n",
      "- id=29451 name=레일라정\n",
      "\n",
      "[OK] saved -> C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\\reports\\coco_schema_summary.json\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 3-2. COCO JSON 구조(Top-level / images / annotations / categories) 스키마 스캔 + 요약 리포트 저장\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "import statistics as stats\n",
    "\n",
    "# --- paths (이전 셀 변수 우선 사용) ---\n",
    "ROOT_ = globals().get(\"ROOT\", Path(\".\").resolve())\n",
    "INPUT_ = globals().get(\"INPUT\", {\n",
    "    \"TRAIN_IMAGES\": ROOT_ / \"train_images\",\n",
    "    \"TRAIN_ANN_DIR\": ROOT_ / \"train_annotations\",\n",
    "    \"TEST_IMAGES\": ROOT_ / \"test_images\",\n",
    "})\n",
    "REPORT_DIR = Path(globals().get(\"DIRS\", {}).get(\"REPORTS\", ROOT_ / \"artifacts\" / \"reports\"))\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ann_root = Path(INPUT_[\"TRAIN_ANN_DIR\"])\n",
    "json_files = sorted(ann_root.rglob(\"*.json\"))\n",
    "if not json_files:\n",
    "    raise FileNotFoundError(f\"train_annotations 아래에서 json을 찾지 못했습니다: {ann_root}\")\n",
    "\n",
    "# --- scan controls ---\n",
    "MAX_SCAN = int(os.environ.get(\"COCO_SCHEMA_SCAN_N\", \"50\"))  # 많으면 느려질 수 있어 기본 50\n",
    "scan_files = json_files[:min(MAX_SCAN, len(json_files))]\n",
    "\n",
    "def _type_name(x):\n",
    "    return type(x).__name__\n",
    "\n",
    "def _safe_get_first(lst):\n",
    "    return lst[0] if isinstance(lst, list) and len(lst) > 0 else None\n",
    "\n",
    "def _summarize_counts(vals):\n",
    "    if not vals:\n",
    "        return {\"n\": 0}\n",
    "    return {\n",
    "        \"n\": len(vals),\n",
    "        \"min\": min(vals),\n",
    "        \"max\": max(vals),\n",
    "        \"mean\": sum(vals) / len(vals),\n",
    "        \"median\": stats.median(vals),\n",
    "        \"top5\": Counter(vals).most_common(5),\n",
    "    }\n",
    "\n",
    "# --- collectors ---\n",
    "top_keys = Counter()\n",
    "images_keys = Counter()\n",
    "ann_keys = Counter()\n",
    "cat_keys = Counter()\n",
    "\n",
    "images_count_list = []\n",
    "ann_count_list = []\n",
    "cat_count_list = []\n",
    "\n",
    "bbox_len = Counter()\n",
    "bbox_bad_examples = []\n",
    "bbox_examples = []\n",
    "\n",
    "image_extra_keys_examples = defaultdict(int)  # extra key 빈도(이미지 객체에서)\n",
    "cat_id_name = {}  # 마지막으로 본 name으로 덮어쓰기(같으면 문제 없음)\n",
    "cat_id_name_examples = []\n",
    "\n",
    "warnings = []\n",
    "\n",
    "# --- scanning ---\n",
    "for fp in scan_files:\n",
    "    try:\n",
    "        with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if not isinstance(data, dict):\n",
    "            warnings.append({\"file\": str(fp), \"warn\": f\"top-level is not dict: {type(data)}\"})\n",
    "            continue\n",
    "\n",
    "        for k in data.keys():\n",
    "            top_keys[k] += 1\n",
    "\n",
    "        images = data.get(\"images\", [])\n",
    "        anns = data.get(\"annotations\", [])\n",
    "        cats = data.get(\"categories\", [])\n",
    "\n",
    "        if not isinstance(images, list):\n",
    "            warnings.append({\"file\": str(fp), \"warn\": f\"'images' is not list: {type(images)}\"})\n",
    "            images = []\n",
    "        if not isinstance(anns, list):\n",
    "            warnings.append({\"file\": str(fp), \"warn\": f\"'annotations' is not list: {type(anns)}\"})\n",
    "            anns = []\n",
    "        if not isinstance(cats, list):\n",
    "            warnings.append({\"file\": str(fp), \"warn\": f\"'categories' is not list: {type(cats)}\"})\n",
    "            cats = []\n",
    "\n",
    "        images_count_list.append(len(images))\n",
    "        ann_count_list.append(len(anns))\n",
    "        cat_count_list.append(len(cats))\n",
    "\n",
    "        # images keys\n",
    "        img0 = _safe_get_first(images)\n",
    "        if isinstance(img0, dict):\n",
    "            for k in img0.keys():\n",
    "                images_keys[k] += 1\n",
    "                # extra meta 키 분포를 크게 보고 싶으면(기본적으로 다 포함)\n",
    "                image_extra_keys_examples[k] += 1\n",
    "\n",
    "        # annotations keys + bbox sanity\n",
    "        ann0 = _safe_get_first(anns)\n",
    "        if isinstance(ann0, dict):\n",
    "            for k in ann0.keys():\n",
    "                ann_keys[k] += 1\n",
    "\n",
    "        # bbox checks: 샘플 몇 개만\n",
    "        for a in anns[:20]:\n",
    "            if not isinstance(a, dict):\n",
    "                continue\n",
    "            b = a.get(\"bbox\", None)\n",
    "            if b is None:\n",
    "                bbox_len[\"None\"] += 1\n",
    "                continue\n",
    "            if not isinstance(b, list):\n",
    "                bbox_len[\"not_list\"] += 1\n",
    "                bbox_bad_examples.append({\"file\": str(fp), \"bbox\": str(b)[:200]})\n",
    "                continue\n",
    "            bbox_len[len(b)] += 1\n",
    "            if len(b) == 4 and len(bbox_examples) < 5:\n",
    "                bbox_examples.append({\"file\": str(fp), \"bbox\": b})\n",
    "            if len(b) != 4 and len(bbox_bad_examples) < 10:\n",
    "                bbox_bad_examples.append({\"file\": str(fp), \"bbox\": b})\n",
    "\n",
    "        # categories keys + id/name mapping\n",
    "        cat0 = _safe_get_first(cats)\n",
    "        if isinstance(cat0, dict):\n",
    "            for k in cat0.keys():\n",
    "                cat_keys[k] += 1\n",
    "\n",
    "        # id->name 예시 10개까지만\n",
    "        for c in cats[:20]:\n",
    "            if not isinstance(c, dict):\n",
    "                continue\n",
    "            cid = c.get(\"id\", None)\n",
    "            name = c.get(\"name\", None)\n",
    "            if cid is not None and name is not None:\n",
    "                cat_id_name[cid] = name\n",
    "                if len(cat_id_name_examples) < 10:\n",
    "                    cat_id_name_examples.append({\"id\": cid, \"name\": name})\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.append({\"file\": str(fp), \"warn\": f\"json load failed: {repr(e)}\"})\n",
    "\n",
    "# --- build summary ---\n",
    "summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"ann_root\": str(ann_root.resolve()),\n",
    "    \"n_json_files_total\": len(json_files),\n",
    "    \"n_json_files_scanned\": len(scan_files),\n",
    "    \"scan_limit\": MAX_SCAN,\n",
    "    \"top_level_keys_presence\": top_keys.most_common(),\n",
    "    \"counts\": {\n",
    "        \"images_per_json\": _summarize_counts(images_count_list),\n",
    "        \"annotations_per_json\": _summarize_counts(ann_count_list),\n",
    "        \"categories_per_json\": _summarize_counts(cat_count_list),\n",
    "    },\n",
    "    \"images\": {\n",
    "        \"common_keys_top30\": images_keys.most_common(30),\n",
    "    },\n",
    "    \"annotations\": {\n",
    "        \"common_keys_top30\": ann_keys.most_common(30),\n",
    "        \"bbox_len_distribution\": dict(bbox_len),\n",
    "        \"bbox_examples\": bbox_examples,\n",
    "        \"bbox_bad_examples\": bbox_bad_examples[:10],\n",
    "    },\n",
    "    \"categories\": {\n",
    "        \"common_keys_top30\": cat_keys.most_common(30),\n",
    "        \"id_name_examples\": cat_id_name_examples,\n",
    "        \"unique_ids_seen_in_scanned\": len(cat_id_name),\n",
    "    },\n",
    "    \"warnings\": warnings,\n",
    "}\n",
    "\n",
    "out_json = REPORT_DIR / \"coco_schema_summary.json\"\n",
    "with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# --- print key points ---\n",
    "print(\"[COCO SCHEMA SUMMARY]\")\n",
    "print(f\"- ann_root            : {ann_root}\")\n",
    "print(f\"- json total / scanned: {len(json_files)} / {len(scan_files)} (limit={MAX_SCAN})\")\n",
    "\n",
    "c_images = summary[\"counts\"][\"images_per_json\"]\n",
    "c_anns = summary[\"counts\"][\"annotations_per_json\"]\n",
    "c_cats = summary[\"counts\"][\"categories_per_json\"]\n",
    "print(\"\\n[PER-JSON COUNTS]\")\n",
    "print(f\"- images/json      : min={c_images.get('min')} max={c_images.get('max')} mean={c_images.get('mean'):.2f} median={c_images.get('median')}\")\n",
    "print(f\"- annotations/json : min={c_anns.get('min')} max={c_anns.get('max')} mean={c_anns.get('mean'):.2f} median={c_anns.get('median')}\")\n",
    "print(f\"- categories/json  : min={c_cats.get('min')} max={c_cats.get('max')} mean={c_cats.get('mean'):.2f} median={c_cats.get('median')}\")\n",
    "\n",
    "print(\"\\n[TOP-LEVEL KEYS (presence count over scanned jsons)]\")\n",
    "for k, v in top_keys.most_common():\n",
    "    print(f\"- {k}: {v}\")\n",
    "\n",
    "print(\"\\n[COMMON KEYS]\")\n",
    "print(\"- images keys (top10):\", [k for k, _ in summary[\"images\"][\"common_keys_top30\"][:10]])\n",
    "print(\"- ann keys    (top10):\", [k for k, _ in summary[\"annotations\"][\"common_keys_top30\"][:10]])\n",
    "print(\"- cat keys    (top10):\", [k for k, _ in summary[\"categories\"][\"common_keys_top30\"][:10]])\n",
    "\n",
    "print(\"\\n[BBOX FORMAT CHECK]\")\n",
    "print(\"- bbox length distribution:\", summary[\"annotations\"][\"bbox_len_distribution\"])\n",
    "if summary[\"annotations\"][\"bbox_examples\"]:\n",
    "    print(\"- bbox examples:\")\n",
    "    for ex in summary[\"annotations\"][\"bbox_examples\"]:\n",
    "        print(f\"  - {Path(ex['file']).name}: {ex['bbox']}\")\n",
    "\n",
    "print(\"\\n[CATEGORIES SAMPLE]\")\n",
    "for ex in summary[\"categories\"][\"id_name_examples\"][:10]:\n",
    "    print(f\"- id={ex['id']} name={ex['name']}\")\n",
    "\n",
    "print(f\"\\n[OK] saved -> {out_json}\")\n",
    "if warnings:\n",
    "    print(f\"[WARN] warnings={len(warnings)} (see JSON for details)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9759668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QC GATE 1 SUMMARY]\n",
      "- json files                : 763\n",
      "- ann records               : 763\n",
      "- unique images(annotated)  : 232\n",
      "- objects/image dist        : {4: 74, 3: 151, 2: 7}\n",
      "- max objects per image     : 4 (gt4 images=0)\n",
      "- invalid bbox              : 0\n",
      "- nonpositive bbox          : 0\n",
      "- out-of-bounds bbox        : 1\n",
      "- missing image files       : 0\n",
      "- unique category ids       : 56\n",
      "- inconsistent id->name     : 0\n",
      "- duplicate ann keys        : 0\n",
      "\n",
      "[SAVED]\n",
      "- C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\\reports\\qc_gate1_summary.json\n",
      "- C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\\reports\\objects_per_image.csv\n",
      "- C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\\reports\\categories_id_to_name.csv\n",
      "- C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\\reports\\missing_images_samples.json\n",
      "- C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\\reports\\invalid_bbox_samples.json\n",
      "- C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\\reports\\oob_bbox_samples.json\n",
      "- C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\\reports\\nonpos_bbox_samples.json\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 3-3. QC Gate 1: bbox/이미지 매칭/중복/객체수(<=4)/카테고리 일관성 점검 리포트\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# --- paths (이전 셀 변수 우선 사용) ---\n",
    "ROOT_ = globals().get(\"ROOT\", Path(\".\").resolve())\n",
    "INPUT_ = globals().get(\"INPUT\", {\n",
    "    \"TRAIN_IMAGES\": ROOT_ / \"train_images\",\n",
    "    \"TRAIN_ANN_DIR\": ROOT_ / \"train_annotations\",\n",
    "    \"TEST_IMAGES\": ROOT_ / \"test_images\",\n",
    "})\n",
    "REPORT_DIR = Path(globals().get(\"DIRS\", {}).get(\"REPORTS\", ROOT_ / \"artifacts\" / \"reports\"))\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_img_dir = Path(INPUT_[\"TRAIN_IMAGES\"])\n",
    "ann_root = Path(INPUT_[\"TRAIN_ANN_DIR\"])\n",
    "\n",
    "json_files = sorted(ann_root.rglob(\"*.json\"))\n",
    "if not json_files:\n",
    "    raise FileNotFoundError(f\"train_annotations 아래에서 json을 찾지 못했습니다: {ann_root}\")\n",
    "\n",
    "# --- train_images 파일 존재 체크용 set ---\n",
    "train_image_files = {p.name for p in train_img_dir.glob(\"*.png\")}\n",
    "\n",
    "# --- collectors ---\n",
    "records = []  # annotation-level records\n",
    "image_meta = {}  # file_name -> {w,h,seen_paths}\n",
    "missing_images = []\n",
    "invalid_bbox = []\n",
    "oob_bbox = []  # out-of-bounds\n",
    "nonpos_bbox = []\n",
    "dup_key_counter = Counter()\n",
    "\n",
    "cat_id_to_names = defaultdict(set)  # id -> set(names)\n",
    "\n",
    "def _is_number(x):\n",
    "    return isinstance(x, (int, float))\n",
    "\n",
    "def _round3(x):\n",
    "    try:\n",
    "        return round(float(x), 3)\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "def _bbox_key(file_name, category_id, bbox):\n",
    "    # 중복 판단용 키(소수점이 있을 수도 있으니 반올림)\n",
    "    x, y, w, h = bbox\n",
    "    return (file_name, int(category_id), _round3(x), _round3(y), _round3(w), _round3(h))\n",
    "\n",
    "for fp in json_files:\n",
    "    try:\n",
    "        with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        images = data.get(\"images\", [])\n",
    "        anns = data.get(\"annotations\", [])\n",
    "        cats = data.get(\"categories\", [])\n",
    "\n",
    "        # 방어적 처리\n",
    "        if not isinstance(images, list): images = []\n",
    "        if not isinstance(anns, list): anns = []\n",
    "        if not isinstance(cats, list): cats = []\n",
    "\n",
    "        # categories: id-name 수집\n",
    "        for c in cats:\n",
    "            if not isinstance(c, dict): \n",
    "                continue\n",
    "            cid = c.get(\"id\", None)\n",
    "            nm = c.get(\"name\", None)\n",
    "            if cid is not None and nm is not None:\n",
    "                try:\n",
    "                    cid_int = int(cid)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                cat_id_to_names[cid_int].add(str(nm))\n",
    "\n",
    "        # images: 일반적으로 1개지만, 여러 개여도 대응\n",
    "        # annotation은 image_id로 연결되지만 여기서는 file_name 기준 집계를 위해 file_name을 우선 확보\n",
    "        # (대회 제출 image_id도 file stem 숫자 기반이므로 file_name이 중요)\n",
    "        if len(images) == 0:\n",
    "            # images가 비어있으면 이후 연결이 어려워서 경고 레코드만 남김\n",
    "            invalid_bbox.append({\"file\": str(fp), \"reason\": \"images_empty\"})\n",
    "            continue\n",
    "\n",
    "        # images dict 만들기: image_id -> (file_name, w, h)\n",
    "        img_by_id = {}\n",
    "        for im in images:\n",
    "            if not isinstance(im, dict):\n",
    "                continue\n",
    "            image_id = im.get(\"id\", None)\n",
    "            file_name = im.get(\"file_name\", None)\n",
    "            w = im.get(\"width\", None)\n",
    "            h = im.get(\"height\", None)\n",
    "            if image_id is None or file_name is None:\n",
    "                continue\n",
    "            img_by_id[image_id] = (str(file_name), w, h)\n",
    "\n",
    "            # file_name 메타 누적\n",
    "            if str(file_name) not in image_meta:\n",
    "                image_meta[str(file_name)] = {\n",
    "                    \"width\": w,\n",
    "                    \"height\": h,\n",
    "                    \"json_paths\": set(),\n",
    "                }\n",
    "            image_meta[str(file_name)][\"json_paths\"].add(str(fp))\n",
    "\n",
    "        # annotations 처리\n",
    "        if len(anns) == 0:\n",
    "            invalid_bbox.append({\"file\": str(fp), \"reason\": \"annotations_empty\"})\n",
    "            continue\n",
    "\n",
    "        for a in anns:\n",
    "            if not isinstance(a, dict):\n",
    "                continue\n",
    "\n",
    "            image_id = a.get(\"image_id\", None)\n",
    "            category_id = a.get(\"category_id\", None)\n",
    "            bbox = a.get(\"bbox\", None)\n",
    "\n",
    "            # file_name/size 연결\n",
    "            file_name, w, h = None, None, None\n",
    "            if image_id in img_by_id:\n",
    "                file_name, w, h = img_by_id[image_id]\n",
    "            else:\n",
    "                # image_id 키가 images에 없으면 images[0]로 fallback\n",
    "                # (데이터 구조가 항상 1개일 때 안전장치)\n",
    "                only = next(iter(img_by_id.values()))\n",
    "                file_name, w, h = only\n",
    "\n",
    "            # 이미지 파일 존재 체크\n",
    "            if file_name not in train_image_files:\n",
    "                missing_images.append({\"file_name\": file_name, \"json\": str(fp)})\n",
    "\n",
    "            # bbox 유효성\n",
    "            if bbox is None or (not isinstance(bbox, list)) or len(bbox) != 4:\n",
    "                invalid_bbox.append({\n",
    "                    \"file_name\": file_name,\n",
    "                    \"json\": str(fp),\n",
    "                    \"reason\": \"bbox_missing_or_bad_format\",\n",
    "                    \"bbox\": bbox,\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            x, y, bw, bh = bbox\n",
    "            if not all(_is_number(v) for v in [x, y, bw, bh]):\n",
    "                invalid_bbox.append({\n",
    "                    \"file_name\": file_name,\n",
    "                    \"json\": str(fp),\n",
    "                    \"reason\": \"bbox_not_numeric\",\n",
    "                    \"bbox\": bbox,\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            x, y, bw, bh = float(x), float(y), float(bw), float(bh)\n",
    "\n",
    "            if bw <= 0 or bh <= 0:\n",
    "                nonpos_bbox.append({\n",
    "                    \"file_name\": file_name,\n",
    "                    \"json\": str(fp),\n",
    "                    \"bbox\": [x, y, bw, bh],\n",
    "                })\n",
    "\n",
    "            # out-of-bounds 체크(이미지 크기 w/h가 있으면)\n",
    "            oob = False\n",
    "            if _is_number(w) and _is_number(h):\n",
    "                W, H = float(w), float(h)\n",
    "                if x < 0 or y < 0 or (x + bw) > W or (y + bh) > H:\n",
    "                    oob = True\n",
    "                    oob_bbox.append({\n",
    "                        \"file_name\": file_name,\n",
    "                        \"json\": str(fp),\n",
    "                        \"image_wh\": [W, H],\n",
    "                        \"bbox\": [x, y, bw, bh],\n",
    "                    })\n",
    "\n",
    "            # 중복 체크 키\n",
    "            if category_id is not None:\n",
    "                try:\n",
    "                    ck = _bbox_key(file_name, category_id, [x, y, bw, bh])\n",
    "                    dup_key_counter[ck] += 1\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            records.append({\n",
    "                \"json_path\": str(fp),\n",
    "                \"file_name\": file_name,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": int(category_id) if category_id is not None else None,\n",
    "                \"bbox_x\": x, \"bbox_y\": y, \"bbox_w\": bw, \"bbox_h\": bh,\n",
    "                \"image_w\": w, \"image_h\": h,\n",
    "                \"oob\": oob,\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        invalid_bbox.append({\"file\": str(fp), \"reason\": f\"json_load_or_parse_failed: {repr(e)}\"})\n",
    "\n",
    "# --- image-wise aggregation ---\n",
    "obj_count_by_image = Counter()\n",
    "for r in records:\n",
    "    obj_count_by_image[r[\"file_name\"]] += 1\n",
    "\n",
    "# 이미지당 객체 수 분포\n",
    "dist_obj_per_image = Counter(obj_count_by_image.values())\n",
    "max_objs = max(obj_count_by_image.values()) if obj_count_by_image else 0\n",
    "n_images = len(obj_count_by_image)\n",
    "\n",
    "# >4 객체 이미지(규칙 위반 여부 확인)\n",
    "gt4_images = [fn for fn, c in obj_count_by_image.items() if c > 4]\n",
    "\n",
    "# category id-name 일관성(같은 id에 name 여러 개면 문제)\n",
    "cat_inconsistency = {cid: sorted(list(names)) for cid, names in cat_id_to_names.items() if len(names) > 1}\n",
    "\n",
    "# 중복 어노테이션(키 count>1)\n",
    "dup_ann = [(k, c) for k, c in dup_key_counter.items() if c > 1]\n",
    "dup_ann_sorted = sorted(dup_ann, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"ann_root\": str(ann_root.resolve()),\n",
    "    \"train_images_dir\": str(train_img_dir.resolve()),\n",
    "    \"n_json_files\": len(json_files),\n",
    "    \"n_ann_records\": len(records),\n",
    "\n",
    "    \"n_unique_images_in_annotations\": n_images,\n",
    "    \"objects_per_image_distribution\": dict(dist_obj_per_image),\n",
    "    \"max_objects_per_image\": max_objs,\n",
    "    \"n_images_gt4\": len(gt4_images),\n",
    "\n",
    "    \"bbox_issues\": {\n",
    "        \"n_invalid_bbox\": len(invalid_bbox),\n",
    "        \"n_nonpos_bbox\": len(nonpos_bbox),\n",
    "        \"n_oob_bbox\": len(oob_bbox),\n",
    "    },\n",
    "    \"missing_image_files\": {\n",
    "        \"n_missing\": len(missing_images),\n",
    "    },\n",
    "\n",
    "    \"categories\": {\n",
    "        \"n_unique_category_ids_seen\": len(cat_id_to_names),\n",
    "        \"n_inconsistent_id_name\": len(cat_inconsistency),\n",
    "        \"inconsistent_examples\": dict(list(cat_inconsistency.items())[:10]),\n",
    "    },\n",
    "\n",
    "    \"duplicates\": {\n",
    "        \"n_duplicate_ann_keys\": len(dup_ann_sorted),\n",
    "        \"top10_duplicates\": [\n",
    "            {\n",
    "                \"file_name\": k[0],\n",
    "                \"category_id\": k[1],\n",
    "                \"bbox\": [k[2], k[3], k[4], k[5]],\n",
    "                \"count\": c,\n",
    "            }\n",
    "            for (k, c) in dup_ann_sorted[:10]\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "# --- save artifacts ---\n",
    "out_summary = REPORT_DIR / \"qc_gate1_summary.json\"\n",
    "with open(out_summary, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# save image-level counts csv (간단 csv)\n",
    "out_counts = REPORT_DIR / \"objects_per_image.csv\"\n",
    "with open(out_counts, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"file_name,n_objects\\n\")\n",
    "    for fn, c in sorted(obj_count_by_image.items(), key=lambda x: (-x[1], x[0])):\n",
    "        f.write(f\"{fn},{c}\\n\")\n",
    "\n",
    "# save category map csv (id -> name(s))\n",
    "out_cats = REPORT_DIR / \"categories_id_to_name.csv\"\n",
    "with open(out_cats, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"category_id,n_names,names\\n\")\n",
    "    for cid in sorted(cat_id_to_names.keys()):\n",
    "        names = sorted(list(cat_id_to_names[cid]))\n",
    "        f.write(f\"{cid},{len(names)},\\\"{'; '.join(names)}\\\"\\n\")\n",
    "\n",
    "# save issue samples\n",
    "def _save_samples(path: Path, items, k=50):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(items[:k], f, indent=2, ensure_ascii=False)\n",
    "\n",
    "_save_samples(REPORT_DIR / \"missing_images_samples.json\", missing_images, k=50)\n",
    "_save_samples(REPORT_DIR / \"invalid_bbox_samples.json\", invalid_bbox, k=50)\n",
    "_save_samples(REPORT_DIR / \"oob_bbox_samples.json\", oob_bbox, k=50)\n",
    "_save_samples(REPORT_DIR / \"nonpos_bbox_samples.json\", nonpos_bbox, k=50)\n",
    "\n",
    "print(\"[QC GATE 1 SUMMARY]\")\n",
    "print(f\"- json files                : {summary['n_json_files']}\")\n",
    "print(f\"- ann records               : {summary['n_ann_records']}\")\n",
    "print(f\"- unique images(annotated)  : {summary['n_unique_images_in_annotations']}\")\n",
    "print(f\"- objects/image dist        : {summary['objects_per_image_distribution']}\")\n",
    "print(f\"- max objects per image     : {summary['max_objects_per_image']} (gt4 images={summary['n_images_gt4']})\")\n",
    "print(f\"- invalid bbox              : {summary['bbox_issues']['n_invalid_bbox']}\")\n",
    "print(f\"- nonpositive bbox          : {summary['bbox_issues']['n_nonpos_bbox']}\")\n",
    "print(f\"- out-of-bounds bbox        : {summary['bbox_issues']['n_oob_bbox']}\")\n",
    "print(f\"- missing image files       : {summary['missing_image_files']['n_missing']}\")\n",
    "print(f\"- unique category ids       : {summary['categories']['n_unique_category_ids_seen']}\")\n",
    "print(f\"- inconsistent id->name     : {summary['categories']['n_inconsistent_id_name']}\")\n",
    "print(f\"- duplicate ann keys        : {summary['duplicates']['n_duplicate_ann_keys']}\")\n",
    "\n",
    "print(\"\\n[SAVED]\")\n",
    "print(f\"- {out_summary}\")\n",
    "print(f\"- {out_counts}\")\n",
    "print(f\"- {out_cats}\")\n",
    "print(f\"- {REPORT_DIR / 'missing_images_samples.json'}\")\n",
    "print(f\"- {REPORT_DIR / 'invalid_bbox_samples.json'}\")\n",
    "print(f\"- {REPORT_DIR / 'oob_bbox_samples.json'}\")\n",
    "print(f\"- {REPORT_DIR / 'nonpos_bbox_samples.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9bfad32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MERGE COCO DONE]\n",
      "- json files read         : 763\n",
      "- unique images           : 232\n",
      "- annotations             : 762\n",
      "- unique category ids     : 56\n",
      "\n",
      "[CLIP/INVALID]\n",
      "- clipped bbox count      : 1\n",
      "- invalid after clip(drop): 1\n",
      "- skipped(etc)            : 0\n",
      "\n",
      "[OBJECTS PER IMAGE]\n",
      "- dist (n_obj -> n_images): {4: 74, 3: 150, 2: 8}\n",
      "- max objects per image   : 4 (gt4 images=0)\n",
      "\n",
      "[SAVED]\n",
      "- merged coco  : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\data\\cache\\exp_20260202_230604\\train_merged_coco.json\n",
      "- image_id_map : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\data\\cache\\exp_20260202_230604\\image_id_map.json\n",
      "- cat_id->name : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\data\\cache\\exp_20260202_230604\\category_id_to_name.json\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 3-4. train_annotations(객체 1개/JSON) -> 이미지 단위 통합 COCO 생성 + bbox 클리핑 + 매핑 저장\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT_ = globals().get(\"ROOT\", Path(\".\").resolve())\n",
    "INPUT_ = globals().get(\"INPUT\", {\n",
    "    \"TRAIN_IMAGES\": ROOT_ / \"train_images\",\n",
    "    \"TRAIN_ANN_DIR\": ROOT_ / \"train_annotations\",\n",
    "    \"TEST_IMAGES\": ROOT_ / \"test_images\",\n",
    "})\n",
    "DIRS_ = globals().get(\"DIRS\", {})\n",
    "CACHE_DIR = Path(DIRS_.get(\"CACHE\", ROOT_ / \"data\" / \"cache\" / \"merged\"))\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_img_dir = Path(INPUT_[\"TRAIN_IMAGES\"])\n",
    "ann_root = Path(INPUT_[\"TRAIN_ANN_DIR\"])\n",
    "json_files = sorted(ann_root.rglob(\"*.json\"))\n",
    "if not json_files:\n",
    "    raise FileNotFoundError(f\"train_annotations 아래에서 json을 찾지 못했습니다: {ann_root}\")\n",
    "\n",
    "# 옵션: file stem이 숫자면 그것을 image_id로 사용(제출 규칙과 연결하기 쉬움)\n",
    "USE_FILE_STEM_AS_IMAGE_ID = True\n",
    "\n",
    "def _to_int_stem(file_name: str):\n",
    "    try:\n",
    "        return int(Path(file_name).stem)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _clip_bbox_xywh(bbox, W, H):\n",
    "    \"\"\"bbox=[x,y,w,h]를 이미지 경계로 클리핑. 반환: (new_bbox, clipped_flag, valid_flag)\"\"\"\n",
    "    x, y, w, h = map(float, bbox)\n",
    "    x1, y1 = x, y\n",
    "    x2, y2 = x + w, y + h\n",
    "\n",
    "    # clip\n",
    "    cx1 = max(0.0, min(x1, float(W)))\n",
    "    cy1 = max(0.0, min(y1, float(H)))\n",
    "    cx2 = max(0.0, min(x2, float(W)))\n",
    "    cy2 = max(0.0, min(y2, float(H)))\n",
    "\n",
    "    nw = cx2 - cx1\n",
    "    nh = cy2 - cy1\n",
    "    clipped = (cx1 != x1) or (cy1 != y1) or (cx2 != x2) or (cy2 != y2)\n",
    "    valid = (nw > 0.0) and (nh > 0.0)\n",
    "    return [cx1, cy1, nw, nh], clipped, valid\n",
    "\n",
    "# ---- aggregate per image(file_name) ----\n",
    "img_meta_by_name = {}\n",
    "anns_by_name = defaultdict(list)\n",
    "cat_id_to_name = {}\n",
    "\n",
    "seen_json = 0\n",
    "skipped_ann = 0\n",
    "clipped_ann = 0\n",
    "invalid_after_clip = 0\n",
    "\n",
    "for fp in json_files:\n",
    "    seen_json += 1\n",
    "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    images = data.get(\"images\", [])\n",
    "    anns = data.get(\"annotations\", [])\n",
    "    cats = data.get(\"categories\", [])\n",
    "\n",
    "    if not images or not anns or not cats:\n",
    "        skipped_ann += 1\n",
    "        continue\n",
    "\n",
    "    im = images[0]\n",
    "    a = anns[0]\n",
    "    c = cats[0]\n",
    "\n",
    "    file_name = str(im.get(\"file_name\"))\n",
    "    W = im.get(\"width\", None)\n",
    "    H = im.get(\"height\", None)\n",
    "\n",
    "    # categories map 수집\n",
    "    cid = c.get(\"id\", None)\n",
    "    cname = c.get(\"name\", None)\n",
    "    if cid is not None and cname is not None:\n",
    "        cat_id_to_name[int(cid)] = str(cname)\n",
    "\n",
    "    # 이미지 메타 고정(처음 본 걸 기준)\n",
    "    if file_name not in img_meta_by_name:\n",
    "        img_meta_by_name[file_name] = {\n",
    "            \"file_name\": file_name,\n",
    "            \"width\": W,\n",
    "            \"height\": H,\n",
    "            # 필요하면 추가 메타도 유지 가능(너무 크면 최소 필드만 권장)\n",
    "        }\n",
    "\n",
    "    bbox = a.get(\"bbox\", None)\n",
    "    category_id = a.get(\"category_id\", None)\n",
    "\n",
    "    if bbox is None or (not isinstance(bbox, list)) or len(bbox) != 4:\n",
    "        skipped_ann += 1\n",
    "        continue\n",
    "    if category_id is None:\n",
    "        skipped_ann += 1\n",
    "        continue\n",
    "\n",
    "    # bbox clip(이미지 W/H가 있을 때만)\n",
    "    clipped = False\n",
    "    valid = True\n",
    "    new_bbox = list(map(float, bbox))\n",
    "    if isinstance(W, (int, float)) and isinstance(H, (int, float)):\n",
    "        new_bbox, clipped, valid = _clip_bbox_xywh(bbox, W, H)\n",
    "        if clipped:\n",
    "            clipped_ann += 1\n",
    "        if not valid:\n",
    "            invalid_after_clip += 1\n",
    "            continue\n",
    "\n",
    "    anns_by_name[file_name].append({\n",
    "        \"category_id\": int(category_id),\n",
    "        \"bbox\": new_bbox,\n",
    "        # COCO 최소 필드(필요하면 확장)\n",
    "        \"iscrowd\": int(a.get(\"iscrowd\", 0) or 0),\n",
    "        \"ignore\": int(a.get(\"ignore\", 0) or 0),\n",
    "        \"area\": float(a.get(\"area\", new_bbox[2] * new_bbox[3])),\n",
    "        \"segmentation\": a.get(\"segmentation\", []),\n",
    "    })\n",
    "\n",
    "# ---- build merged COCO ----\n",
    "# image_id 정책\n",
    "file_names = sorted(img_meta_by_name.keys())\n",
    "image_id_map = {}\n",
    "images_out = []\n",
    "annotations_out = []\n",
    "\n",
    "next_img_id = 1\n",
    "next_ann_id = 1\n",
    "\n",
    "for fn in file_names:\n",
    "    meta = img_meta_by_name[fn]\n",
    "    W, H = meta.get(\"width\"), meta.get(\"height\")\n",
    "\n",
    "    img_id = None\n",
    "    if USE_FILE_STEM_AS_IMAGE_ID:\n",
    "        img_id = _to_int_stem(fn)\n",
    "    if img_id is None:\n",
    "        img_id = next_img_id\n",
    "        next_img_id += 1\n",
    "\n",
    "    image_id_map[fn] = img_id\n",
    "\n",
    "    images_out.append({\n",
    "        \"id\": img_id,\n",
    "        \"file_name\": fn,\n",
    "        \"width\": W,\n",
    "        \"height\": H,\n",
    "    })\n",
    "\n",
    "    # annotations (이미지당 최대 4개 기대)\n",
    "    for ann in anns_by_name.get(fn, []):\n",
    "        x, y, w, h = ann[\"bbox\"]\n",
    "        annotations_out.append({\n",
    "            \"id\": next_ann_id,\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": ann[\"category_id\"],\n",
    "            \"bbox\": [x, y, w, h],\n",
    "            \"area\": float(ann.get(\"area\", w * h)),\n",
    "            \"iscrowd\": int(ann.get(\"iscrowd\", 0)),\n",
    "            \"ignore\": int(ann.get(\"ignore\", 0)),\n",
    "            \"segmentation\": ann.get(\"segmentation\", []),\n",
    "        })\n",
    "        next_ann_id += 1\n",
    "\n",
    "# categories 정리(id 정렬)\n",
    "categories_out = []\n",
    "for cid in sorted(cat_id_to_name.keys()):\n",
    "    categories_out.append({\n",
    "        \"id\": int(cid),\n",
    "        \"name\": cat_id_to_name[cid],\n",
    "        \"supercategory\": \"pill\",\n",
    "    })\n",
    "\n",
    "merged = {\n",
    "    \"info\": {\n",
    "        \"description\": \"AI07 pill OD - merged coco (train)\",\n",
    "        \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "        \"source\": str(ann_root.resolve()),\n",
    "    },\n",
    "    \"images\": images_out,\n",
    "    \"annotations\": annotations_out,\n",
    "    \"categories\": categories_out,\n",
    "}\n",
    "\n",
    "# ---- sanity checks ----\n",
    "# 객체수 분포\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for a in annotations_out:\n",
    "    cnt[a[\"image_id\"]] += 1\n",
    "dist = Counter(cnt.values())\n",
    "max_objs = max(cnt.values()) if cnt else 0\n",
    "gt4 = sum(1 for v in cnt.values() if v > 4)\n",
    "\n",
    "# ---- save ----\n",
    "out_coco = CACHE_DIR / \"train_merged_coco.json\"\n",
    "out_map = CACHE_DIR / \"image_id_map.json\"\n",
    "out_cat = CACHE_DIR / \"category_id_to_name.json\"\n",
    "\n",
    "with open(out_coco, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(merged, f, indent=2, ensure_ascii=False)\n",
    "with open(out_map, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(image_id_map, f, indent=2, ensure_ascii=False)\n",
    "with open(out_cat, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cat_id_to_name, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"[MERGE COCO DONE]\")\n",
    "print(f\"- json files read         : {seen_json}\")\n",
    "print(f\"- unique images           : {len(images_out)}\")\n",
    "print(f\"- annotations             : {len(annotations_out)}\")\n",
    "print(f\"- unique category ids     : {len(categories_out)}\")\n",
    "\n",
    "print(\"\\n[CLIP/INVALID]\")\n",
    "print(f\"- clipped bbox count      : {clipped_ann}\")\n",
    "print(f\"- invalid after clip(drop): {invalid_after_clip}\")\n",
    "print(f\"- skipped(etc)            : {skipped_ann}\")\n",
    "\n",
    "print(\"\\n[OBJECTS PER IMAGE]\")\n",
    "print(f\"- dist (n_obj -> n_images): {dict(dist)}\")\n",
    "print(f\"- max objects per image   : {max_objs} (gt4 images={gt4})\")\n",
    "\n",
    "print(\"\\n[SAVED]\")\n",
    "print(f\"- merged coco  : {out_coco}\")\n",
    "print(f\"- image_id_map : {out_map}\")\n",
    "print(f\"- cat_id->name : {out_cat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9156d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LABEL MAP BUILT]\n",
      "- merged coco           : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\data\\cache\\exp_20260202_230604\\train_merged_coco.json\n",
      "- num train categories  : 56\n",
      "- saved full map        : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\data\\cache\\exp_20260202_230604\\label_map_full.json\n",
      "- saved class counts    : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\\reports\\class_counts_by_category_id.csv\n",
      "\n",
      "[CLASS COUNT TOP5]\n",
      "- 3351 | 일양하이트린정 2mg | n=153\n",
      "- 3483 | 기넥신에프정(은행엽엑스)(수출용) | n=45\n",
      "- 35206 | 아토젯정 10/40mg | n=37\n",
      "- 16262 | 크레스토정 20mg | n=23\n",
      "- 21325 | 아토르바정 10mg | n=22\n",
      "\n",
      "[CLASS COUNT BOTTOM5]\n",
      "- 29451 | 레일라정 | n=3\n",
      "- 33009 | 신바로정 | n=3\n",
      "- 21771 | 라비에트정 20mg | n=3\n",
      "- 27926 | 울트라셋이알서방정 | n=3\n",
      "- 24850 | 놀텍정 10mg | n=3\n",
      "\n",
      "[WHITELIST NOT FOUND]\n",
      "- 지금은 train(56 클래스) 기준 full 매핑만 저장했습니다.\n",
      "- 나중에 test 40개 클래스 id 리스트를 파일로 저장하면(예: ./data/whitelist_40.txt) 자동으로 whitelist 매핑도 생성됩니다.\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 3-5. category_id 매핑(연속 index) + class distribution + (optional) test whitelist 적용 준비\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT_ = globals().get(\"ROOT\", Path(\".\").resolve())\n",
    "DIRS_ = globals().get(\"DIRS\", {})\n",
    "\n",
    "# 3-4에서 사용한 CACHE_DIR 우선, 없으면 DIRS[\"CACHE\"] 기반\n",
    "CACHE_DIR = Path(globals().get(\"CACHE_DIR\", DIRS_.get(\"CACHE\", ROOT_ / \"data\" / \"cache\" / \"merged\")))\n",
    "\n",
    "# merged coco 경로 탐색(기본 파일명)\n",
    "MERGED_COCO = CACHE_DIR / \"train_merged_coco.json\"\n",
    "if not MERGED_COCO.exists():\n",
    "    # 혹시 파일명이 다르거나 경로가 달라졌을 때를 대비해 rglob fallback\n",
    "    cand = list(CACHE_DIR.rglob(\"train_merged_coco.json\"))\n",
    "    if cand:\n",
    "        MERGED_COCO = cand[0]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"train_merged_coco.json 을 찾지 못했습니다. CACHE_DIR={CACHE_DIR}\")\n",
    "\n",
    "REPORT_DIR = Path(DIRS_.get(\"REPORTS\", ROOT_ / \"artifacts\" / \"reports\"))\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) merged coco 로드\n",
    "# -----------------------------\n",
    "with open(MERGED_COCO, \"r\", encoding=\"utf-8\") as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "images = coco.get(\"images\", [])\n",
    "anns = coco.get(\"annotations\", [])\n",
    "cats = coco.get(\"categories\", [])\n",
    "\n",
    "if not (isinstance(images, list) and isinstance(anns, list) and isinstance(cats, list)):\n",
    "    raise ValueError(\"merged coco 구조가 예상과 다릅니다. images/annotations/categories가 list인지 확인하세요.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) category_id -> name, 정렬된 category_id 목록\n",
    "# -----------------------------\n",
    "cat_id_to_name = {}\n",
    "for c in cats:\n",
    "    if not isinstance(c, dict):\n",
    "        continue\n",
    "    cid = c.get(\"id\", None)\n",
    "    name = c.get(\"name\", None)\n",
    "    if cid is None or name is None:\n",
    "        continue\n",
    "    cat_id_to_name[int(cid)] = str(name)\n",
    "\n",
    "cat_ids_sorted = sorted(cat_id_to_name.keys())\n",
    "if not cat_ids_sorted:\n",
    "    raise ValueError(\"categories에서 유효한 category id를 추출하지 못했습니다.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) 클래스 분포(원본 category_id 기준)\n",
    "# -----------------------------\n",
    "cat_counts = Counter()\n",
    "for a in anns:\n",
    "    if not isinstance(a, dict):\n",
    "        continue\n",
    "    cid = a.get(\"category_id\", None)\n",
    "    if cid is None:\n",
    "        continue\n",
    "    cat_counts[int(cid)] += 1\n",
    "\n",
    "# -----------------------------\n",
    "# 4) (옵션) test whitelist 로드 시도\n",
    "#    - 있으면: whitelist 기반 매핑도 추가로 저장\n",
    "#    - 없으면: full 매핑만 저장하고 안내 메시지 출력\n",
    "# -----------------------------\n",
    "def _load_whitelist():\n",
    "    \"\"\"\n",
    "    whitelist 파일 자동 탐색:\n",
    "    - ./data/test_class_whitelist.json  (예: {\"whitelist\":[1,2,...]} 또는 [1,2,...])\n",
    "    - ./data/test_classes_40.json\n",
    "    - ./data/whitelist_40.txt (한 줄에 하나)\n",
    "    - ./data/whitelist.txt\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        ROOT_ / \"data\" / \"test_class_whitelist.json\",\n",
    "        ROOT_ / \"data\" / \"test_classes_40.json\",\n",
    "        ROOT_ / \"data\" / \"whitelist_40.txt\",\n",
    "        ROOT_ / \"data\" / \"whitelist.txt\",\n",
    "        CACHE_DIR / \"test_class_whitelist.json\",\n",
    "        CACHE_DIR / \"test_classes_40.json\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if not p.exists():\n",
    "            continue\n",
    "        if p.suffix.lower() == \".txt\":\n",
    "            vals = []\n",
    "            for line in p.read_text(encoding=\"utf-8\").splitlines():\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                vals.append(int(line))\n",
    "            return sorted(set(vals)), str(p)\n",
    "        if p.suffix.lower() == \".json\":\n",
    "            obj = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "            if isinstance(obj, list):\n",
    "                return sorted(set(int(x) for x in obj)), str(p)\n",
    "            if isinstance(obj, dict):\n",
    "                key = \"whitelist\" if \"whitelist\" in obj else (\"classes\" if \"classes\" in obj else None)\n",
    "                if key and isinstance(obj[key], list):\n",
    "                    return sorted(set(int(x) for x in obj[key])), str(p)\n",
    "    return None, None\n",
    "\n",
    "whitelist_ids, whitelist_path = _load_whitelist()\n",
    "\n",
    "# -----------------------------\n",
    "# 5) 매핑 생성\n",
    "#    - full mapping: 모든 train category_id -> contiguous index(0..N-1)\n",
    "#    - whitelist mapping(옵션): whitelist에 있는 category_id만 -> contiguous index\n",
    "# -----------------------------\n",
    "def build_mapping(category_ids, cat_id_to_name_dict):\n",
    "    id2idx = {cid: i for i, cid in enumerate(category_ids)}\n",
    "    idx2id = {i: cid for cid, i in id2idx.items()}\n",
    "    names = [cat_id_to_name_dict[cid] for cid in category_ids]\n",
    "    return {\n",
    "        \"category_ids\": category_ids,\n",
    "        \"id2idx\": id2idx,\n",
    "        \"idx2id\": idx2id,\n",
    "        \"names\": names,  # index->class name\n",
    "        \"num_classes\": len(category_ids),\n",
    "    }\n",
    "\n",
    "full_map = build_mapping(cat_ids_sorted, cat_id_to_name)\n",
    "\n",
    "whitelist_map = None\n",
    "train_only_ids = []\n",
    "if whitelist_ids:\n",
    "    # whitelist에 있지만 train에 없는 id는 제외(안전)\n",
    "    whitelist_ids_in_train = [cid for cid in whitelist_ids if cid in cat_id_to_name]\n",
    "    whitelist_map = build_mapping(whitelist_ids_in_train, cat_id_to_name)\n",
    "    train_only_ids = [cid for cid in cat_ids_sorted if cid not in set(whitelist_ids_in_train)]\n",
    "\n",
    "# -----------------------------\n",
    "# 6) 저장 (cache + reports)\n",
    "# -----------------------------\n",
    "out_full = CACHE_DIR / \"label_map_full.json\"\n",
    "with open(out_full, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(full_map, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "out_counts = REPORT_DIR / \"class_counts_by_category_id.csv\"\n",
    "with open(out_counts, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"category_id,class_name,n_annotations\\n\")\n",
    "    for cid in cat_ids_sorted:\n",
    "        f.write(f\"{cid},\\\"{cat_id_to_name[cid]}\\\",{cat_counts.get(cid,0)}\\n\")\n",
    "\n",
    "# whitelist가 있으면 추가 저장\n",
    "out_whitelist = None\n",
    "out_train_only = None\n",
    "if whitelist_map is not None:\n",
    "    out_whitelist = CACHE_DIR / \"label_map_whitelist.json\"\n",
    "    with open(out_whitelist, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            **whitelist_map,\n",
    "            \"whitelist_source\": whitelist_path,\n",
    "        }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    out_train_only = REPORT_DIR / \"train_only_category_ids.json\"\n",
    "    with open(out_train_only, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"train_only_category_ids\": train_only_ids,\n",
    "            \"n_train_only\": len(train_only_ids),\n",
    "            \"whitelist_source\": whitelist_path,\n",
    "        }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 7) CFG 업데이트(가능하면)\n",
    "# -----------------------------\n",
    "if \"CFG\" in globals():\n",
    "    CFG[\"data\"][\"num_classes\"] = full_map[\"num_classes\"]\n",
    "    CFG[\"data\"][\"class_whitelist\"] = whitelist_ids if whitelist_ids else None\n",
    "    # 저장 함수가 있으면 호출\n",
    "    if \"save_cfg\" in globals():\n",
    "        save_cfg()\n",
    "\n",
    "# -----------------------------\n",
    "# 8) 출력 요약\n",
    "# -----------------------------\n",
    "print(\"[LABEL MAP BUILT]\")\n",
    "print(f\"- merged coco           : {MERGED_COCO}\")\n",
    "print(f\"- num train categories  : {full_map['num_classes']}\")\n",
    "print(f\"- saved full map        : {out_full}\")\n",
    "print(f\"- saved class counts    : {out_counts}\")\n",
    "\n",
    "# 클래스 분포 요약(상/하위)\n",
    "most = cat_counts.most_common(5)\n",
    "least = sorted(cat_counts.items(), key=lambda x: x[1])[:5]\n",
    "print(\"\\n[CLASS COUNT TOP5]\")\n",
    "for cid, n in most:\n",
    "    print(f\"- {cid} | {cat_id_to_name.get(cid,'?')} | n={n}\")\n",
    "print(\"\\n[CLASS COUNT BOTTOM5]\")\n",
    "for cid, n in least:\n",
    "    print(f\"- {cid} | {cat_id_to_name.get(cid,'?')} | n={n}\")\n",
    "\n",
    "if whitelist_ids:\n",
    "    print(\"\\n[WHITELIST DETECTED]\")\n",
    "    print(f\"- source              : {whitelist_path}\")\n",
    "    print(f\"- whitelist ids (raw) : {len(whitelist_ids)}\")\n",
    "    print(f\"- whitelist in train  : {whitelist_map['num_classes']}\")\n",
    "    print(f\"- train-only ids      : {len(train_only_ids)}\")\n",
    "    print(f\"- saved whitelist map : {out_whitelist}\")\n",
    "    print(f\"- saved train-only    : {out_train_only}\")\n",
    "else:\n",
    "    print(\"\\n[WHITELIST NOT FOUND]\")\n",
    "    print(\"- 지금은 train(56 클래스) 기준 full 매핑만 저장했습니다.\")\n",
    "    print(\"- 나중에 test 40개 클래스 id 리스트를 파일로 저장하면(예: ./data/whitelist_40.txt) 자동으로 whitelist 매핑도 생성됩니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d98bdaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IMAGE TABLE BUILT]\n",
      "- n_images        : 232\n",
      "- n_annotations   : 762\n",
      "- n_categories    : 56\n",
      "- dist n_objects  : {4: 74, 3: 150, 2: 8}\n",
      "- dist n_labels   : {4: 74, 3: 150, 2: 8}\n",
      "- missing img meta: 0\n",
      "\n",
      "[SAVED]\n",
      "- C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\\reports\\image_table.csv\n",
      "- C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\\reports\\image_table.jsonl\n",
      "- C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\\reports\\image_table_summary.json\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 3-6. merged COCO -> image-level table(멀티라벨/객체수) 생성 + 분포 리포트 저장\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "ROOT_ = globals().get(\"ROOT\", Path(\".\").resolve())\n",
    "DIRS_ = globals().get(\"DIRS\", {})\n",
    "\n",
    "CACHE_DIR = Path(globals().get(\"CACHE_DIR\", DIRS_.get(\"CACHE\", ROOT_ / \"data\" / \"cache\" / \"merged\")))\n",
    "MERGED_COCO = CACHE_DIR / \"train_merged_coco.json\"\n",
    "LABEL_MAP = CACHE_DIR / \"label_map_full.json\"\n",
    "\n",
    "if not MERGED_COCO.exists():\n",
    "    cand = list(CACHE_DIR.rglob(\"train_merged_coco.json\"))\n",
    "    if cand:\n",
    "        MERGED_COCO = cand[0]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"train_merged_coco.json 을 찾지 못했습니다. CACHE_DIR={CACHE_DIR}\")\n",
    "\n",
    "REPORT_DIR = Path(DIRS_.get(\"REPORTS\", ROOT_ / \"artifacts\" / \"reports\"))\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(MERGED_COCO, \"r\", encoding=\"utf-8\") as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "images = coco.get(\"images\", [])\n",
    "anns = coco.get(\"annotations\", [])\n",
    "cats = coco.get(\"categories\", [])\n",
    "\n",
    "# category_id -> name\n",
    "cat_id_to_name = {}\n",
    "for c in cats:\n",
    "    if isinstance(c, dict) and c.get(\"id\") is not None and c.get(\"name\") is not None:\n",
    "        cat_id_to_name[int(c[\"id\"])] = str(c[\"name\"])\n",
    "\n",
    "# (있으면) label_map_full의 id2idx 사용(학습용 인덱스)\n",
    "id2idx = None\n",
    "if LABEL_MAP.exists():\n",
    "    with open(LABEL_MAP, \"r\", encoding=\"utf-8\") as f:\n",
    "        lm = json.load(f)\n",
    "    id2idx = {int(k): int(v) for k, v in lm.get(\"id2idx\", {}).items()}\n",
    "\n",
    "# image_id -> meta\n",
    "img_by_id = {int(im[\"id\"]): im for im in images if isinstance(im, dict) and im.get(\"id\") is not None}\n",
    "\n",
    "# image_id -> list of anns\n",
    "anns_by_img = defaultdict(list)\n",
    "for a in anns:\n",
    "    if not isinstance(a, dict):\n",
    "        continue\n",
    "    iid = a.get(\"image_id\", None)\n",
    "    if iid is None:\n",
    "        continue\n",
    "    anns_by_img[int(iid)].append(a)\n",
    "\n",
    "rows = []\n",
    "n_missing_img_meta = 0\n",
    "\n",
    "for iid, alist in anns_by_img.items():\n",
    "    im = img_by_id.get(iid, None)\n",
    "    if im is None:\n",
    "        n_missing_img_meta += 1\n",
    "        file_name = None\n",
    "        W = None\n",
    "        H = None\n",
    "    else:\n",
    "        file_name = im.get(\"file_name\")\n",
    "        W = im.get(\"width\")\n",
    "        H = im.get(\"height\")\n",
    "\n",
    "    cat_ids = [int(a[\"category_id\"]) for a in alist if a.get(\"category_id\") is not None]\n",
    "    uniq_cat_ids = sorted(set(cat_ids))\n",
    "    n_obj = len(alist)\n",
    "    n_labels = len(uniq_cat_ids)\n",
    "\n",
    "    # 멀티라벨 시그니처(스플릿에서 유용)\n",
    "    sig = \",\".join(map(str, uniq_cat_ids))\n",
    "    sig_hash = hashlib.md5(sig.encode(\"utf-8\")).hexdigest()[:10]  # 짧게\n",
    "\n",
    "    # bbox 요약(작은 객체 분포/품질 진단용)\n",
    "    areas = []\n",
    "    for a in alist:\n",
    "        b = a.get(\"bbox\", None)\n",
    "        if isinstance(b, list) and len(b) == 4:\n",
    "            w = float(b[2]); h = float(b[3])\n",
    "            if w > 0 and h > 0:\n",
    "                areas.append(w * h)\n",
    "\n",
    "    row = {\n",
    "        \"image_id\": int(iid),\n",
    "        \"file_name\": file_name,\n",
    "        \"width\": W,\n",
    "        \"height\": H,\n",
    "        \"n_objects\": n_obj,\n",
    "        \"n_unique_labels\": n_labels,\n",
    "        \"category_ids\": uniq_cat_ids,  # 리스트 그대로(json 저장용)\n",
    "        \"category_names\": [cat_id_to_name.get(cid, \"?\") for cid in uniq_cat_ids],\n",
    "        \"label_signature\": sig,\n",
    "        \"label_sig_hash\": sig_hash,\n",
    "        \"area_min\": min(areas) if areas else None,\n",
    "        \"area_max\": max(areas) if areas else None,\n",
    "        \"area_mean\": (sum(areas) / len(areas)) if areas else None,\n",
    "    }\n",
    "\n",
    "    # 학습용 contiguous class index도 같이 저장(있을 때만)\n",
    "    if id2idx is not None:\n",
    "        row[\"class_indices\"] = [id2idx.get(cid, None) for cid in uniq_cat_ids]\n",
    "    rows.append(row)\n",
    "\n",
    "# ---- distributions ----\n",
    "dist_obj = Counter(r[\"n_objects\"] for r in rows)\n",
    "dist_labels = Counter(r[\"n_unique_labels\"] for r in rows)\n",
    "\n",
    "# 클래스 커버리지(이미지 단위 포함 빈도)\n",
    "img_count_per_cat = Counter()\n",
    "obj_count_per_cat = Counter()\n",
    "for r in rows:\n",
    "    uniq = r[\"category_ids\"]\n",
    "    for cid in uniq:\n",
    "        img_count_per_cat[cid] += 1\n",
    "for a in anns:\n",
    "    if isinstance(a, dict) and a.get(\"category_id\") is not None:\n",
    "        obj_count_per_cat[int(a[\"category_id\"])] += 1\n",
    "\n",
    "summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"merged_coco\": str(MERGED_COCO),\n",
    "    \"n_images\": len(rows),\n",
    "    \"n_annotations\": len(anns),\n",
    "    \"n_categories\": len(cat_id_to_name),\n",
    "    \"n_missing_img_meta\": n_missing_img_meta,\n",
    "    \"dist_n_objects\": dict(dist_obj),\n",
    "    \"dist_n_unique_labels\": dict(dist_labels),\n",
    "    \"top5_cats_by_obj\": [\n",
    "        {\"category_id\": cid, \"name\": cat_id_to_name.get(cid, \"?\"), \"n_objects\": n}\n",
    "        for cid, n in obj_count_per_cat.most_common(5)\n",
    "    ],\n",
    "    \"bottom5_cats_by_obj\": [\n",
    "        {\"category_id\": cid, \"name\": cat_id_to_name.get(cid, \"?\"), \"n_objects\": n}\n",
    "        for cid, n in sorted(obj_count_per_cat.items(), key=lambda x: x[1])[:5]\n",
    "    ],\n",
    "}\n",
    "\n",
    "# ---- save: jsonl (row-level) ----\n",
    "out_jsonl = REPORT_DIR / \"image_table.jsonl\"\n",
    "with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in rows:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# ---- save: csv (리스트 컬럼은 문자열로) ----\n",
    "out_csv = REPORT_DIR / \"image_table.csv\"\n",
    "with open(out_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    f.write(\"image_id,file_name,width,height,n_objects,n_unique_labels,label_sig_hash,label_signature,category_ids,area_min,area_max,area_mean\\n\")\n",
    "    for r in sorted(rows, key=lambda x: x[\"image_id\"]):\n",
    "        cat_ids_str = json.dumps(r[\"category_ids\"], ensure_ascii=False)\n",
    "        sig = r[\"label_signature\"]\n",
    "        f.write(\n",
    "            f\"{r['image_id']},{r['file_name']},{r['width']},{r['height']},\"\n",
    "            f\"{r['n_objects']},{r['n_unique_labels']},{r['label_sig_hash']},\"\n",
    "            f\"\\\"{sig}\\\",\\\"{cat_ids_str}\\\",{r['area_min']},{r['area_max']},{r['area_mean']}\\n\"\n",
    "        )\n",
    "\n",
    "# ---- save: summary ----\n",
    "out_summary = REPORT_DIR / \"image_table_summary.json\"\n",
    "with open(out_summary, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# ---- prints ----\n",
    "print(\"[IMAGE TABLE BUILT]\")\n",
    "print(f\"- n_images        : {summary['n_images']}\")\n",
    "print(f\"- n_annotations   : {summary['n_annotations']}\")\n",
    "print(f\"- n_categories    : {summary['n_categories']}\")\n",
    "print(f\"- dist n_objects  : {summary['dist_n_objects']}\")\n",
    "print(f\"- dist n_labels   : {summary['dist_n_unique_labels']}\")\n",
    "print(f\"- missing img meta: {summary['n_missing_img_meta']}\")\n",
    "\n",
    "print(\"\\n[SAVED]\")\n",
    "print(f\"- {out_csv}\")\n",
    "print(f\"- {out_jsonl}\")\n",
    "print(f\"- {out_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bda01116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPLIT DONE]\n",
      "- seed           : 42\n",
      "- stratify_mode  : n_objects (fallback=True)\n",
      "- counts         : total=232 train=185 valid=47\n",
      "\n",
      "[DISTRIBUTION CHECK]\n",
      "- train n_objects: {3: 120, 4: 59, 2: 6}\n",
      "- valid n_objects: {3: 30, 4: 15, 2: 2}\n",
      "\n",
      "[SAVED]\n",
      "- split json : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\data\\cache\\exp_20260202_230604\\splits\\split_train_valid.json\n",
      "- train txt  : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\data\\cache\\exp_20260202_230604\\splits\\train_ids.txt\n",
      "- valid txt  : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\data\\cache\\exp_20260202_230604\\splits\\valid_ids.txt\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 3-7. 스플릿 고정(train/val) + stratify(객체수/라벨시그니처) + split 파일 저장\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT_ = globals().get(\"ROOT\", Path(\".\").resolve())\n",
    "DIRS_ = globals().get(\"DIRS\", {})\n",
    "CACHE_DIR = Path(globals().get(\"CACHE_DIR\", DIRS_.get(\"CACHE\", ROOT_ / \"data\" / \"cache\" / \"merged\")))\n",
    "REPORT_DIR = Path(DIRS_.get(\"REPORTS\", ROOT_ / \"artifacts\" / \"reports\"))\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# inputs from previous steps\n",
    "MERGED_COCO = CACHE_DIR / \"train_merged_coco.json\"\n",
    "IMAGE_TABLE_JSONL = REPORT_DIR / \"image_table.jsonl\"\n",
    "\n",
    "if not MERGED_COCO.exists():\n",
    "    cand = list(CACHE_DIR.rglob(\"train_merged_coco.json\"))\n",
    "    if cand:\n",
    "        MERGED_COCO = cand[0]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"train_merged_coco.json 을 찾지 못했습니다. CACHE_DIR={CACHE_DIR}\")\n",
    "\n",
    "if not IMAGE_TABLE_JSONL.exists():\n",
    "    cand = list(REPORT_DIR.rglob(\"image_table.jsonl\"))\n",
    "    if cand:\n",
    "        IMAGE_TABLE_JSONL = cand[0]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"image_table.jsonl 을 찾지 못했습니다. REPORT_DIR={REPORT_DIR}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 0) split config\n",
    "# -----------------------------\n",
    "SEED_ = int(globals().get(\"SEED\", 42))\n",
    "RATIOS = globals().get(\"CFG\", {}).get(\"split\", {}).get(\"ratios\", {\"train\": 0.8, \"valid\": 0.2})\n",
    "train_ratio = float(RATIOS.get(\"train\", 0.8))\n",
    "valid_ratio = float(RATIOS.get(\"valid\", 0.2))\n",
    "\n",
    "# 안전: 합이 1이 아니면 valid를 보정\n",
    "s = train_ratio + valid_ratio\n",
    "if abs(s - 1.0) > 1e-6:\n",
    "    valid_ratio = 1.0 - train_ratio\n",
    "\n",
    "# Stratify 모드:\n",
    "# - \"n_objects\": 이미지당 객체수(2/3/4)\n",
    "# - \"signature\": label_sig_hash (멀티라벨 시그니처)\n",
    "# - \"hybrid\": n_objects + signature (가능하면 이게 제일 안전)\n",
    "STRATIFY_MODE = \"hybrid\"  # \"n_objects\" | \"signature\" | \"hybrid\"\n",
    "\n",
    "# 하이브리드에서 너무 쪼개져 샘플이 1개짜리 strata가 많으면 fallback\n",
    "MIN_PER_STRATUM = 2  # strata의 최소 샘플 수(이보다 작으면 fallback)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) image_table 로드\n",
    "# -----------------------------\n",
    "rows = []\n",
    "with open(IMAGE_TABLE_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        rows.append(json.loads(line))\n",
    "\n",
    "if not rows:\n",
    "    raise RuntimeError(\"image_table.jsonl이 비어 있습니다. 3-6 셀을 확인하세요.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) stratify key 생성\n",
    "# -----------------------------\n",
    "def make_key(r):\n",
    "    nobj = int(r[\"n_objects\"])\n",
    "    sig = r.get(\"label_sig_hash\", \"\")\n",
    "    if STRATIFY_MODE == \"n_objects\":\n",
    "        return f\"nobj={nobj}\"\n",
    "    if STRATIFY_MODE == \"signature\":\n",
    "        return f\"sig={sig}\"\n",
    "    # hybrid\n",
    "    return f\"nobj={nobj}|sig={sig}\"\n",
    "\n",
    "keys = [make_key(r) for r in rows]\n",
    "key_counts = Counter(keys)\n",
    "\n",
    "# strata가 너무 쪼개졌는지 판단(샘플 1개 strata가 많으면)\n",
    "n_singletons = sum(1 for k, c in key_counts.items() if c < MIN_PER_STRATUM)\n",
    "singleton_ratio = n_singletons / max(1, len(key_counts))\n",
    "\n",
    "FALLBACK_USED = False\n",
    "if STRATIFY_MODE == \"hybrid\" and (n_singletons > 0):\n",
    "    # 하이브리드가 너무 잘게 나뉘면 n_objects로 fallback\n",
    "    STRATIFY_MODE = \"n_objects\"\n",
    "    FALLBACK_USED = True\n",
    "    keys = [make_key(r) for r in rows]\n",
    "    key_counts = Counter(keys)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) stratified split\n",
    "# -----------------------------\n",
    "rng = random.Random(SEED_)\n",
    "by_key = defaultdict(list)\n",
    "for r, k in zip(rows, keys):\n",
    "    by_key[k].append(r)\n",
    "\n",
    "# 각 strata 내부 shuffle\n",
    "for k in by_key:\n",
    "    rng.shuffle(by_key[k])\n",
    "\n",
    "train_ids = []\n",
    "valid_ids = []\n",
    "\n",
    "for k, items in by_key.items():\n",
    "    n = len(items)\n",
    "    n_train = int(round(n * train_ratio))\n",
    "    # 너무 작은 strata는 최소 1개는 valid로 보내거나(가능하면) train에만 두지 않도록 조절\n",
    "    if n >= 2:\n",
    "        n_train = min(max(1, n_train), n - 1)  # train: [1, n-1]\n",
    "    else:\n",
    "        n_train = n  # 1개면 train로\n",
    "    train_part = items[:n_train]\n",
    "    valid_part = items[n_train:]\n",
    "    train_ids.extend([int(x[\"image_id\"]) for x in train_part])\n",
    "    valid_ids.extend([int(x[\"image_id\"]) for x in valid_part])\n",
    "\n",
    "# 전역 shuffle(고정 seed)\n",
    "rng.shuffle(train_ids)\n",
    "rng.shuffle(valid_ids)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) split 산출물 저장\n",
    "# -----------------------------\n",
    "RUN_NAME_ = globals().get(\"RUN_NAME\", \"run\")\n",
    "SPLIT_DIR = CACHE_DIR / \"splits\"\n",
    "SPLIT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "split_obj = {\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"run_name\": RUN_NAME_,\n",
    "    \"seed\": SEED_,\n",
    "    \"train_ratio\": train_ratio,\n",
    "    \"valid_ratio\": valid_ratio,\n",
    "    \"stratify_mode\": \"n_objects\" if FALLBACK_USED else STRATIFY_MODE,\n",
    "    \"fallback_used\": FALLBACK_USED,\n",
    "    \"counts\": {\n",
    "        \"total\": len(rows),\n",
    "        \"train\": len(train_ids),\n",
    "        \"valid\": len(valid_ids),\n",
    "    },\n",
    "    \"train_image_ids\": train_ids,\n",
    "    \"valid_image_ids\": valid_ids,\n",
    "}\n",
    "\n",
    "out_split_json = SPLIT_DIR / \"split_train_valid.json\"\n",
    "with open(out_split_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(split_obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# 텍스트 버전도 저장(간편용)\n",
    "out_train_txt = SPLIT_DIR / \"train_ids.txt\"\n",
    "out_valid_txt = SPLIT_DIR / \"valid_ids.txt\"\n",
    "out_train_txt.write_text(\"\\n\".join(map(str, train_ids)) + \"\\n\", encoding=\"utf-8\")\n",
    "out_valid_txt.write_text(\"\\n\".join(map(str, valid_ids)) + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5) split 품질 점검(분포 비교)\n",
    "# -----------------------------\n",
    "id_to_row = {int(r[\"image_id\"]): r for r in rows}\n",
    "\n",
    "def dist(ids, key):\n",
    "    c = Counter()\n",
    "    for iid in ids:\n",
    "        r = id_to_row.get(int(iid))\n",
    "        if r is None:\n",
    "            continue\n",
    "        c[int(r[key])] += 1\n",
    "    return dict(c)\n",
    "\n",
    "train_dist_obj = dist(train_ids, \"n_objects\")\n",
    "valid_dist_obj = dist(valid_ids, \"n_objects\")\n",
    "\n",
    "print(\"[SPLIT DONE]\")\n",
    "print(f\"- seed           : {SEED_}\")\n",
    "print(f\"- stratify_mode  : {split_obj['stratify_mode']} (fallback={FALLBACK_USED})\")\n",
    "print(f\"- counts         : total={split_obj['counts']['total']} train={split_obj['counts']['train']} valid={split_obj['counts']['valid']}\")\n",
    "\n",
    "print(\"\\n[DISTRIBUTION CHECK]\")\n",
    "print(f\"- train n_objects: {train_dist_obj}\")\n",
    "print(f\"- valid n_objects: {valid_dist_obj}\")\n",
    "\n",
    "print(\"\\n[SAVED]\")\n",
    "print(f\"- split json : {out_split_json}\")\n",
    "print(f\"- train txt  : {out_train_txt}\")\n",
    "print(f\"- valid txt  : {out_valid_txt}\")\n",
    "\n",
    "# CFG 업데이트(있으면)\n",
    "if \"CFG\" in globals():\n",
    "    CFG[\"split\"][\"seed\"] = SEED_\n",
    "    CFG[\"split\"][\"ratios\"] = {\"train\": train_ratio, \"valid\": valid_ratio}\n",
    "    CFG[\"split\"][\"strategy\"] = split_obj[\"stratify_mode\"]\n",
    "    if \"save_cfg\" in globals():\n",
    "        save_cfg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "709c0e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[YOLO DATASET READY]\n",
      "- dataset_root : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\data\\datasets\\pill_od_yolo_exp_20260202_230604\n",
      "- data.yaml    : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\data\\datasets\\pill_od_yolo_exp_20260202_230604\\data.yaml\n",
      "- nc           : 56\n",
      "- copy_mode    : copy\n",
      "\n",
      "[COUNTS]\n",
      "- train images : 185 | labels: 185 | objects: 608\n",
      "- val images   : 47   | labels: 47   | objects: 154\n",
      "- missing imgs : 0\n",
      "- skipped box  : 0\n",
      "- empty labels : 0\n",
      "\n",
      "[SANITY]\n",
      "- bad train label lines: 0\n",
      "- bad val label lines  : 0\n",
      "\n",
      "[SAVED]\n",
      "- C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\data\\datasets\\pill_od_yolo_exp_20260202_230604\\convert_manifest.json\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 3-8. (Ultralytics YOLO) merged COCO + split -> YOLO dataset(images/labels) 생성 + data.yaml 저장 + 검증\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import shutil\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT_ = globals().get(\"ROOT\", Path(\".\").resolve())\n",
    "INPUT_ = globals().get(\"INPUT\", {\n",
    "    \"TRAIN_IMAGES\": ROOT_ / \"train_images\",\n",
    "    \"TRAIN_ANN_DIR\": ROOT_ / \"train_annotations\",\n",
    "    \"TEST_IMAGES\": ROOT_ / \"test_images\",\n",
    "})\n",
    "WORK_ = globals().get(\"WORK\", {\"DATA\": ROOT_ / \"data\"})\n",
    "DIRS_ = globals().get(\"DIRS\", {})\n",
    "RUN_NAME_ = globals().get(\"RUN_NAME\", \"run\")\n",
    "\n",
    "CACHE_DIR = Path(globals().get(\"CACHE_DIR\", DIRS_.get(\"CACHE\", ROOT_ / \"data\" / \"cache\" / \"merged\")))\n",
    "MERGED_COCO = CACHE_DIR / \"train_merged_coco.json\"\n",
    "LABEL_MAP = CACHE_DIR / \"label_map_full.json\"\n",
    "SPLIT_JSON = CACHE_DIR / \"splits\" / \"split_train_valid.json\"\n",
    "\n",
    "if not MERGED_COCO.exists():\n",
    "    cand = list(CACHE_DIR.rglob(\"train_merged_coco.json\"))\n",
    "    if cand: MERGED_COCO = cand[0]\n",
    "    else: raise FileNotFoundError(f\"train_merged_coco.json not found in {CACHE_DIR}\")\n",
    "\n",
    "if not LABEL_MAP.exists():\n",
    "    cand = list(CACHE_DIR.rglob(\"label_map_full.json\"))\n",
    "    if cand: LABEL_MAP = cand[0]\n",
    "    else: raise FileNotFoundError(f\"label_map_full.json not found in {CACHE_DIR}\")\n",
    "\n",
    "if not SPLIT_JSON.exists():\n",
    "    cand = list(CACHE_DIR.rglob(\"split_train_valid.json\"))\n",
    "    if cand: SPLIT_JSON = cand[0]\n",
    "    else: raise FileNotFoundError(f\"split_train_valid.json not found in {CACHE_DIR}\")\n",
    "\n",
    "train_img_src = Path(INPUT_[\"TRAIN_IMAGES\"])\n",
    "if not train_img_src.exists():\n",
    "    raise FileNotFoundError(f\"train_images dir not found: {train_img_src}\")\n",
    "\n",
    "# -------------------------\n",
    "# 1) Load inputs\n",
    "# -------------------------\n",
    "with open(MERGED_COCO, \"r\", encoding=\"utf-8\") as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "with open(LABEL_MAP, \"r\", encoding=\"utf-8\") as f:\n",
    "    lm = json.load(f)\n",
    "\n",
    "with open(SPLIT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    split = json.load(f)\n",
    "\n",
    "images = coco.get(\"images\", [])\n",
    "anns = coco.get(\"annotations\", [])\n",
    "cats = coco.get(\"categories\", [])\n",
    "\n",
    "# id2idx: category_id -> class_index(0..nc-1)\n",
    "id2idx = {int(k): int(v) for k, v in lm.get(\"id2idx\", {}).items()}\n",
    "names = lm.get(\"names\", None)\n",
    "nc = int(lm.get(\"num_classes\", len(id2idx)))\n",
    "\n",
    "if names is None or len(names) != nc:\n",
    "    # fallback: categories에서 name을 id2idx 순으로 구성\n",
    "    cat_id_to_name = {int(c[\"id\"]): str(c[\"name\"]) for c in cats if isinstance(c, dict) and c.get(\"id\") is not None and c.get(\"name\") is not None}\n",
    "    names = [cat_id_to_name.get(cid, str(cid)) for cid in sorted(id2idx, key=lambda x: id2idx[x])]\n",
    "\n",
    "train_ids = [int(x) for x in split.get(\"train_image_ids\", [])]\n",
    "val_ids = [int(x) for x in split.get(\"valid_image_ids\", [])]\n",
    "train_set = set(train_ids)\n",
    "val_set = set(val_ids)\n",
    "\n",
    "# image_id -> meta\n",
    "img_by_id = {int(im[\"id\"]): im for im in images if isinstance(im, dict) and im.get(\"id\") is not None}\n",
    "\n",
    "# image_id -> annotations\n",
    "anns_by_img = defaultdict(list)\n",
    "for a in anns:\n",
    "    if not isinstance(a, dict): \n",
    "        continue\n",
    "    iid = a.get(\"image_id\", None)\n",
    "    if iid is None:\n",
    "        continue\n",
    "    anns_by_img[int(iid)].append(a)\n",
    "\n",
    "# -------------------------\n",
    "# 2) YOLO dataset dirs\n",
    "# -------------------------\n",
    "DATASET_ROOT = Path(WORK_[\"DATA\"]) / \"datasets\" / f\"pill_od_yolo_{RUN_NAME_}\"\n",
    "IMG_TRAIN_DIR = DATASET_ROOT / \"images\" / \"train\"\n",
    "IMG_VAL_DIR = DATASET_ROOT / \"images\" / \"val\"\n",
    "LBL_TRAIN_DIR = DATASET_ROOT / \"labels\" / \"train\"\n",
    "LBL_VAL_DIR = DATASET_ROOT / \"labels\" / \"val\"\n",
    "\n",
    "for p in [IMG_TRAIN_DIR, IMG_VAL_DIR, LBL_TRAIN_DIR, LBL_VAL_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# copy mode: 'copy' 기본 (232장이라 부담 적음). 필요하면 'symlink'로 바꿔도 됨.\n",
    "COPY_MODE = \"copy\"  # \"copy\" | \"symlink\"\n",
    "\n",
    "def put_image(src: Path, dst: Path):\n",
    "    if dst.exists():\n",
    "        return\n",
    "    if COPY_MODE == \"symlink\":\n",
    "        try:\n",
    "            dst.symlink_to(src.resolve())\n",
    "            return\n",
    "        except Exception:\n",
    "            # symlink 권한/환경 이슈면 copy로 fallback\n",
    "            shutil.copy2(src, dst)\n",
    "            return\n",
    "    shutil.copy2(src, dst)\n",
    "\n",
    "def xywh_to_yolo(bbox_xywh, W, H):\n",
    "    x, y, w, h = map(float, bbox_xywh)\n",
    "    # center\n",
    "    xc = (x + w / 2.0) / float(W)\n",
    "    yc = (y + h / 2.0) / float(H)\n",
    "    ww = w / float(W)\n",
    "    hh = h / float(H)\n",
    "    # clamp (안전)\n",
    "    def clamp01(v): \n",
    "        return max(0.0, min(1.0, v))\n",
    "    return clamp01(xc), clamp01(yc), clamp01(ww), clamp01(hh)\n",
    "\n",
    "# -------------------------\n",
    "# 3) Build split datasets\n",
    "# -------------------------\n",
    "stats = {\n",
    "    \"n_train_images\": 0,\n",
    "    \"n_val_images\": 0,\n",
    "    \"n_train_labels\": 0,\n",
    "    \"n_val_labels\": 0,\n",
    "    \"n_train_objects\": 0,\n",
    "    \"n_val_objects\": 0,\n",
    "    \"n_missing_images\": 0,\n",
    "    \"n_skipped_boxes\": 0,\n",
    "    \"n_empty_labels\": 0,  # 이 대회는 보통 0이 정상\n",
    "}\n",
    "\n",
    "def process_split(image_ids, img_dir, lbl_dir, split_name):\n",
    "    for iid in image_ids:\n",
    "        im = img_by_id.get(int(iid))\n",
    "        if im is None:\n",
    "            continue\n",
    "\n",
    "        file_name = str(im.get(\"file_name\"))\n",
    "        W = im.get(\"width\", None)\n",
    "        H = im.get(\"height\", None)\n",
    "        if W is None or H is None:\n",
    "            # width/height 없으면 변환 불가 -> skip\n",
    "            stats[\"n_skipped_boxes\"] += len(anns_by_img.get(int(iid), []))\n",
    "            continue\n",
    "\n",
    "        src_img = train_img_src / Path(file_name).name\n",
    "        if not src_img.exists():\n",
    "            stats[\"n_missing_images\"] += 1\n",
    "            continue\n",
    "\n",
    "        dst_img = img_dir / Path(file_name).name\n",
    "        put_image(src_img, dst_img)\n",
    "\n",
    "        # label file path: 이미지와 동일 stem\n",
    "        label_path = lbl_dir / (Path(file_name).stem + \".txt\")\n",
    "\n",
    "        lines = []\n",
    "        for a in anns_by_img.get(int(iid), []):\n",
    "            cid = a.get(\"category_id\", None)\n",
    "            bbox = a.get(\"bbox\", None)\n",
    "            if cid is None or bbox is None or (not isinstance(bbox, list)) or len(bbox) != 4:\n",
    "                stats[\"n_skipped_boxes\"] += 1\n",
    "                continue\n",
    "            cid = int(cid)\n",
    "            if cid not in id2idx:\n",
    "                stats[\"n_skipped_boxes\"] += 1\n",
    "                continue\n",
    "            cls = id2idx[cid]\n",
    "\n",
    "            x, y, w, h = map(float, bbox)\n",
    "            if w <= 0 or h <= 0:\n",
    "                stats[\"n_skipped_boxes\"] += 1\n",
    "                continue\n",
    "\n",
    "            xc, yc, ww, hh = xywh_to_yolo([x, y, w, h], W, H)\n",
    "\n",
    "            # ww/hh가 0에 너무 가까우면 skip\n",
    "            if ww <= 0 or hh <= 0:\n",
    "                stats[\"n_skipped_boxes\"] += 1\n",
    "                continue\n",
    "\n",
    "            lines.append(f\"{cls} {xc:.6f} {yc:.6f} {ww:.6f} {hh:.6f}\")\n",
    "\n",
    "        if not lines:\n",
    "            stats[\"n_empty_labels\"] += 1\n",
    "            label_path.write_text(\"\", encoding=\"utf-8\")\n",
    "        else:\n",
    "            label_path.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "        # stats\n",
    "        if split_name == \"train\":\n",
    "            stats[\"n_train_images\"] += 1\n",
    "            stats[\"n_train_labels\"] += 1\n",
    "            stats[\"n_train_objects\"] += len(lines)\n",
    "        else:\n",
    "            stats[\"n_val_images\"] += 1\n",
    "            stats[\"n_val_labels\"] += 1\n",
    "            stats[\"n_val_objects\"] += len(lines)\n",
    "\n",
    "process_split(train_ids, IMG_TRAIN_DIR, LBL_TRAIN_DIR, \"train\")\n",
    "process_split(val_ids, IMG_VAL_DIR, LBL_VAL_DIR, \"val\")\n",
    "\n",
    "# -------------------------\n",
    "# 4) data.yaml 생성 (Ultralytics)\n",
    "# -------------------------\n",
    "data_yaml = DATASET_ROOT / \"data.yaml\"\n",
    "yaml_text = []\n",
    "yaml_text.append(f\"path: {DATASET_ROOT.as_posix()}\")\n",
    "yaml_text.append(\"train: images/train\")\n",
    "yaml_text.append(\"val: images/val\")\n",
    "yaml_text.append(f\"nc: {nc}\")\n",
    "yaml_text.append(\"names:\")\n",
    "for i, n in enumerate(names):\n",
    "    # yaml 문자열 안전 처리\n",
    "    n2 = str(n).replace('\"', '\\\\\"')\n",
    "    yaml_text.append(f\"  {i}: \\\"{n2}\\\"\")\n",
    "\n",
    "data_yaml.write_text(\"\\n\".join(yaml_text) + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Sanity check (라벨 포맷 검사)\n",
    "# -------------------------\n",
    "def check_labels(lbl_dir: Path, nc: int, max_report=10):\n",
    "    bad = []\n",
    "    for p in sorted(lbl_dir.glob(\"*.txt\")):\n",
    "        txt = p.read_text(encoding=\"utf-8\").strip()\n",
    "        if not txt:\n",
    "            continue\n",
    "        for ln in txt.splitlines():\n",
    "            parts = ln.strip().split()\n",
    "            if len(parts) != 5:\n",
    "                bad.append((p.name, ln, \"len!=5\"))\n",
    "                continue\n",
    "            try:\n",
    "                c = int(parts[0])\n",
    "                vals = list(map(float, parts[1:]))\n",
    "            except Exception:\n",
    "                bad.append((p.name, ln, \"parse_error\"))\n",
    "                continue\n",
    "            if not (0 <= c < nc):\n",
    "                bad.append((p.name, ln, \"class_out_of_range\"))\n",
    "                continue\n",
    "            if any(v < 0 or v > 1 for v in vals):\n",
    "                bad.append((p.name, ln, \"val_out_of_0_1\"))\n",
    "                continue\n",
    "    return bad[:max_report], len(bad)\n",
    "\n",
    "bad_train, n_bad_train = check_labels(LBL_TRAIN_DIR, nc)\n",
    "bad_val, n_bad_val = check_labels(LBL_VAL_DIR, nc)\n",
    "\n",
    "# -------------------------\n",
    "# 6) Save conversion manifest\n",
    "# -------------------------\n",
    "manifest = {\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"dataset_root\": str(DATASET_ROOT),\n",
    "    \"copy_mode\": COPY_MODE,\n",
    "    \"merged_coco\": str(MERGED_COCO),\n",
    "    \"split_json\": str(SPLIT_JSON),\n",
    "    \"label_map\": str(LABEL_MAP),\n",
    "    \"stats\": stats,\n",
    "    \"sanity\": {\n",
    "        \"n_bad_train_lines\": n_bad_train,\n",
    "        \"n_bad_val_lines\": n_bad_val,\n",
    "        \"bad_train_examples\": bad_train,\n",
    "        \"bad_val_examples\": bad_val,\n",
    "    },\n",
    "}\n",
    "out_manifest = DATASET_ROOT / \"convert_manifest.json\"\n",
    "out_manifest.write_text(json.dumps(manifest, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "# -------------------------\n",
    "# 7) Print summary\n",
    "# -------------------------\n",
    "print(\"[YOLO DATASET READY]\")\n",
    "print(f\"- dataset_root : {DATASET_ROOT}\")\n",
    "print(f\"- data.yaml    : {data_yaml}\")\n",
    "print(f\"- nc           : {nc}\")\n",
    "print(f\"- copy_mode    : {COPY_MODE}\")\n",
    "\n",
    "print(\"\\n[COUNTS]\")\n",
    "print(f\"- train images : {stats['n_train_images']} | labels: {stats['n_train_labels']} | objects: {stats['n_train_objects']}\")\n",
    "print(f\"- val images   : {stats['n_val_images']}   | labels: {stats['n_val_labels']}   | objects: {stats['n_val_objects']}\")\n",
    "print(f\"- missing imgs : {stats['n_missing_images']}\")\n",
    "print(f\"- skipped box  : {stats['n_skipped_boxes']}\")\n",
    "print(f\"- empty labels : {stats['n_empty_labels']}\")\n",
    "\n",
    "print(\"\\n[SANITY]\")\n",
    "print(f\"- bad train label lines: {n_bad_train}\")\n",
    "if bad_train:\n",
    "    for ex in bad_train:\n",
    "        print(\"  -\", ex)\n",
    "print(f\"- bad val label lines  : {n_bad_val}\")\n",
    "if bad_val:\n",
    "    for ex in bad_val:\n",
    "        print(\"  -\", ex)\n",
    "\n",
    "print(\"\\n[SAVED]\")\n",
    "print(f\"- {out_manifest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "363903fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] data.yaml: C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\data\\datasets\\pill_od_yolo_exp_20260202_230604\\data.yaml\n",
      "\n",
      "[ENV]\n",
      "- torch: 2.5.1+cu121\n",
      "- cuda_available: True\n",
      "- cuda_version: 12.1\n",
      "- device_count: 1\n",
      "- device_name: NVIDIA GeForce RTX 3080\n",
      "- ultralytics: 8.4.9\n",
      "\n",
      "[DATA.YAML CHECK]\n",
      "- nc: 56\n",
      "- names count: 56\n",
      "\n",
      "[BASELINE PLAN]\n",
      "- model: yolov8s.pt\n",
      "- imgsz: 768\n",
      "- epochs: 100\n",
      "- batch: 8\n",
      "- patience: 30\n",
      "- device: 0\n",
      "- close_mosaic: 10\n",
      "- amp: True\n",
      "\n",
      "[OK] saved train plan -> C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\config\\train_plan_ultralytics_baseline.json\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 4-1. 베이스라인 학습 계획 고정(ultralytics YOLO) + 환경 점검 + train_plan 저장\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# 1) dataset yaml 경로 확보 (3-8에서 만든 DATASET_ROOT/data.yaml 우선)\n",
    "data_yaml = None\n",
    "if \"DATASET_ROOT\" in globals():\n",
    "    p = Path(globals()[\"DATASET_ROOT\"]) / \"data.yaml\"\n",
    "    if p.exists():\n",
    "        data_yaml = p\n",
    "\n",
    "# fallback: WORK[\"DATA\"]/datasets 아래에서 현재 RUN_NAME 포함한 폴더 탐색\n",
    "if data_yaml is None:\n",
    "    ROOT_ = globals().get(\"ROOT\", Path(\".\").resolve())\n",
    "    WORK_ = globals().get(\"WORK\", {\"DATA\": ROOT_ / \"data\"})\n",
    "    RUN_NAME_ = globals().get(\"RUN_NAME\", \"\")\n",
    "    ds_root = Path(WORK_[\"DATA\"]) / \"datasets\"\n",
    "    cands = sorted(ds_root.glob(f\"*{RUN_NAME_}*/data.yaml\")) if RUN_NAME_ else sorted(ds_root.rglob(\"data.yaml\"))\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"data.yaml을 찾지 못했습니다. datasets dir: {ds_root}\")\n",
    "    data_yaml = cands[0]\n",
    "\n",
    "print(f\"[OK] data.yaml: {data_yaml}\")\n",
    "\n",
    "# 2) torch/ultralytics 환경 점검\n",
    "torch_info = {}\n",
    "ultra_info = {}\n",
    "try:\n",
    "    import torch\n",
    "    torch_info = {\n",
    "        \"torch\": torch.__version__,\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "        \"cuda_version\": getattr(torch.version, \"cuda\", None),\n",
    "        \"device_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "        \"device_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() and torch.cuda.device_count() > 0 else None,\n",
    "    }\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] torch import failed: {e}\")\n",
    "\n",
    "try:\n",
    "    import ultralytics\n",
    "    ultra_info = {\n",
    "        \"ultralytics\": getattr(ultralytics, \"__version__\", None),\n",
    "    }\n",
    "except Exception as e:\n",
    "    print(f\"[WARN] ultralytics import failed: {e}\")\n",
    "\n",
    "print(\"\\n[ENV]\")\n",
    "for k, v in torch_info.items():\n",
    "    print(f\"- {k}: {v}\")\n",
    "for k, v in ultra_info.items():\n",
    "    print(f\"- {k}: {v}\")\n",
    "\n",
    "# 3) data.yaml 로드해서 nc/names 확인\n",
    "def load_yaml_like(path: Path):\n",
    "    # pyyaml 없이도 파싱 가능하지만, 있으면 yaml로 읽고 없으면 간단히 문자열만 사용\n",
    "    try:\n",
    "        import yaml\n",
    "        return yaml.safe_load(path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "data_cfg = load_yaml_like(data_yaml)\n",
    "if data_cfg:\n",
    "    nc = int(data_cfg.get(\"nc\", -1))\n",
    "    names = data_cfg.get(\"names\", None)\n",
    "    n_names = len(names) if isinstance(names, dict) else (len(names) if isinstance(names, list) else None)\n",
    "    print(\"\\n[DATA.YAML CHECK]\")\n",
    "    print(f\"- nc: {nc}\")\n",
    "    print(f\"- names count: {n_names}\")\n",
    "else:\n",
    "    print(\"\\n[WARN] yaml 파서가 없어서 data.yaml 내용 검증을 생략합니다. (PyYAML 설치 시 자동 검증)\")\n",
    "\n",
    "# 4) 베이스라인 학습 플랜 고정\n",
    "#   - 데이터가 작으니 일단 yolov8s + imgsz=768 기준으로 시작 (필요하면 m로 올림)\n",
    "BASELINE = {\n",
    "    \"framework\": \"ultralytics\",\n",
    "    \"task\": \"detect\",\n",
    "    \"model\": \"yolov8s.pt\",\n",
    "    \"data\": str(data_yaml),\n",
    "    \"imgsz\": 768,\n",
    "    \"epochs\": 100,\n",
    "    \"batch\": 8,\n",
    "    \"patience\": 30,\n",
    "    \"optimizer\": \"auto\",\n",
    "    \"lr0\": None,\n",
    "    \"lrf\": None,\n",
    "    \"weight_decay\": None,\n",
    "    \"workers\": 4,\n",
    "    \"device\": 0 if torch_info.get(\"cuda_available\") else \"cpu\",\n",
    "    \"seed\": int(globals().get(\"SEED\", 42)),\n",
    "    \"deterministic\": True,\n",
    "    \"cache\": False,\n",
    "    \"amp\": True,          # mixed precision (GPU 있을 때)\n",
    "    \"cos_lr\": False,\n",
    "    \"close_mosaic\": 10,   # 마지막 몇 epoch에서 mosaic off (박스 정밀도에 도움될 때 많음)\n",
    "    \"save\": True,\n",
    "    \"save_period\": -1,\n",
    "    \"plots\": True,\n",
    "    \"val\": True,\n",
    "}\n",
    "\n",
    "# 5) CFG 반영 + 저장\n",
    "if \"CFG\" in globals():\n",
    "    CFG[\"train\"][\"framework\"] = \"ultralytics_yolo\"\n",
    "    CFG[\"train\"][\"model\"][\"name\"] = \"yolov8s\"\n",
    "    CFG[\"train\"][\"model\"][\"imgsz\"] = BASELINE[\"imgsz\"]\n",
    "    CFG[\"train\"][\"hyperparams\"][\"epochs\"] = BASELINE[\"epochs\"]\n",
    "    CFG[\"train\"][\"hyperparams\"][\"batch\"] = BASELINE[\"batch\"]\n",
    "    CFG[\"train\"][\"hyperparams\"][\"workers\"] = BASELINE[\"workers\"]\n",
    "    CFG[\"infer\"][\"max_det_per_image\"] = 4\n",
    "    if \"save_cfg\" in globals():\n",
    "        save_cfg()\n",
    "\n",
    "# train plan 저장\n",
    "DIRS_ = globals().get(\"DIRS\", {})\n",
    "plan_dir = Path(DIRS_.get(\"CONFIG\", Path(\".\") / \"runs\" / \"config\"))\n",
    "plan_dir.mkdir(parents=True, exist_ok=True)\n",
    "train_plan_path = plan_dir / \"train_plan_ultralytics_baseline.json\"\n",
    "payload = {\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"run_name\": globals().get(\"RUN_NAME\", \"\"),\n",
    "    \"data_yaml\": str(data_yaml),\n",
    "    \"baseline\": BASELINE,\n",
    "    \"torch\": torch_info,\n",
    "    \"ultralytics\": ultra_info,\n",
    "}\n",
    "train_plan_path.write_text(json.dumps(payload, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\n[BASELINE PLAN]\")\n",
    "for k in [\"model\", \"imgsz\", \"epochs\", \"batch\", \"patience\", \"device\", \"close_mosaic\", \"amp\"]:\n",
    "    print(f\"- {k}: {BASELINE[k]}\")\n",
    "print(f\"\\n[OK] saved train plan -> {train_plan_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9755ff4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN START]\n",
      "- project: C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\ultralytics\n",
      "- name   : train_baseline\n",
      "- data   : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\data\\datasets\\pill_od_yolo_exp_20260202_230604\\data.yaml\n",
      "- model  : yolov8s.pt\n",
      "- imgsz/epochs/batch: 768 100 8\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolov8s.pt to 'yolov8s.pt': 100% ━━━━━━━━━━━━ 21.5MB 58.4MB/s 0.4s0.3s<0.1s\n",
      "Ultralytics 8.4.9  Python-3.11.9 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080, 10240MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=C:\\Users\\amy\\Desktop\\sprint\\ \\pjt-sprint_ai07_healthcare\\experiments\\DM2\\data\\datasets\\pill_od_yolo_exp_20260202_230604\\data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=768, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=train_baseline, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=30, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=C:\\Users\\amy\\Desktop\\sprint\\ \\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\ultralytics, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=C:\\Users\\amy\\Desktop\\sprint\\ \\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\ultralytics\\train_baseline, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=56\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2137720  ultralytics.nn.modules.head.Detect           [56, 16, None, [128, 256, 512]]\n",
      "Model summary: 130 layers, 11,157,272 parameters, 11,157,256 gradients, 28.8 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n.pt to 'yolo26n.pt': 100% ━━━━━━━━━━━━ 5.3MB 52.5MB/s 0.1s\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 179.916.6 MB/s, size: 1683.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\data\\datasets\\pill_od_yolo_exp_20260202_230604\\labels\\train... 185 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 185/185 550.5it/s 0.3s0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\amy\\Desktop\\sprint\\ \\pjt-sprint_ai07_healthcare\\experiments\\DM2\\data\\datasets\\pill_od_yolo_exp_20260202_230604\\labels\\train.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 135.619.7 MB/s, size: 1754.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\data\\datasets\\pill_od_yolo_exp_20260202_230604\\labels\\val... 47 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 47/47 436.9it/s 0.1s0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\amy\\Desktop\\sprint\\ \\pjt-sprint_ai07_healthcare\\experiments\\DM2\\data\\datasets\\pill_od_yolo_exp_20260202_230604\\labels\\val.cache\n",
      "Plotting labels to C:\\Users\\amy\\Desktop\\sprint\\ \\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\ultralytics\\train_baseline\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000167, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 768 train, 768 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\amy\\Desktop\\sprint\\ \\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\ultralytics\\train_baseline\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      1/100      2.73G      1.353      6.378      1.384          3        768: 100% ━━━━━━━━━━━━ 24/24 4.8it/s 5.0s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 3.8it/s 0.8s0.5s\n",
      "                   all         47        154      0.318      0.173      0.073     0.0569\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      2/100      3.14G     0.7063      3.734      1.043          4        768: 100% ━━━━━━━━━━━━ 24/24 8.7it/s 2.8s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 3.5it/s 0.8s0.6s\n",
      "                   all         47        154      0.756      0.192       0.28      0.261\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      3/100      3.14G     0.5946      2.929      0.991          4        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 7.4it/s 0.4s0.3s\n",
      "                   all         47        154      0.665       0.37      0.417       0.39\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      4/100      3.14G     0.5588      2.399     0.9776          6        768: 100% ━━━━━━━━━━━━ 24/24 9.0it/s 2.7s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.1it/s 0.4s0.2s\n",
      "                   all         47        154      0.682       0.38      0.488      0.458\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      5/100      3.14G     0.5282      2.159     0.9575          4        768: 100% ━━━━━━━━━━━━ 24/24 9.0it/s 2.7s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.4it/s 0.4s0.2s\n",
      "                   all         47        154       0.68      0.566      0.623      0.582\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      6/100      3.14G     0.4885      1.907     0.9337          4        768: 100% ━━━━━━━━━━━━ 24/24 8.8it/s 2.7s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.4it/s 0.4s0.2s\n",
      "                   all         47        154      0.647      0.678      0.716      0.683\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      7/100      3.14G     0.4796       1.66      0.919         11        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 7.9it/s 0.4s0.3s\n",
      "                   all         47        154      0.686      0.728      0.739      0.692\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      8/100      3.14G     0.4576      1.513     0.9183          4        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.9it/s 0.3s0.2s\n",
      "                   all         47        154      0.739      0.743      0.775       0.74\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      9/100      3.14G     0.4197      1.347     0.8923          3        768: 100% ━━━━━━━━━━━━ 24/24 9.4it/s 2.5s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.2it/s 0.3s0.2s\n",
      "                   all         47        154      0.744       0.78      0.825      0.795\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     10/100      3.14G     0.4465      1.247      0.912          6        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.0it/s 0.3s0.2s\n",
      "                   all         47        154      0.825      0.773      0.852      0.818\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     11/100      3.14G     0.4078      1.107     0.8877          7        768: 100% ━━━━━━━━━━━━ 24/24 9.3it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.7it/s 0.3s0.2s\n",
      "                   all         47        154      0.787      0.801      0.868       0.83\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     12/100      3.14G     0.3956      1.006     0.8806          6        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.7s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.8it/s 0.3s0.2s\n",
      "                   all         47        154      0.872      0.781      0.886      0.857\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     13/100      3.14G     0.3945     0.9384     0.8819         13        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.6it/s 0.3s0.2s\n",
      "                   all         47        154      0.832      0.833        0.9      0.866\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     14/100      3.14G     0.3951     0.8939     0.8866          5        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.2it/s 0.3s0.2s\n",
      "                   all         47        154      0.888      0.816      0.911      0.887\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     15/100      3.14G     0.4051     0.8795     0.9123          7        768: 100% ━━━━━━━━━━━━ 24/24 9.3it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.7it/s 0.3s0.2s\n",
      "                   all         47        154      0.868       0.86      0.915      0.879\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     16/100      3.14G     0.3991     0.8366     0.8853          4        768: 100% ━━━━━━━━━━━━ 24/24 9.3it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.6it/s 0.3s0.2s\n",
      "                   all         47        154      0.879      0.833      0.921      0.891\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     17/100      3.14G     0.3823     0.7805     0.8771          7        768: 100% ━━━━━━━━━━━━ 24/24 9.3it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.1it/s 0.3s0.2s\n",
      "                   all         47        154      0.814      0.858      0.924      0.901\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     18/100      3.14G     0.3422     0.7265     0.8574          3        768: 100% ━━━━━━━━━━━━ 24/24 9.3it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.5it/s 0.3s0.2s\n",
      "                   all         47        154      0.908      0.835       0.94      0.918\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     19/100      3.14G     0.3629     0.7353      0.868          6        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.7it/s 0.3s0.2s\n",
      "                   all         47        154      0.924       0.81      0.947      0.919\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     20/100      3.14G     0.3943     0.7481     0.8922          5        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.8it/s 0.3s0.2s\n",
      "                   all         47        154       0.92      0.826      0.958      0.936\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     21/100      3.14G     0.3639      0.708     0.8671          3        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.4it/s 0.3s0.2s\n",
      "                   all         47        154      0.905       0.89      0.959      0.931\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     22/100      3.14G     0.3727     0.7222      0.885          3        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.4it/s 0.3s0.2s\n",
      "                   all         47        154      0.867      0.917      0.958      0.928\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     23/100      3.14G     0.3558     0.6418     0.8621          4        768: 100% ━━━━━━━━━━━━ 24/24 8.9it/s 2.7s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.0it/s 0.4s0.3s\n",
      "                   all         47        154       0.93      0.851      0.956      0.924\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     24/100      3.14G     0.3524     0.6542     0.8701          5        768: 100% ━━━━━━━━━━━━ 24/24 8.9it/s 2.7s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.5it/s 0.3s0.2s\n",
      "                   all         47        154      0.924       0.86      0.947      0.924\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     25/100      3.14G      0.362     0.6831     0.8818          4        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.4it/s 0.3s0.2s\n",
      "                   all         47        154      0.924      0.854      0.957       0.93\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     26/100      3.14G      0.363     0.7257     0.8824          3        768: 100% ━━━━━━━━━━━━ 24/24 9.0it/s 2.7s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.0it/s 0.3s0.2s\n",
      "                   all         47        154      0.802      0.959      0.959      0.933\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     27/100      3.14G      0.351     0.5998     0.8645          8        768: 100% ━━━━━━━━━━━━ 24/24 9.0it/s 2.7s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.9it/s 0.3s0.2s\n",
      "                   all         47        154       0.81      0.948      0.962       0.94\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     28/100      3.14G     0.3303     0.6026     0.8587          4        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.8it/s 0.3s0.2s\n",
      "                   all         47        154      0.859      0.947       0.95      0.925\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     29/100      3.14G     0.3428     0.6171     0.8675          8        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.7it/s 0.3s0.2s\n",
      "                   all         47        154      0.908       0.91      0.946      0.924\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     30/100      3.14G     0.3346      0.536     0.8534          7        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.4it/s 0.3s0.2s\n",
      "                   all         47        154      0.875       0.92      0.947      0.914\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     31/100      3.14G     0.3569     0.5927     0.8762         15        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.7it/s 0.3s0.2s\n",
      "                   all         47        154       0.92       0.93      0.947      0.925\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     32/100      3.14G     0.3548     0.5289     0.8829          4        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.8it/s 0.3s0.2s\n",
      "                   all         47        154      0.881      0.936      0.947       0.93\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     33/100      3.14G     0.3511     0.5613     0.8707          4        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.4it/s 0.3s0.2s\n",
      "                   all         47        154      0.928      0.894      0.958       0.94\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     34/100      3.14G     0.3265     0.5134     0.8572         10        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.3it/s 0.3s0.2s\n",
      "                   all         47        154      0.923      0.892      0.958      0.937\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     35/100      3.14G      0.327     0.4818     0.8642          8        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.7it/s 0.3s0.2s\n",
      "                   all         47        154      0.864      0.944      0.958      0.938\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     36/100      3.14G     0.3288      0.502     0.8533         11        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.7it/s 0.3s0.2s\n",
      "                   all         47        154        0.9      0.943      0.946      0.925\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     37/100      3.14G     0.3229     0.5026     0.8557          5        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.4it/s 0.3s0.2s\n",
      "                   all         47        154      0.912      0.958      0.944      0.921\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     38/100      3.14G     0.3162     0.4898     0.8476          4        768: 100% ━━━━━━━━━━━━ 24/24 9.3it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.6it/s 0.3s0.2s\n",
      "                   all         47        154      0.903       0.91      0.944      0.924\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     39/100      3.14G     0.3154     0.4725     0.8633          4        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.6it/s 0.3s0.2s\n",
      "                   all         47        154      0.899       0.92      0.946      0.933\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     40/100      3.14G     0.3173     0.4523     0.8638          7        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.8it/s 0.3s0.2s\n",
      "                   all         47        154       0.87      0.944      0.959       0.94\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     41/100      3.14G     0.3131     0.4461     0.8493          4        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.4it/s 0.3s0.2s\n",
      "                   all         47        154      0.879      0.965      0.959      0.937\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     42/100      3.14G       0.31     0.4337     0.8477          4        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.2it/s 0.3s0.2s\n",
      "                   all         47        154      0.881       0.96      0.959      0.938\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     43/100      3.14G     0.3194     0.4747     0.8642          4        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.5it/s 0.3s0.2s\n",
      "                   all         47        154       0.91      0.958      0.947      0.932\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     44/100      3.14G     0.3287     0.4448     0.8543          7        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.4it/s 0.3s0.2s\n",
      "                   all         47        154      0.926      0.956      0.936      0.919\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     45/100      3.14G     0.3328     0.4521     0.8603          4        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.4it/s 0.3s0.2s\n",
      "                   all         47        154      0.931      0.954      0.935       0.92\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     46/100      3.14G     0.3345     0.4432      0.884          5        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.3it/s 0.3s0.2s\n",
      "                   all         47        154      0.899      0.959      0.935      0.922\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     47/100      3.14G      0.312     0.4381     0.8517          5        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.4it/s 0.3s0.2s\n",
      "                   all         47        154      0.883      0.966      0.951      0.939\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     48/100      3.14G     0.3061     0.4501      0.846          7        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.5it/s 0.3s0.2s\n",
      "                   all         47        154      0.899      0.971      0.951      0.936\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     49/100      3.14G     0.3081      0.444     0.8526          4        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.1it/s 0.3s0.2s\n",
      "                   all         47        154      0.902       0.96      0.947      0.929\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     50/100      3.14G     0.2841      0.395     0.8421          9        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.6it/s 0.3s0.2s\n",
      "                   all         47        154       0.91      0.961      0.947      0.931\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     51/100      3.14G      0.297     0.4477     0.8438         12        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.6it/s 0.3s0.2s\n",
      "                   all         47        154      0.909      0.969      0.936      0.922\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     52/100      3.14G     0.2881     0.3785     0.8457          5        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.5it/s 0.3s0.2s\n",
      "                   all         47        154      0.907      0.966      0.947      0.939\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     53/100      3.14G     0.3071     0.4243     0.8676          3        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.8it/s 0.3s0.2s\n",
      "                   all         47        154      0.894      0.967      0.938      0.928\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     54/100      3.14G     0.3141     0.4418     0.8561          7        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.9it/s 0.3s0.2s\n",
      "                   all         47        154      0.894      0.946      0.949       0.94\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     55/100      3.14G     0.3158     0.4326     0.8614          5        768: 100% ━━━━━━━━━━━━ 24/24 9.0it/s 2.7s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.1it/s 0.3s0.2s\n",
      "                   all         47        154      0.912       0.94      0.959      0.949\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     56/100      3.14G     0.2893     0.3903     0.8507          8        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.4it/s 0.3s0.2s\n",
      "                   all         47        154      0.876      0.972      0.959      0.952\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     57/100      3.14G     0.2927     0.3819     0.8534          4        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.2it/s 0.3s0.2s\n",
      "                   all         47        154      0.896      0.963       0.96      0.951\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     58/100      3.14G     0.2809     0.3798     0.8441          8        768: 100% ━━━━━━━━━━━━ 24/24 9.0it/s 2.7s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.8it/s 0.3s0.2s\n",
      "                   all         47        154      0.904      0.954       0.96      0.951\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     59/100      3.14G     0.2961     0.3708     0.8541          4        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.4it/s 0.3s0.2s\n",
      "                   all         47        154      0.906      0.951       0.96      0.952\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     60/100      3.14G     0.2934     0.3948     0.8477          2        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.2it/s 0.3s0.2s\n",
      "                   all         47        154      0.906      0.953       0.96      0.943\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     61/100      3.14G     0.2927     0.3655     0.8417         11        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.3it/s 0.3s0.2s\n",
      "                   all         47        154      0.904      0.962      0.953      0.941\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     62/100      3.14G     0.3004     0.4124     0.8501          3        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.4it/s 0.3s0.2s\n",
      "                   all         47        154      0.848       0.97      0.968      0.955\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     63/100      3.14G     0.2822     0.3645     0.8446         14        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.4it/s 0.3s0.2s\n",
      "                   all         47        154      0.897      0.938      0.956      0.944\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     64/100      3.14G     0.2664     0.3502      0.839          4        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.4it/s 0.3s0.2s\n",
      "                   all         47        154      0.915      0.934      0.957      0.947\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     65/100      3.14G     0.2837     0.3629      0.853         10        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.2it/s 0.3s0.2s\n",
      "                   all         47        154      0.912      0.934      0.957      0.947\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     66/100      3.14G     0.2653     0.3801      0.828          4        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.6it/s 0.3s0.2s\n",
      "                   all         47        154      0.908      0.959      0.953       0.94\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     67/100      3.14G     0.3012     0.4001     0.8539          7        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.4it/s 0.3s0.2s\n",
      "                   all         47        154      0.913      0.957      0.953      0.938\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     68/100      3.14G     0.2695     0.3406     0.8451          4        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.0it/s 0.3s0.2s\n",
      "                   all         47        154      0.913      0.958      0.953      0.941\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     69/100      3.14G     0.2772     0.3712     0.8386          5        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.1it/s 0.3s0.2s\n",
      "                   all         47        154      0.911      0.954      0.952      0.943\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     70/100      3.14G     0.2815     0.3565     0.8476          8        768: 100% ━━━━━━━━━━━━ 24/24 9.0it/s 2.7s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.3it/s 0.3s0.2s\n",
      "                   all         47        154      0.907      0.949      0.952      0.942\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     71/100      3.14G     0.2827     0.3509     0.8475          8        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.4it/s 0.3s0.2s\n",
      "                   all         47        154       0.91      0.951      0.952      0.942\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     72/100      3.14G     0.2713     0.3678     0.8375          7        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.5it/s 0.3s0.2s\n",
      "                   all         47        154       0.91      0.956      0.952       0.94\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     73/100      3.14G     0.2782     0.3567     0.8578          5        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.4it/s 0.3s0.2s\n",
      "                   all         47        154      0.909      0.959      0.952      0.937\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     74/100      3.14G     0.2838     0.3621     0.8472          2        768: 100% ━━━━━━━━━━━━ 24/24 9.1it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.5it/s 0.3s0.2s\n",
      "                   all         47        154      0.912      0.962      0.952      0.936\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     75/100      3.14G     0.2707     0.3515     0.8405          4        768: 100% ━━━━━━━━━━━━ 24/24 9.2it/s 2.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.2it/s 0.3s0.2s\n",
      "                   all         47        154      0.911      0.963       0.94      0.932\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     76/100      3.14G     0.2654     0.3352     0.8431         10        768: 100% ━━━━━━━━━━━━ 24/24 9.0it/s 2.7s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.5it/s 0.4s0.2s\n",
      "                   all         47        154        0.9      0.967       0.94      0.932\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     77/100      3.14G     0.2651     0.3362     0.8431          3        768: 100% ━━━━━━━━━━━━ 24/24 9.0it/s 2.7s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.6it/s 0.3s0.2s\n",
      "                   all         47        154      0.894      0.968      0.936       0.93\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     78/100      3.14G     0.2778     0.3728     0.8393          2        768: 100% ━━━━━━━━━━━━ 24/24 8.7it/s 2.8s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.7it/s 0.3s0.2s\n",
      "                   all         47        154      0.904      0.964      0.941      0.934\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     79/100      3.14G     0.2817     0.3449     0.8503          4        768: 100% ━━━━━━━━━━━━ 24/24 8.7it/s 2.8s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.7it/s 0.3s0.2s\n",
      "                   all         47        154      0.896      0.964      0.953      0.948\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     80/100      3.14G     0.2763     0.3387      0.837          4        768: 100% ━━━━━━━━━━━━ 24/24 8.7it/s 2.8s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.9it/s 0.3s0.2s\n",
      "                   all         47        154      0.902       0.96      0.953      0.947\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     81/100      3.14G     0.2657     0.3187     0.8428         11        768: 100% ━━━━━━━━━━━━ 24/24 8.6it/s 2.8s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.8it/s 0.3s0.2s\n",
      "                   all         47        154       0.91      0.958      0.953      0.946\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     82/100      3.14G     0.2709     0.3509     0.8398          3        768: 100% ━━━━━━━━━━━━ 24/24 8.8it/s 2.7s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.7it/s 0.3s0.2s\n",
      "                   all         47        154      0.911      0.956      0.952      0.944\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     83/100      3.14G     0.2665     0.3325     0.8404          3        768: 100% ━━━━━━━━━━━━ 24/24 8.6it/s 2.8s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.6it/s 0.3s0.2s\n",
      "                   all         47        154      0.912      0.956      0.948      0.943\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     84/100      3.14G     0.2493     0.3159     0.8304         11        768: 100% ━━━━━━━━━━━━ 24/24 8.7it/s 2.8s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.6it/s 0.3s0.2s\n",
      "                   all         47        154      0.912      0.956      0.952      0.949\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     85/100      3.14G     0.2643     0.3411       0.84         10        768: 100% ━━━━━━━━━━━━ 24/24 8.8it/s 2.7s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.8it/s 0.3s0.2s\n",
      "                   all         47        154      0.911      0.956      0.953      0.949\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     86/100      3.14G     0.2533     0.3262     0.8312          6        768: 100% ━━━━━━━━━━━━ 24/24 8.8it/s 2.7s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.7it/s 0.3s0.2s\n",
      "                   all         47        154      0.912      0.958      0.957      0.949\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     87/100      3.14G     0.2583     0.3288     0.8393          5        768: 100% ━━━━━━━━━━━━ 24/24 8.7it/s 2.8s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.8it/s 0.3s0.2s\n",
      "                   all         47        154      0.912      0.958      0.957      0.952\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     88/100      3.14G     0.2533     0.3139     0.8388          4        768: 100% ━━━━━━━━━━━━ 24/24 8.8it/s 2.7s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 9.0it/s 0.3s0.2s\n",
      "                   all         47        154      0.914      0.958      0.957      0.952\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     89/100      3.14G     0.2721     0.3441     0.8593          2        768: 100% ━━━━━━━━━━━━ 24/24 8.8it/s 2.7s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.6it/s 0.3s0.2s\n",
      "                   all         47        154      0.916      0.957      0.957      0.952\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     90/100      3.14G     0.2838     0.3413      0.855          4        768: 100% ━━━━━━━━━━━━ 24/24 8.6it/s 2.8s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.9it/s 0.3s0.2s\n",
      "                   all         47        154      0.915      0.957      0.955       0.95\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     91/100      3.14G     0.2108     0.2954     0.7899          4        768: 100% ━━━━━━━━━━━━ 24/24 7.4it/s 3.2s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.7it/s 0.3s0.2s\n",
      "                   all         47        154      0.916      0.959      0.955       0.95\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     92/100      3.14G     0.2024     0.2585     0.7904          3        768: 100% ━━━━━━━━━━━━ 24/24 8.9it/s 2.7s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 8.1it/s 0.4s0.2s\n",
      "                   all         47        154      0.912       0.96      0.955      0.951\n",
      "\u001b[34m\u001b[1mEarlyStopping: \u001b[0mTraining stopped early as no improvement observed in last 30 epochs. Best results observed at epoch 62, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=30) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
      "\n",
      "92 epochs completed in 0.095 hours.\n",
      "Optimizer stripped from C:\\Users\\amy\\Desktop\\sprint\\ \\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\ultralytics\\train_baseline\\weights\\last.pt, 22.6MB\n",
      "Optimizer stripped from C:\\Users\\amy\\Desktop\\sprint\\ \\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\ultralytics\\train_baseline\\weights\\best.pt, 22.6MB\n",
      "\n",
      "Validating C:\\Users\\amy\\Desktop\\sprint\\ \\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\ultralytics\\train_baseline\\weights\\best.pt...\n",
      "Ultralytics 8.4.9  Python-3.11.9 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3080, 10240MiB)\n",
      "Model summary (fused): 73 layers, 11,147,256 parameters, 0 gradients, 28.6 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 4.7it/s 0.6s0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48372 (\\N{HANGUL SYLLABLE BO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47161 (\\N{HANGUL SYLLABLE RYEONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48512 (\\N{HANGUL SYLLABLE BU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49828 (\\N{HANGUL SYLLABLE SEU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54028 (\\N{HANGUL SYLLABLE PA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51221 (\\N{HANGUL SYLLABLE JEONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48036 (\\N{HANGUL SYLLABLE MYU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53580 (\\N{HANGUL SYLLABLE TE}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 46976 (\\N{HANGUL SYLLABLE RAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 52897 (\\N{HANGUL SYLLABLE KAEB}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49808 (\\N{HANGUL SYLLABLE SYUL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51068 (\\N{HANGUL SYLLABLE IL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50577 (\\N{HANGUL SYLLABLE YANG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54616 (\\N{HANGUL SYLLABLE HA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53944 (\\N{HANGUL SYLLABLE TEU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47536 (\\N{HANGUL SYLLABLE RIN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 44592 (\\N{HANGUL SYLLABLE GI}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 45349 (\\N{HANGUL SYLLABLE NEG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49888 (\\N{HANGUL SYLLABLE SIN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50640 (\\N{HANGUL SYLLABLE E}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54532 (\\N{HANGUL SYLLABLE PEU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51008 (\\N{HANGUL SYLLABLE EUN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54665 (\\N{HANGUL SYLLABLE HAENG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50685 (\\N{HANGUL SYLLABLE YEOB}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50641 (\\N{HANGUL SYLLABLE EG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 52636 (\\N{HANGUL SYLLABLE CUL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50857 (\\N{HANGUL SYLLABLE YONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47924 (\\N{HANGUL SYLLABLE MU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53076 (\\N{HANGUL SYLLABLE KO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53440 (\\N{HANGUL SYLLABLE TA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47112 (\\N{HANGUL SYLLABLE RE}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48148 (\\N{HANGUL SYLLABLE BA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48120 (\\N{HANGUL SYLLABLE MI}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54588 (\\N{HANGUL SYLLABLE PI}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 46300 (\\N{HANGUL SYLLABLE DEU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48708 (\\N{HANGUL SYLLABLE BI}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47588 (\\N{HANGUL SYLLABLE MAE}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54408 (\\N{HANGUL SYLLABLE PUM}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50508 (\\N{HANGUL SYLLABLE AL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 45684 (\\N{HANGUL SYLLABLE NYU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47196 (\\N{HANGUL SYLLABLE RO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47700 (\\N{HANGUL SYLLABLE ME}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50725 (\\N{HANGUL SYLLABLE OG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49884 (\\N{HANGUL SYLLABLE SI}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 46972 (\\N{HANGUL SYLLABLE RA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49464 (\\N{HANGUL SYLLABLE SE}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53456 (\\N{HANGUL SYLLABLE TAM}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50612 (\\N{HANGUL SYLLABLE EO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53448 (\\N{HANGUL SYLLABLE TAL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50500 (\\N{HANGUL SYLLABLE A}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53364 (\\N{HANGUL SYLLABLE KEUL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54168 (\\N{HANGUL SYLLABLE PE}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 45209 (\\N{HANGUL SYLLABLE NAG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47532 (\\N{HANGUL SYLLABLE RI}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47113 (\\N{HANGUL SYLLABLE REG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54172 (\\N{HANGUL SYLLABLE PEN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48716 (\\N{HANGUL SYLLABLE BIL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 45796 (\\N{HANGUL SYLLABLE DA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48124 (\\N{HANGUL SYLLABLE MIN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53328 (\\N{HANGUL SYLLABLE KYU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48337 (\\N{HANGUL SYLLABLE BYEONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50024 (\\N{HANGUL SYLLABLE SSEO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 44036 (\\N{HANGUL SYLLABLE GAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49436 (\\N{HANGUL SYLLABLE SEO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48169 (\\N{HANGUL SYLLABLE BANG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48709 (\\N{HANGUL SYLLABLE BIG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49324 (\\N{HANGUL SYLLABLE SA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47564 (\\N{HANGUL SYLLABLE MAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54004 (\\N{HANGUL SYLLABLE TIN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50684 (\\N{HANGUL SYLLABLE YEOM}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49328 (\\N{HANGUL SYLLABLE SAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53664 (\\N{HANGUL SYLLABLE TO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53356 (\\N{HANGUL SYLLABLE KEU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 44032 (\\N{HANGUL SYLLABLE GA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 46041 (\\N{HANGUL SYLLABLE DONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50724 (\\N{HANGUL SYLLABLE O}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47560 (\\N{HANGUL SYLLABLE MA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50672 (\\N{HANGUL SYLLABLE YEON}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51656 (\\N{HANGUL SYLLABLE JIL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54008 (\\N{HANGUL SYLLABLE TIL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47476 (\\N{HANGUL SYLLABLE REU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 52852 (\\N{HANGUL SYLLABLE KA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51333 (\\N{HANGUL SYLLABLE JONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 44540 (\\N{HANGUL SYLLABLE GEUN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 45817 (\\N{HANGUL SYLLABLE DANG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 44544 (\\N{HANGUL SYLLABLE GEUL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54000 (\\N{HANGUL SYLLABLE TI}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53084 (\\N{HANGUL SYLLABLE KOL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54252 (\\N{HANGUL SYLLABLE PO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 45348 (\\N{HANGUL SYLLABLE NE}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47336 (\\N{HANGUL SYLLABLE RU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 45432 (\\N{HANGUL SYLLABLE NO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 46020 (\\N{HANGUL SYLLABLE DO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54540 (\\N{HANGUL SYLLABLE PEUL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51648 (\\N{HANGUL SYLLABLE JI}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50864 (\\N{HANGUL SYLLABLE U}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51088 (\\N{HANGUL SYLLABLE JA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 45572 (\\N{HANGUL SYLLABLE NU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47589 (\\N{HANGUL SYLLABLE MAEG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 45440 (\\N{HANGUL SYLLABLE NOL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53581 (\\N{HANGUL SYLLABLE TEG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47784 (\\N{HANGUL SYLLABLE MO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51096 (\\N{HANGUL SYLLABLE JAL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53444 (\\N{HANGUL SYLLABLE TAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50952 (\\N{HANGUL SYLLABLE WIN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 45208 (\\N{HANGUL SYLLABLE NA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48652 (\\N{HANGUL SYLLABLE BEU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50872 (\\N{HANGUL SYLLABLE UL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49483 (\\N{HANGUL SYLLABLE SES}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51320 (\\N{HANGUL SYLLABLE JOL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54392 (\\N{HANGUL SYLLABLE PU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51232 (\\N{HANGUL SYLLABLE JEN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47549 (\\N{HANGUL SYLLABLE RIB}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 46272 (\\N{HANGUL SYLLABLE DYU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 44600 (\\N{HANGUL SYLLABLE GIL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49892 (\\N{HANGUL SYLLABLE SIL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50896 (\\N{HANGUL SYLLABLE WEON}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50656 (\\N{HANGUL SYLLABLE EM}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51228 (\\N{HANGUL SYLLABLE JE}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51247 (\\N{HANGUL SYLLABLE JES}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48128 (\\N{HANGUL SYLLABLE MIL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 44536 (\\N{HANGUL SYLLABLE GEU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47016 (\\N{HANGUL SYLLABLE RAEM}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48156 (\\N{HANGUL SYLLABLE BAL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48372 (\\N{HANGUL SYLLABLE BO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47161 (\\N{HANGUL SYLLABLE RYEONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48512 (\\N{HANGUL SYLLABLE BU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49828 (\\N{HANGUL SYLLABLE SEU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54028 (\\N{HANGUL SYLLABLE PA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51221 (\\N{HANGUL SYLLABLE JEONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48036 (\\N{HANGUL SYLLABLE MYU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53580 (\\N{HANGUL SYLLABLE TE}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 46976 (\\N{HANGUL SYLLABLE RAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 52897 (\\N{HANGUL SYLLABLE KAEB}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49808 (\\N{HANGUL SYLLABLE SYUL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51068 (\\N{HANGUL SYLLABLE IL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50577 (\\N{HANGUL SYLLABLE YANG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54616 (\\N{HANGUL SYLLABLE HA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53944 (\\N{HANGUL SYLLABLE TEU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47536 (\\N{HANGUL SYLLABLE RIN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 44592 (\\N{HANGUL SYLLABLE GI}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 45349 (\\N{HANGUL SYLLABLE NEG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49888 (\\N{HANGUL SYLLABLE SIN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50640 (\\N{HANGUL SYLLABLE E}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54532 (\\N{HANGUL SYLLABLE PEU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51008 (\\N{HANGUL SYLLABLE EUN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54665 (\\N{HANGUL SYLLABLE HAENG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50685 (\\N{HANGUL SYLLABLE YEOB}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50641 (\\N{HANGUL SYLLABLE EG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 52636 (\\N{HANGUL SYLLABLE CUL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50857 (\\N{HANGUL SYLLABLE YONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47924 (\\N{HANGUL SYLLABLE MU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53076 (\\N{HANGUL SYLLABLE KO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53440 (\\N{HANGUL SYLLABLE TA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47112 (\\N{HANGUL SYLLABLE RE}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48148 (\\N{HANGUL SYLLABLE BA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48120 (\\N{HANGUL SYLLABLE MI}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54588 (\\N{HANGUL SYLLABLE PI}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 46300 (\\N{HANGUL SYLLABLE DEU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48708 (\\N{HANGUL SYLLABLE BI}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47588 (\\N{HANGUL SYLLABLE MAE}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54408 (\\N{HANGUL SYLLABLE PUM}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50508 (\\N{HANGUL SYLLABLE AL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 45684 (\\N{HANGUL SYLLABLE NYU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47196 (\\N{HANGUL SYLLABLE RO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47700 (\\N{HANGUL SYLLABLE ME}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50725 (\\N{HANGUL SYLLABLE OG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49884 (\\N{HANGUL SYLLABLE SI}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 46972 (\\N{HANGUL SYLLABLE RA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49464 (\\N{HANGUL SYLLABLE SE}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53456 (\\N{HANGUL SYLLABLE TAM}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50612 (\\N{HANGUL SYLLABLE EO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53448 (\\N{HANGUL SYLLABLE TAL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50500 (\\N{HANGUL SYLLABLE A}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53364 (\\N{HANGUL SYLLABLE KEUL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54168 (\\N{HANGUL SYLLABLE PE}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 45209 (\\N{HANGUL SYLLABLE NAG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47532 (\\N{HANGUL SYLLABLE RI}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47113 (\\N{HANGUL SYLLABLE REG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54172 (\\N{HANGUL SYLLABLE PEN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48716 (\\N{HANGUL SYLLABLE BIL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 45796 (\\N{HANGUL SYLLABLE DA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48124 (\\N{HANGUL SYLLABLE MIN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53328 (\\N{HANGUL SYLLABLE KYU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48337 (\\N{HANGUL SYLLABLE BYEONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50024 (\\N{HANGUL SYLLABLE SSEO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 44036 (\\N{HANGUL SYLLABLE GAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49436 (\\N{HANGUL SYLLABLE SEO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48169 (\\N{HANGUL SYLLABLE BANG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48709 (\\N{HANGUL SYLLABLE BIG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49324 (\\N{HANGUL SYLLABLE SA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47564 (\\N{HANGUL SYLLABLE MAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54004 (\\N{HANGUL SYLLABLE TIN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50684 (\\N{HANGUL SYLLABLE YEOM}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49328 (\\N{HANGUL SYLLABLE SAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53664 (\\N{HANGUL SYLLABLE TO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53356 (\\N{HANGUL SYLLABLE KEU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 44032 (\\N{HANGUL SYLLABLE GA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 46041 (\\N{HANGUL SYLLABLE DONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50724 (\\N{HANGUL SYLLABLE O}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47560 (\\N{HANGUL SYLLABLE MA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50672 (\\N{HANGUL SYLLABLE YEON}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51656 (\\N{HANGUL SYLLABLE JIL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54008 (\\N{HANGUL SYLLABLE TIL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47476 (\\N{HANGUL SYLLABLE REU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 52852 (\\N{HANGUL SYLLABLE KA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51333 (\\N{HANGUL SYLLABLE JONG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 44540 (\\N{HANGUL SYLLABLE GEUN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 45817 (\\N{HANGUL SYLLABLE DANG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 44544 (\\N{HANGUL SYLLABLE GEUL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54000 (\\N{HANGUL SYLLABLE TI}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53084 (\\N{HANGUL SYLLABLE KOL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54252 (\\N{HANGUL SYLLABLE PO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 45348 (\\N{HANGUL SYLLABLE NE}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47336 (\\N{HANGUL SYLLABLE RU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 45432 (\\N{HANGUL SYLLABLE NO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 46020 (\\N{HANGUL SYLLABLE DO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54540 (\\N{HANGUL SYLLABLE PEUL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51648 (\\N{HANGUL SYLLABLE JI}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50864 (\\N{HANGUL SYLLABLE U}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51088 (\\N{HANGUL SYLLABLE JA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 45572 (\\N{HANGUL SYLLABLE NU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47589 (\\N{HANGUL SYLLABLE MAEG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 45440 (\\N{HANGUL SYLLABLE NOL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53581 (\\N{HANGUL SYLLABLE TEG}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47784 (\\N{HANGUL SYLLABLE MO}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51096 (\\N{HANGUL SYLLABLE JAL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 53444 (\\N{HANGUL SYLLABLE TAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50952 (\\N{HANGUL SYLLABLE WIN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 45208 (\\N{HANGUL SYLLABLE NA}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48652 (\\N{HANGUL SYLLABLE BEU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50872 (\\N{HANGUL SYLLABLE UL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49483 (\\N{HANGUL SYLLABLE SES}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51320 (\\N{HANGUL SYLLABLE JOL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 54392 (\\N{HANGUL SYLLABLE PU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51232 (\\N{HANGUL SYLLABLE JEN}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47549 (\\N{HANGUL SYLLABLE RIB}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 46272 (\\N{HANGUL SYLLABLE DYU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 44600 (\\N{HANGUL SYLLABLE GIL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 49892 (\\N{HANGUL SYLLABLE SIL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50896 (\\N{HANGUL SYLLABLE WEON}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 50656 (\\N{HANGUL SYLLABLE EM}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51228 (\\N{HANGUL SYLLABLE JE}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 51247 (\\N{HANGUL SYLLABLE JES}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48128 (\\N{HANGUL SYLLABLE MIL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 44536 (\\N{HANGUL SYLLABLE GEU}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 47016 (\\N{HANGUL SYLLABLE RAEM}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "c:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\venv\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:569: UserWarning: Glyph 48156 (\\N{HANGUL SYLLABLE BAL}) missing from font(s) DejaVu Sans.\n",
      "  fig.savefig(plot_fname, dpi=250)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         47        154       0.85       0.97      0.968      0.955\n",
      "             5mg          6          6      0.958          1      0.995      0.995\n",
      "            100mg          1          1       0.83          1      0.995      0.995\n",
      "            2mg         32         32      0.966      0.882      0.989      0.989\n",
      "    ()()          6          6      0.957          1      0.995      0.969\n",
      "     ()()          2          2      0.888          1      0.995      0.995\n",
      "                            1          1      0.834          1      0.995      0.995\n",
      "          ()          4          4      0.934          1      0.995      0.995\n",
      "          ()          2          2      0.944          1      0.995      0.995\n",
      "          10mg/          1          1      0.838          1      0.995      0.895\n",
      "     8 650mg          1          1      0.965          1      0.995      0.995\n",
      "              20mg          5          5       0.95          1      0.995      0.995\n",
      "             20mg          5          5      0.946          1      0.995      0.967\n",
      "            100mg          6          6      0.798      0.833      0.832      0.832\n",
      "          800mg          2          2       0.89          1      0.995      0.995\n",
      "(-3-90)          3          3      0.959          1      0.995      0.914\n",
      "            150mg          4          4      0.939          1      0.995      0.959\n",
      "()          4          4      0.934          1      0.995      0.995\n",
      "        400mg          1          1      0.844          1      0.995      0.995\n",
      "              10mg          3          3      0.918          1      0.995      0.995\n",
      "              5mg          1          1      0.814          1      0.995      0.995\n",
      "                            2          2      0.612          1      0.995      0.995\n",
      "          5/160mg          1          1      0.868          1      0.995      0.995\n",
      "             10mg          7          7      0.959          1      0.995      0.995\n",
      "             20mg          1          1      0.844          1      0.995      0.995\n",
      "             50mg          2          2      0.889          1      0.995      0.995\n",
      "               10mg          1          1      0.244          1      0.995      0.995\n",
      "         50/850mg          1          1      0.835          1      0.995      0.995\n",
      "          5/100mg          4          4      0.889          1      0.995      0.995\n",
      "            100mg          2          2      0.889          1      0.995      0.995\n",
      "          ()          1          1       0.85          1      0.995      0.995\n",
      "          500/20mg          3          3      0.924          1      0.995      0.941\n",
      "               4mg          3          3      0.919          1      0.995      0.995\n",
      "      2.5/850mg          1          1      0.833          1      0.995      0.995\n",
      "       ()          4          4      0.936          1      0.995      0.995\n",
      "                       1          1       0.45          1      0.995      0.895\n",
      "                            3          3          0          0          0          0\n",
      "            20mg          2          2      0.918          1      0.995      0.995\n",
      "        ()          4          4      0.961          1      0.995      0.995\n",
      "      50/1000mg          1          1      0.832          1      0.995      0.895\n",
      "           10/40mg          6          6      0.954          1      0.995      0.995\n",
      "          10/5          3          3      0.924          1      0.995      0.995\n",
      "         10/20mg          5          5      0.947          1      0.995      0.995\n",
      "             25mg          6          6      0.967          1      0.995      0.952\n",
      "Speed: 0.9ms preprocess, 5.1ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\amy\\Desktop\\sprint\\ \\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\ultralytics\\train_baseline\u001b[0m\n",
      "\n",
      "[TRAIN DONE]\n",
      "- exp_dir : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\ultralytics\\train_baseline\n",
      "- best.pt : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\ultralytics\\train_baseline\\weights\\best.pt | copied -> C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\checkpoints\\best.pt\n",
      "- last.pt : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\ultralytics\\train_baseline\\weights\\last.pt | copied -> C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\checkpoints\\last.pt\n",
      "- mAP_75_95: 0.9506\n",
      "- mAP_50   : 0.95502\n",
      "- mAP_75   : None\n",
      "- saved    : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\logs\\train_baseline_out.json\n",
      "[OK] recorded -> results.csv | baseline_yolov8s_img768 (val)\n",
      "[OK] jsonl    -> results.jsonl\n",
      "[OK] summary  -> latest_summary.md\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 4-2. (Ultralytics) 베이스라인 학습 실행 + best/last 경로 수집 + 결과 기록(results.csv/jsonl)\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# --- 필수 경로/설정 로드 ---\n",
    "DIRS_ = globals().get(\"DIRS\", {})\n",
    "RUN_DIR = Path(DIRS_[\"RUN_DIR\"])\n",
    "CKPT_DIR = Path(DIRS_[\"CKPT\"])\n",
    "LOGS_DIR = Path(DIRS_[\"LOGS\"])\n",
    "CONFIG_DIR = Path(DIRS_[\"CONFIG\"])\n",
    "\n",
    "TRAIN_PLAN_PATH = CONFIG_DIR / \"train_plan_ultralytics_baseline.json\"\n",
    "if not TRAIN_PLAN_PATH.exists():\n",
    "    raise FileNotFoundError(f\"train plan not found: {TRAIN_PLAN_PATH}\")\n",
    "\n",
    "train_plan = json.loads(TRAIN_PLAN_PATH.read_text(encoding=\"utf-8\"))\n",
    "BASELINE = train_plan[\"baseline\"]\n",
    "\n",
    "# data.yaml 경로\n",
    "DATA_YAML = Path(train_plan[\"data_yaml\"])\n",
    "if not DATA_YAML.exists():\n",
    "    raise FileNotFoundError(f\"data.yaml not found: {DATA_YAML}\")\n",
    "\n",
    "# --- ultralytics import ---\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# --- 학습 결과를 이 RUN 폴더로 강제 저장 ---\n",
    "# Ultralytics는 project/name으로 runs를 만들기 때문에, project=RUN_DIR, name=\"ultra_train\"로 고정\n",
    "PROJECT_DIR = RUN_DIR / \"ultralytics\"\n",
    "EXP_NAME = \"train_baseline\"\n",
    "\n",
    "# 학습 파라미터 구성\n",
    "train_kwargs = dict(\n",
    "    data=str(DATA_YAML),\n",
    "    imgsz=int(BASELINE[\"imgsz\"]),\n",
    "    epochs=int(BASELINE[\"epochs\"]),\n",
    "    batch=int(BASELINE[\"batch\"]),\n",
    "    patience=int(BASELINE[\"patience\"]),\n",
    "    workers=int(BASELINE[\"workers\"]),\n",
    "    device=BASELINE[\"device\"],\n",
    "    seed=int(BASELINE[\"seed\"]),\n",
    "    deterministic=bool(BASELINE[\"deterministic\"]),\n",
    "    amp=bool(BASELINE[\"amp\"]),\n",
    "    close_mosaic=int(BASELINE[\"close_mosaic\"]),\n",
    "    plots=bool(BASELINE[\"plots\"]),\n",
    "    val=bool(BASELINE[\"val\"]),\n",
    "    save=bool(BASELINE[\"save\"]),\n",
    "    save_period=int(BASELINE[\"save_period\"]),\n",
    "    cache=bool(BASELINE[\"cache\"]),\n",
    "    project=str(PROJECT_DIR),\n",
    "    name=str(EXP_NAME),\n",
    ")\n",
    "\n",
    "# 옵션값(None)은 ultralytics에 넘기지 않기(오류 방지)\n",
    "for k in [\"lr0\", \"lrf\", \"weight_decay\", \"optimizer\", \"cos_lr\"]:\n",
    "    v = BASELINE.get(k, None)\n",
    "    if v is not None:\n",
    "        train_kwargs[k] = v\n",
    "\n",
    "# --- 이벤트 로그 ---\n",
    "if \"log_event\" in globals():\n",
    "    log_event(\"train_start\", {\"exp\": EXP_NAME, \"project\": str(PROJECT_DIR), \"kwargs\": train_kwargs})\n",
    "\n",
    "print(\"[TRAIN START]\")\n",
    "print(\"- project:\", PROJECT_DIR)\n",
    "print(\"- name   :\", EXP_NAME)\n",
    "print(\"- data   :\", DATA_YAML)\n",
    "print(\"- model  :\", BASELINE[\"model\"])\n",
    "print(\"- imgsz/epochs/batch:\", BASELINE[\"imgsz\"], BASELINE[\"epochs\"], BASELINE[\"batch\"])\n",
    "\n",
    "# --- run ---\n",
    "model = YOLO(BASELINE[\"model\"])\n",
    "results = model.train(**train_kwargs)\n",
    "\n",
    "# --- 결과 디렉터리 탐색 ---\n",
    "# 보통: PROJECT_DIR/EXP_NAME/\n",
    "exp_dir = PROJECT_DIR / EXP_NAME\n",
    "if not exp_dir.exists():\n",
    "    # 혹시 ultralytics가 name을 자동 변경했을 경우를 대비\n",
    "    cands = sorted(PROJECT_DIR.glob(f\"{EXP_NAME}*\"))\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"ultralytics exp dir not found under {PROJECT_DIR}\")\n",
    "    exp_dir = cands[-1]\n",
    "\n",
    "weights_dir = exp_dir / \"weights\"\n",
    "best_pt = weights_dir / \"best.pt\"\n",
    "last_pt = weights_dir / \"last.pt\"\n",
    "\n",
    "# best/last를 우리 CKPT_DIR로 복사(또는 overwrite)\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def safe_copy(src: Path, dst: Path):\n",
    "    if not src.exists():\n",
    "        return False\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    dst.write_bytes(src.read_bytes())\n",
    "    return True\n",
    "\n",
    "best_dst = CKPT_DIR / \"best.pt\"\n",
    "last_dst = CKPT_DIR / \"last.pt\"\n",
    "copied_best = safe_copy(best_pt, best_dst)\n",
    "copied_last = safe_copy(last_pt, last_dst)\n",
    "\n",
    "# --- metrics 요약 수집 ---\n",
    "# results는 ultralytics 버전에 따라 구조가 달라서, 파일 기반으로도 수집\n",
    "metrics = {}\n",
    "results_csv = exp_dir / \"results.csv\"\n",
    "if results_csv.exists():\n",
    "    # 가장 마지막 epoch row를 읽어서 key metric 추출\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(results_csv)\n",
    "    if len(df) > 0:\n",
    "        last = df.iloc[-1].to_dict()\n",
    "        # 가능한 키들(버전별로 다를 수 있음)\n",
    "        # map50, map50-95 같은 컬럼명을 우선 시도\n",
    "        for k in [\"metrics/mAP50-95(B)\", \"metrics/mAP50(B)\", \"metrics/mAP75(B)\",\n",
    "                  \"metrics/mAP50-95\", \"metrics/mAP50\", \"metrics/mAP75\",\n",
    "                  \"val/box_map\", \"val/box_map50\", \"val/box_map75\"]:\n",
    "            if k in last:\n",
    "                metrics[k] = float(last[k])\n",
    "        # loss도 남김(있으면)\n",
    "        for k in [\"train/box_loss\", \"train/cls_loss\", \"train/dfl_loss\",\n",
    "                  \"val/box_loss\", \"val/cls_loss\", \"val/dfl_loss\"]:\n",
    "            if k in last:\n",
    "                metrics[k] = float(last[k])\n",
    "\n",
    "# 표준화된 키로도 한 번 넣기(있으면)\n",
    "# - 대회 지표: mAP@[0.75:0.95] => mAP_75_95 로 기록\n",
    "mAP_75_95 = None\n",
    "mAP_50 = None\n",
    "mAP_75 = None\n",
    "for k, v in metrics.items():\n",
    "    if \"mAP50-95\" in k:\n",
    "        mAP_75_95 = v\n",
    "    elif \"mAP50\" in k and \"mAP50-95\" not in k:\n",
    "        mAP_50 = v\n",
    "    elif \"mAP75\" in k:\n",
    "        mAP_75 = v\n",
    "\n",
    "std_metrics = {\n",
    "    \"mAP_75_95\": mAP_75_95,\n",
    "    \"mAP_50\": mAP_50,\n",
    "    \"mAP_75\": mAP_75,\n",
    "    \"exp_dir\": str(exp_dir),\n",
    "    \"best_pt\": str(best_pt) if best_pt.exists() else None,\n",
    "    \"last_pt\": str(last_pt) if last_pt.exists() else None,\n",
    "    \"best_copied_to\": str(best_dst) if copied_best else None,\n",
    "    \"last_copied_to\": str(last_dst) if copied_last else None,\n",
    "    \"results_csv\": str(results_csv) if results_csv.exists() else None,\n",
    "}\n",
    "\n",
    "# --- artifacts 저장 ---\n",
    "train_out = {\n",
    "    \"finished_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"exp_dir\": str(exp_dir),\n",
    "    \"train_kwargs\": train_kwargs,\n",
    "    \"std_metrics\": std_metrics,\n",
    "    \"raw_metrics\": metrics,\n",
    "    \"weights\": {\n",
    "        \"best_pt\": str(best_pt) if best_pt.exists() else None,\n",
    "        \"last_pt\": str(last_pt) if last_pt.exists() else None,\n",
    "    },\n",
    "}\n",
    "\n",
    "out_path = LOGS_DIR / \"train_baseline_out.json\"\n",
    "out_path.write_text(json.dumps(train_out, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\n[TRAIN DONE]\")\n",
    "print(\"- exp_dir :\", exp_dir)\n",
    "print(\"- best.pt :\", best_pt, \"| copied ->\", best_dst if copied_best else \"(missing)\")\n",
    "print(\"- last.pt :\", last_pt, \"| copied ->\", last_dst if copied_last else \"(missing)\")\n",
    "print(\"- mAP_75_95:\", mAP_75_95)\n",
    "print(\"- mAP_50   :\", mAP_50)\n",
    "print(\"- mAP_75   :\", mAP_75)\n",
    "print(\"- saved    :\", out_path)\n",
    "\n",
    "# --- results table 기록 (2-4에서 만든 record_result 사용) ---\n",
    "if \"record_result\" in globals():\n",
    "    record_result(\n",
    "        result_name=\"baseline_yolov8s_img768\",\n",
    "        stage=\"val\",\n",
    "        metrics={\n",
    "            \"mAP_75_95\": mAP_75_95,\n",
    "            \"mAP_50\": mAP_50,\n",
    "            \"mAP_75\": mAP_75,\n",
    "            **{f\"ultra.{k}\": v for k, v in metrics.items()},\n",
    "        },\n",
    "        notes=\"Ultralytics YOLOv8s baseline (imgsz=768, close_mosaic=10, amp=True)\",\n",
    "        submission_path=None,\n",
    "    )\n",
    "\n",
    "# --- 이벤트 로그 ---\n",
    "if \"log_event\" in globals():\n",
    "    log_event(\"train_end\", {\"exp_dir\": str(exp_dir), \"std_metrics\": std_metrics})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee3c08e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WHITELIST] not found -> will submit across all trained classes (56).\n",
      "\n",
      "[INFER+SUBMISSION DONE]\n",
      "- test images processed : 842\n",
      "- det count (after whitelist filter, before topk): 8088\n",
      "- det count (after topk=4)                : 3367\n",
      "- rows in submission   : 3367\n",
      "- whitelist filtered   : 0\n",
      "- bad class idx skipped: 0\n",
      "- per-image >4 rows: 0\n",
      "- bad rows(bw/bh/score): 0\n",
      "\n",
      "[SAVED]\n",
      "- raw preds jsonl: C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\\submissions\\pred_test_raw_exp_20260202_230604.jsonl\n",
      "- submission csv : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\\submissions\\submission_exp_20260202_230604_best.csv\n",
      "[OK] recorded -> results.csv | submission_best_top4 (kaggle_submit)\n",
      "[OK] jsonl    -> results.jsonl\n",
      "[OK] summary  -> latest_summary.md\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 5-1. test 추론(best.pt) + Top-4 후처리 + submission.csv 생성/검증\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Paths / config\n",
    "# -----------------------------\n",
    "ROOT_ = globals().get(\"ROOT\", Path(\".\").resolve())\n",
    "INPUT_ = globals().get(\"INPUT\", {\n",
    "    \"TRAIN_IMAGES\": ROOT_ / \"train_images\",\n",
    "    \"TRAIN_ANN_DIR\": ROOT_ / \"train_annotations\",\n",
    "    \"TEST_IMAGES\": ROOT_ / \"test_images\",\n",
    "})\n",
    "DIRS_ = globals().get(\"DIRS\", {})\n",
    "RUN_NAME_ = globals().get(\"RUN_NAME\", \"run\")\n",
    "CFG_ = globals().get(\"CFG\", {})\n",
    "\n",
    "TEST_DIR = Path(INPUT_[\"TEST_IMAGES\"])\n",
    "if not TEST_DIR.exists():\n",
    "    raise FileNotFoundError(f\"test_images not found: {TEST_DIR}\")\n",
    "\n",
    "CKPT_DIR = Path(DIRS_[\"CKPT\"])\n",
    "BEST_PT = CKPT_DIR / \"best.pt\"\n",
    "if not BEST_PT.exists():\n",
    "    raise FileNotFoundError(f\"best.pt not found: {BEST_PT}\")\n",
    "\n",
    "SUB_DIR = Path(DIRS_[\"SUBMISSIONS\"])\n",
    "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CACHE_DIR = Path(globals().get(\"CACHE_DIR\", DIRS_.get(\"CACHE\", ROOT_ / \"data\" / \"cache\" / \"merged\")))\n",
    "LABEL_MAP = CACHE_DIR / \"label_map_full.json\"\n",
    "if not LABEL_MAP.exists():\n",
    "    cand = list(CACHE_DIR.rglob(\"label_map_full.json\"))\n",
    "    if cand:\n",
    "        LABEL_MAP = cand[0]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"label_map_full.json not found under {CACHE_DIR}\")\n",
    "\n",
    "# infer settings\n",
    "CONF_THR = float(CFG_.get(\"infer\", {}).get(\"conf_thr\", 0.001))\n",
    "NMS_IOU = float(CFG_.get(\"infer\", {}).get(\"nms_iou_thr\", 0.5))\n",
    "TOPK = int(CFG_.get(\"postprocess\", {}).get(\"topk\", 4))\n",
    "MAX_DET = max(50, TOPK)  # raw는 넉넉히 받고 TopK로 자르기\n",
    "IMGSZ = int(CFG_.get(\"train\", {}).get(\"model\", {}).get(\"imgsz\", 768))\n",
    "\n",
    "# -----------------------------\n",
    "# 1) label map: class_index -> category_id\n",
    "# -----------------------------\n",
    "with open(LABEL_MAP, \"r\", encoding=\"utf-8\") as f:\n",
    "    lm = json.load(f)\n",
    "\n",
    "idx2id = {int(k): int(v) for k, v in lm.get(\"idx2id\", {}).items()}\n",
    "nc = int(lm.get(\"num_classes\", len(idx2id)))\n",
    "if len(idx2id) != nc:\n",
    "    # idx2id가 dict가 아닐 수도 있어서 보정\n",
    "    idx2id = {int(i): int(cid) for i, cid in enumerate(lm.get(\"category_ids\", []))}\n",
    "    nc = len(idx2id)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) (optional) whitelist 로드: test 40 클래스만 남기기\n",
    "# -----------------------------\n",
    "def _load_whitelist():\n",
    "    candidates = [\n",
    "        ROOT_ / \"data\" / \"test_class_whitelist.json\",\n",
    "        ROOT_ / \"data\" / \"test_classes_40.json\",\n",
    "        ROOT_ / \"data\" / \"whitelist_40.txt\",\n",
    "        ROOT_ / \"data\" / \"whitelist.txt\",\n",
    "        CACHE_DIR / \"test_class_whitelist.json\",\n",
    "        CACHE_DIR / \"test_classes_40.json\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if not p.exists():\n",
    "            continue\n",
    "        if p.suffix.lower() == \".txt\":\n",
    "            vals = []\n",
    "            for line in p.read_text(encoding=\"utf-8\").splitlines():\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    vals.append(int(line))\n",
    "            return sorted(set(vals)), str(p)\n",
    "        if p.suffix.lower() == \".json\":\n",
    "            obj = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "            if isinstance(obj, list):\n",
    "                return sorted(set(int(x) for x in obj)), str(p)\n",
    "            if isinstance(obj, dict):\n",
    "                key = \"whitelist\" if \"whitelist\" in obj else (\"classes\" if \"classes\" in obj else None)\n",
    "                if key and isinstance(obj[key], list):\n",
    "                    return sorted(set(int(x) for x in obj[key])), str(p)\n",
    "    return None, None\n",
    "\n",
    "WHITELIST_IDS, WHITELIST_PATH = _load_whitelist()\n",
    "WHITELIST_SET = set(WHITELIST_IDS) if WHITELIST_IDS else None\n",
    "\n",
    "if WHITELIST_SET:\n",
    "    print(f\"[WHITELIST] loaded {len(WHITELIST_SET)} ids from: {WHITELIST_PATH}\")\n",
    "else:\n",
    "    print(\"[WHITELIST] not found -> will submit across all trained classes (56).\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) helper: filename -> image_id(int)\n",
    "# -----------------------------\n",
    "def parse_image_id(path: Path) -> int:\n",
    "    s = path.stem\n",
    "    try:\n",
    "        return int(s)\n",
    "    except Exception:\n",
    "        raise ValueError(f\"image file stem must be int for submission. got: {path.name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Inference (Ultralytics)\n",
    "# -----------------------------\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(str(BEST_PT))\n",
    "\n",
    "raw_jsonl = SUB_DIR / f\"pred_test_raw_{RUN_NAME_}.jsonl\"\n",
    "sub_csv = SUB_DIR / f\"submission_{RUN_NAME_}_best.csv\"\n",
    "\n",
    "# Kaggle 제출 포맷 헤더\n",
    "header = [\"annotation_id\", \"image_id\", \"category_id\", \"bbox_x\", \"bbox_y\", \"bbox_w\", \"bbox_h\", \"score\"]\n",
    "\n",
    "ann_id = 1\n",
    "n_images = 0\n",
    "n_det_raw = 0\n",
    "n_det_after = 0\n",
    "n_filtered_whitelist = 0\n",
    "bad_class_idx = 0\n",
    "\n",
    "# raw 저장(jsonl) + submission(csv) 동시 생성\n",
    "with open(raw_jsonl, \"w\", encoding=\"utf-8\") as f_raw, open(sub_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f_csv:\n",
    "    w = csv.writer(f_csv)\n",
    "    w.writerow(header)\n",
    "\n",
    "    results_stream = model.predict(\n",
    "        source=str(TEST_DIR),\n",
    "        conf=CONF_THR,\n",
    "        iou=NMS_IOU,\n",
    "        imgsz=IMGSZ,\n",
    "        max_det=MAX_DET,\n",
    "        stream=True,\n",
    "        verbose=False,\n",
    "        device=0,  # GPU\n",
    "    )\n",
    "\n",
    "    for r in results_stream:\n",
    "        # r.path: 이미지 경로\n",
    "        img_path = Path(r.path)\n",
    "        image_id = parse_image_id(img_path)\n",
    "        n_images += 1\n",
    "\n",
    "        # boxes: xyxy + conf + cls\n",
    "        boxes = r.boxes\n",
    "        dets = []\n",
    "        if boxes is not None and len(boxes) > 0:\n",
    "            xyxy = boxes.xyxy.cpu().numpy()\n",
    "            conf = boxes.conf.cpu().numpy()\n",
    "            cls = boxes.cls.cpu().numpy()\n",
    "\n",
    "            for (x1, y1, x2, y2), sc, ci in zip(xyxy, conf, cls):\n",
    "                ci = int(ci)\n",
    "                if ci not in idx2id:\n",
    "                    bad_class_idx += 1\n",
    "                    continue\n",
    "                cat_id = idx2id[ci]\n",
    "\n",
    "                # whitelist filter (optional)\n",
    "                if WHITELIST_SET is not None and cat_id not in WHITELIST_SET:\n",
    "                    n_filtered_whitelist += 1\n",
    "                    continue\n",
    "\n",
    "                x1 = float(x1); y1 = float(y1); x2 = float(x2); y2 = float(y2)\n",
    "                bw = max(0.0, x2 - x1)\n",
    "                bh = max(0.0, y2 - y1)\n",
    "\n",
    "                dets.append({\n",
    "                    \"category_id\": int(cat_id),\n",
    "                    \"bbox_x\": x1,\n",
    "                    \"bbox_y\": y1,\n",
    "                    \"bbox_w\": bw,\n",
    "                    \"bbox_h\": bh,\n",
    "                    \"score\": float(sc),\n",
    "                })\n",
    "\n",
    "        n_det_raw += len(dets)\n",
    "\n",
    "        # Top-K by score (대회는 image당 최대 4개)\n",
    "        dets = sorted(dets, key=lambda d: d[\"score\"], reverse=True)[:TOPK]\n",
    "        n_det_after += len(dets)\n",
    "\n",
    "        # raw jsonl 저장(이미지 단위)\n",
    "        f_raw.write(json.dumps({\n",
    "            \"image_id\": image_id,\n",
    "            \"file_name\": img_path.name,\n",
    "            \"n_dets_raw_after_filter\": len(dets),\n",
    "            \"dets\": dets,\n",
    "        }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        # submission rows\n",
    "        for d in dets:\n",
    "            w.writerow([\n",
    "                ann_id,\n",
    "                image_id,\n",
    "                d[\"category_id\"],\n",
    "                int(round(d[\"bbox_x\"])),\n",
    "                int(round(d[\"bbox_y\"])),\n",
    "                int(round(d[\"bbox_w\"])),\n",
    "                int(round(d[\"bbox_h\"])),\n",
    "                float(d[\"score\"]),\n",
    "            ])\n",
    "            ann_id += 1\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Submission sanity check\n",
    "# -----------------------------\n",
    "# - 각 image_id 당 row <= 4\n",
    "# - header/컬럼수 정상\n",
    "# - bbox_w/h >=0\n",
    "from collections import Counter\n",
    "\n",
    "per_img = Counter()\n",
    "n_rows = 0\n",
    "bad_rows = 0\n",
    "\n",
    "with open(sub_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "    rdr = csv.DictReader(f)\n",
    "    if rdr.fieldnames != header:\n",
    "        raise ValueError(f\"submission header mismatch.\\nexpected={header}\\ngot={rdr.fieldnames}\")\n",
    "\n",
    "    for row in rdr:\n",
    "        n_rows += 1\n",
    "        iid = int(row[\"image_id\"])\n",
    "        per_img[iid] += 1\n",
    "\n",
    "        bw = float(row[\"bbox_w\"])\n",
    "        bh = float(row[\"bbox_h\"])\n",
    "        sc = float(row[\"score\"])\n",
    "        if bw < 0 or bh < 0 or sc < 0:\n",
    "            bad_rows += 1\n",
    "\n",
    "too_many = sum(1 for iid, c in per_img.items() if c > TOPK)\n",
    "\n",
    "print(\"\\n[INFER+SUBMISSION DONE]\")\n",
    "print(f\"- test images processed : {n_images}\")\n",
    "print(f\"- det count (after whitelist filter, before topk): {n_det_raw}\")\n",
    "print(f\"- det count (after topk={TOPK})                : {n_det_after}\")\n",
    "print(f\"- rows in submission   : {n_rows}\")\n",
    "print(f\"- whitelist filtered   : {n_filtered_whitelist}\")\n",
    "print(f\"- bad class idx skipped: {bad_class_idx}\")\n",
    "print(f\"- per-image >{TOPK} rows: {too_many}\")\n",
    "print(f\"- bad rows(bw/bh/score): {bad_rows}\")\n",
    "\n",
    "print(\"\\n[SAVED]\")\n",
    "print(f\"- raw preds jsonl: {raw_jsonl}\")\n",
    "print(f\"- submission csv : {sub_csv}\")\n",
    "\n",
    "# results 기록(있으면)\n",
    "if \"record_result\" in globals():\n",
    "    record_result(\n",
    "        result_name=\"submission_best_top4\",\n",
    "        stage=\"kaggle_submit\",\n",
    "        metrics={\n",
    "            \"conf_thr\": CONF_THR,\n",
    "            \"nms_iou_thr\": NMS_IOU,\n",
    "            \"topk\": TOPK,\n",
    "            \"n_images\": n_images,\n",
    "            \"n_rows\": n_rows,\n",
    "            \"whitelist_used\": bool(WHITELIST_SET),\n",
    "            \"whitelist_n\": len(WHITELIST_SET) if WHITELIST_SET else 0,\n",
    "        },\n",
    "        notes=\"best.pt inference on test_images -> top4 -> submission csv\",\n",
    "        submission_path=str(sub_csv),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1821da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CACHE BUILD] start\n",
      "[CACHE BUILD] done\n",
      "- cache_jsonl      : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\\submissions\\pred_test_cache_full_exp_20260202_230604.jsonl\n",
      "- test images      : 842\n",
      "- total dets(saved): 8088\n",
      "- bad class idx    : 0\n",
      "\n",
      "[CONF SWEEP] generate submissions\n",
      "- conf=0.001 | rows=3367 | per-image>4=0 | submission_exp_20260202_230604_best_top4_conf0.001.csv\n",
      "- conf=0.010 | rows=3254 | per-image>4=0 | submission_exp_20260202_230604_best_top4_conf0.010.csv\n",
      "- conf=0.020 | rows=3247 | per-image>4=0 | submission_exp_20260202_230604_best_top4_conf0.020.csv\n",
      "- conf=0.030 | rows=3244 | per-image>4=0 | submission_exp_20260202_230604_best_top4_conf0.030.csv\n",
      "- conf=0.050 | rows=3235 | per-image>4=0 | submission_exp_20260202_230604_best_top4_conf0.050.csv\n",
      "[OK] recorded -> results.csv | submission_best_top4_conf0.001 (kaggle_submit)\n",
      "[OK] jsonl    -> results.jsonl\n",
      "[OK] summary  -> latest_summary.md\n",
      "[OK] recorded -> results.csv | submission_best_top4_conf0.010 (kaggle_submit)\n",
      "[OK] jsonl    -> results.jsonl\n",
      "[OK] summary  -> latest_summary.md\n",
      "[OK] recorded -> results.csv | submission_best_top4_conf0.020 (kaggle_submit)\n",
      "[OK] jsonl    -> results.jsonl\n",
      "[OK] summary  -> latest_summary.md\n",
      "[OK] recorded -> results.csv | submission_best_top4_conf0.030 (kaggle_submit)\n",
      "[OK] jsonl    -> results.jsonl\n",
      "[OK] summary  -> latest_summary.md\n",
      "[OK] recorded -> results.csv | submission_best_top4_conf0.050 (kaggle_submit)\n",
      "[OK] jsonl    -> results.jsonl\n",
      "[OK] summary  -> latest_summary.md\n",
      "\n",
      "[DONE] 이제 위에서 생성된 submission_* 파일 중 하나를 골라 Kaggle에 제출하면 됩니다.\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 5-2. (whitelist 없이) test 예측 캐시 저장 + conf_thr sweep로 여러 submission 생성\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "ROOT_ = globals().get(\"ROOT\", Path(\".\").resolve())\n",
    "INPUT_ = globals().get(\"INPUT\", {\n",
    "    \"TRAIN_IMAGES\": ROOT_ / \"train_images\",\n",
    "    \"TRAIN_ANN_DIR\": ROOT_ / \"train_annotations\",\n",
    "    \"TEST_IMAGES\": ROOT_ / \"test_images\",\n",
    "})\n",
    "DIRS_ = globals().get(\"DIRS\", {})\n",
    "RUN_NAME_ = globals().get(\"RUN_NAME\", \"run\")\n",
    "CFG_ = globals().get(\"CFG\", {})\n",
    "\n",
    "TEST_DIR = Path(INPUT_[\"TEST_IMAGES\"])\n",
    "if not TEST_DIR.exists():\n",
    "    raise FileNotFoundError(f\"test_images not found: {TEST_DIR}\")\n",
    "\n",
    "CKPT_DIR = Path(DIRS_[\"CKPT\"])\n",
    "BEST_PT = CKPT_DIR / \"best.pt\"\n",
    "if not BEST_PT.exists():\n",
    "    raise FileNotFoundError(f\"best.pt not found: {BEST_PT}\")\n",
    "\n",
    "SUB_DIR = Path(DIRS_[\"SUBMISSIONS\"])\n",
    "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CACHE_DIR = Path(globals().get(\"CACHE_DIR\", DIRS_.get(\"CACHE\", ROOT_ / \"data\" / \"cache\" / \"merged\")))\n",
    "LABEL_MAP = CACHE_DIR / \"label_map_full.json\"\n",
    "if not LABEL_MAP.exists():\n",
    "    cand = list(CACHE_DIR.rglob(\"label_map_full.json\"))\n",
    "    if cand:\n",
    "        LABEL_MAP = cand[0]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"label_map_full.json not found under {CACHE_DIR}\")\n",
    "\n",
    "# infer base settings (NMS/IMGSZ는 캐시 생성에만 영향)\n",
    "NMS_IOU = float(CFG_.get(\"infer\", {}).get(\"nms_iou_thr\", 0.5))\n",
    "IMGSZ = int(CFG_.get(\"train\", {}).get(\"model\", {}).get(\"imgsz\", 768))\n",
    "\n",
    "# 캐시 만들 때는 conf 낮게 + max_det 크게\n",
    "CACHE_CONF = 0.001\n",
    "CACHE_MAX_DET = 200\n",
    "\n",
    "# 이후 sweep에서 쓸 후보 conf들\n",
    "CONF_CANDIDATES = [0.001, 0.01, 0.02, 0.03, 0.05]\n",
    "\n",
    "TOPK = int(CFG_.get(\"postprocess\", {}).get(\"topk\", 4))\n",
    "\n",
    "# -----------------------------\n",
    "# 1) label map: class_index -> category_id\n",
    "# -----------------------------\n",
    "lm = json.loads(LABEL_MAP.read_text(encoding=\"utf-8\"))\n",
    "idx2id = {int(k): int(v) for k, v in lm.get(\"idx2id\", {}).items()}\n",
    "if not idx2id:\n",
    "    idx2id = {int(i): int(cid) for i, cid in enumerate(lm.get(\"category_ids\", []))}\n",
    "\n",
    "# -----------------------------\n",
    "# 2) helpers\n",
    "# -----------------------------\n",
    "def parse_image_id(path: Path) -> int:\n",
    "    try:\n",
    "        return int(path.stem)\n",
    "    except Exception:\n",
    "        raise ValueError(f\"image file stem must be int for submission. got: {path.name}\")\n",
    "\n",
    "def write_submission_from_cache(cache_jsonl: Path, conf_thr: float, topk: int, out_csv: Path):\n",
    "    header = [\"annotation_id\", \"image_id\", \"category_id\", \"bbox_x\", \"bbox_y\", \"bbox_w\", \"bbox_h\", \"score\"]\n",
    "\n",
    "    ann_id = 1\n",
    "    per_img = Counter()\n",
    "    n_rows = 0\n",
    "\n",
    "    with open(cache_jsonl, \"r\", encoding=\"utf-8\") as f_in, open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f_out:\n",
    "        w = csv.writer(f_out)\n",
    "        w.writerow(header)\n",
    "\n",
    "        for line in f_in:\n",
    "            obj = json.loads(line)\n",
    "            image_id = int(obj[\"image_id\"])\n",
    "            dets = obj.get(\"dets\", [])\n",
    "\n",
    "            # conf 필터 + topk\n",
    "            dets2 = [d for d in dets if float(d[\"score\"]) >= conf_thr]\n",
    "            dets2 = sorted(dets2, key=lambda d: d[\"score\"], reverse=True)[:topk]\n",
    "\n",
    "            for d in dets2:\n",
    "                w.writerow([\n",
    "                    ann_id,\n",
    "                    image_id,\n",
    "                    int(d[\"category_id\"]),\n",
    "                    int(round(float(d[\"bbox_x\"]))),\n",
    "                    int(round(float(d[\"bbox_y\"]))),\n",
    "                    int(round(float(d[\"bbox_w\"]))),\n",
    "                    int(round(float(d[\"bbox_h\"]))),\n",
    "                    float(d[\"score\"]),\n",
    "                ])\n",
    "                ann_id += 1\n",
    "                n_rows += 1\n",
    "                per_img[image_id] += 1\n",
    "\n",
    "    too_many = sum(1 for _, c in per_img.items() if c > topk)\n",
    "    return {\"rows\": n_rows, \"too_many\": too_many}\n",
    "\n",
    "# -----------------------------\n",
    "# 3) 캐시 생성 (한 번만)\n",
    "# -----------------------------\n",
    "from ultralytics import YOLO\n",
    "\n",
    "cache_jsonl = SUB_DIR / f\"pred_test_cache_full_{RUN_NAME_}.jsonl\"\n",
    "need_build_cache = True\n",
    "if cache_jsonl.exists():\n",
    "    # 이미 만들어둔 캐시가 있으면 재사용\n",
    "    need_build_cache = False\n",
    "\n",
    "if need_build_cache:\n",
    "    print(\"[CACHE BUILD] start\")\n",
    "    model = YOLO(str(BEST_PT))\n",
    "\n",
    "    n_images = 0\n",
    "    n_dets = 0\n",
    "    bad_class_idx = 0\n",
    "\n",
    "    with open(cache_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
    "        stream = model.predict(\n",
    "            source=str(TEST_DIR),\n",
    "            conf=CACHE_CONF,\n",
    "            iou=NMS_IOU,\n",
    "            imgsz=IMGSZ,\n",
    "            max_det=CACHE_MAX_DET,\n",
    "            stream=True,\n",
    "            verbose=False,\n",
    "            device=0,\n",
    "        )\n",
    "\n",
    "        for r in stream:\n",
    "            img_path = Path(r.path)\n",
    "            image_id = parse_image_id(img_path)\n",
    "            n_images += 1\n",
    "\n",
    "            dets = []\n",
    "            boxes = r.boxes\n",
    "            if boxes is not None and len(boxes) > 0:\n",
    "                xyxy = boxes.xyxy.cpu().numpy()\n",
    "                conf = boxes.conf.cpu().numpy()\n",
    "                cls = boxes.cls.cpu().numpy()\n",
    "\n",
    "                for (x1, y1, x2, y2), sc, ci in zip(xyxy, conf, cls):\n",
    "                    ci = int(ci)\n",
    "                    if ci not in idx2id:\n",
    "                        bad_class_idx += 1\n",
    "                        continue\n",
    "                    cat_id = idx2id[ci]\n",
    "                    x1 = float(x1); y1 = float(y1); x2 = float(x2); y2 = float(y2)\n",
    "                    bw = max(0.0, x2 - x1)\n",
    "                    bh = max(0.0, y2 - y1)\n",
    "                    dets.append({\n",
    "                        \"category_id\": int(cat_id),\n",
    "                        \"bbox_x\": x1,\n",
    "                        \"bbox_y\": y1,\n",
    "                        \"bbox_w\": bw,\n",
    "                        \"bbox_h\": bh,\n",
    "                        \"score\": float(sc),\n",
    "                    })\n",
    "\n",
    "            # 점수 내림차순 정렬해서 캐시에 저장(offline sweep 효율)\n",
    "            dets = sorted(dets, key=lambda d: d[\"score\"], reverse=True)\n",
    "            n_dets += len(dets)\n",
    "\n",
    "            f.write(json.dumps({\n",
    "                \"image_id\": image_id,\n",
    "                \"file_name\": img_path.name,\n",
    "                \"n_dets\": len(dets),\n",
    "                \"dets\": dets,\n",
    "            }, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(\"[CACHE BUILD] done\")\n",
    "    print(f\"- cache_jsonl      : {cache_jsonl}\")\n",
    "    print(f\"- test images      : {n_images}\")\n",
    "    print(f\"- total dets(saved): {n_dets}\")\n",
    "    print(f\"- bad class idx    : {bad_class_idx}\")\n",
    "else:\n",
    "    print(\"[CACHE BUILD] skipped (already exists)\")\n",
    "    print(f\"- cache_jsonl: {cache_jsonl}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) conf sweep -> submissions\n",
    "# -----------------------------\n",
    "print(\"\\n[CONF SWEEP] generate submissions\")\n",
    "made = []\n",
    "for conf_thr in CONF_CANDIDATES:\n",
    "    out_csv = SUB_DIR / f\"submission_{RUN_NAME_}_best_top{TOPK}_conf{conf_thr:.3f}.csv\"\n",
    "    stat = write_submission_from_cache(cache_jsonl, conf_thr=conf_thr, topk=TOPK, out_csv=out_csv)\n",
    "    made.append((conf_thr, out_csv, stat[\"rows\"], stat[\"too_many\"]))\n",
    "    print(f\"- conf={conf_thr:.3f} | rows={stat['rows']} | per-image>{TOPK}={stat['too_many']} | {out_csv.name}\")\n",
    "\n",
    "# results 기록(있으면): 가장 기본(conf=0.02)만 남겨도 됨\n",
    "if \"record_result\" in globals():\n",
    "    for conf_thr, out_csv, n_rows, too_many in made:\n",
    "        record_result(\n",
    "            result_name=f\"submission_best_top{TOPK}_conf{conf_thr:.3f}\",\n",
    "            stage=\"kaggle_submit\",\n",
    "            metrics={\n",
    "                \"conf_thr\": conf_thr,\n",
    "                \"nms_iou_thr\": NMS_IOU,\n",
    "                \"topk\": TOPK,\n",
    "                \"rows\": n_rows,\n",
    "                \"per_image_too_many\": too_many,\n",
    "                \"cache_conf\": CACHE_CONF,\n",
    "                \"cache_max_det\": CACHE_MAX_DET,\n",
    "            },\n",
    "            notes=\"offline conf sweep from cached test predictions (no whitelist)\",\n",
    "            submission_path=str(out_csv),\n",
    "        )\n",
    "\n",
    "print(\"\\n[DONE] 이제 위에서 생성된 submission_* 파일 중 하나를 골라 Kaggle에 제출하면 됩니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79a29e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FILES]\n",
      "- 0.020: submission_exp_20260202_230604_best_top4_conf0.020.csv rows= 3247\n",
      "- 0.030: submission_exp_20260202_230604_best_top4_conf0.030.csv rows= 3244\n",
      "\n",
      "[EXACT SAME?] False\n",
      "\n",
      "[DIFF SUMMARY]\n",
      "- diff image_ids: 3 / 842\n",
      "- first 20 diff ids: [238, 857, 1421]\n",
      "\n",
      "--- image_id=238 ---\n",
      "[0.020]\n",
      " image_id  category_id  bbox_x  bbox_y  bbox_w  bbox_h    score\n",
      "      238        16262     626     240     244     235 0.991336\n",
      "      238         3351     403     834     190     190 0.918899\n",
      "      238        18357      68     262     302     297 0.529229\n",
      "      238        33880      66     259     303     300 0.021516\n",
      "[0.030]\n",
      " image_id  category_id  bbox_x  bbox_y  bbox_w  bbox_h    score\n",
      "      238        16262     626     240     244     235 0.991336\n",
      "      238         3351     403     834     190     190 0.918899\n",
      "      238        18357      68     262     302     297 0.529229\n",
      "\n",
      "--- image_id=857 ---\n",
      "[0.020]\n",
      " image_id  category_id  bbox_x  bbox_y  bbox_w  bbox_h    score\n",
      "      857        31863     576     746     207     212 0.989198\n",
      "      857        16262      92     679     249     245 0.978526\n",
      "      857         3351     338     133     184     188 0.952642\n",
      "      857         3351     574     746     210     214 0.022617\n",
      "[0.030]\n",
      " image_id  category_id  bbox_x  bbox_y  bbox_w  bbox_h    score\n",
      "      857        31863     576     746     207     212 0.989198\n",
      "      857        16262      92     679     249     245 0.978526\n",
      "      857         3351     338     133     184     188 0.952642\n",
      "\n",
      "--- image_id=1421 ---\n",
      "[0.020]\n",
      " image_id  category_id  bbox_x  bbox_y  bbox_w  bbox_h    score\n",
      "     1421        41768     554     722     302     280 0.979144\n",
      "     1421        16688     107     508     288     680 0.942209\n",
      "     1421         3351     337     164     184     189 0.938733\n",
      "     1421        12778       0     531      31     749 0.025343\n",
      "[0.030]\n",
      " image_id  category_id  bbox_x  bbox_y  bbox_w  bbox_h    score\n",
      "     1421        41768     554     722     302     280 0.979144\n",
      "     1421        16688     107     508     288     680 0.942209\n",
      "     1421         3351     337     164     184     189 0.938733\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 5-2b. submission 파일 diff (conf 0.020 vs 0.030)\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "SUB_DIR = Path(DIRS_[\"SUBMISSIONS\"])\n",
    "f020 = SUB_DIR / \"submission_exp_20260202_230604_best_top4_conf0.020.csv\"\n",
    "f030 = SUB_DIR / \"submission_exp_20260202_230604_best_top4_conf0.030.csv\"\n",
    "\n",
    "assert f020.exists(), f\"not found: {f020}\"\n",
    "assert f030.exists(), f\"not found: {f030}\"\n",
    "\n",
    "def load_norm(p: Path):\n",
    "    df = pd.read_csv(p)\n",
    "    # 비교 안정화를 위해 정렬 + 타입 통일\n",
    "    df = df.sort_values([\"image_id\", \"score\", \"category_id\", \"bbox_x\", \"bbox_y\", \"bbox_w\", \"bbox_h\"], ascending=[True, False, True, True, True, True, True]).reset_index(drop=True)\n",
    "    # annotation_id는 의미 없으니 제거\n",
    "    df = df.drop(columns=[\"annotation_id\"])\n",
    "    return df\n",
    "\n",
    "a = load_norm(f020)\n",
    "b = load_norm(f030)\n",
    "\n",
    "print(\"[FILES]\")\n",
    "print(\"- 0.020:\", f020.name, \"rows=\", len(a))\n",
    "print(\"- 0.030:\", f030.name, \"rows=\", len(b))\n",
    "\n",
    "# 전체가 같은지\n",
    "same = a.equals(b)\n",
    "print(\"\\n[EXACT SAME?]\", same)\n",
    "\n",
    "# 다르면 어디가 다른지(이미지 단위)\n",
    "if not same:\n",
    "    # image_id별로 문자열로 묶어서 비교(간단/확실)\n",
    "    def pack(df):\n",
    "        g = df.groupby(\"image_id\").apply(lambda x: \"|\".join(x.astype(str).agg(\",\".join, axis=1))).to_dict()\n",
    "        return g\n",
    "    pa = pack(a)\n",
    "    pb = pack(b)\n",
    "\n",
    "    all_ids = sorted(set(pa.keys()) | set(pb.keys()))\n",
    "    diff_ids = [iid for iid in all_ids if pa.get(iid, \"\") != pb.get(iid, \"\")]\n",
    "    print(\"\\n[DIFF SUMMARY]\")\n",
    "    print(\"- diff image_ids:\", len(diff_ids), \"/\", len(all_ids))\n",
    "    print(\"- first 20 diff ids:\", diff_ids[:20])\n",
    "\n",
    "    # 몇 개 예시 출력\n",
    "    for iid in diff_ids[:5]:\n",
    "        print(f\"\\n--- image_id={iid} ---\")\n",
    "        print(\"[0.020]\")\n",
    "        print(a[a[\"image_id\"] == iid].to_string(index=False))\n",
    "        print(\"[0.030]\")\n",
    "        print(b[b[\"image_id\"] == iid].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "faa35b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SWEEP START] (conf fixed at 0.02)\n",
      "- iou=0.40 imgsz=768 | rows=3247 | per-image>4=0 | submission_exp_20260202_230604_best_top4_conf0.020_iou0.40_img768.csv\n",
      "- iou=0.40 imgsz=896 | rows=3242 | per-image>4=0 | submission_exp_20260202_230604_best_top4_conf0.020_iou0.40_img896.csv\n",
      "- iou=0.40 imgsz=1024 | rows=3248 | per-image>4=0 | submission_exp_20260202_230604_best_top4_conf0.020_iou0.40_img1024.csv\n",
      "- iou=0.50 imgsz=768 | rows=3247 | per-image>4=0 | submission_exp_20260202_230604_best_top4_conf0.020_iou0.50_img768.csv\n",
      "- iou=0.50 imgsz=896 | rows=3242 | per-image>4=0 | submission_exp_20260202_230604_best_top4_conf0.020_iou0.50_img896.csv\n",
      "- iou=0.50 imgsz=1024 | rows=3248 | per-image>4=0 | submission_exp_20260202_230604_best_top4_conf0.020_iou0.50_img1024.csv\n",
      "- iou=0.60 imgsz=768 | rows=3247 | per-image>4=0 | submission_exp_20260202_230604_best_top4_conf0.020_iou0.60_img768.csv\n",
      "- iou=0.60 imgsz=896 | rows=3242 | per-image>4=0 | submission_exp_20260202_230604_best_top4_conf0.020_iou0.60_img896.csv\n",
      "- iou=0.60 imgsz=1024 | rows=3248 | per-image>4=0 | submission_exp_20260202_230604_best_top4_conf0.020_iou0.60_img1024.csv\n",
      "\n",
      "[DONE] 위에 생성된 submission_*.csv 중에서 하나씩 제출해보고 점수 비교하면 됩니다.\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 5-3. NMS IoU / imgsz sweep (best.pt) -> 여러 submission 생성\n",
    "\n",
    "from pathlib import Path\n",
    "import json, csv\n",
    "from collections import Counter\n",
    "from ultralytics import YOLO\n",
    "\n",
    "ROOT_ = globals().get(\"ROOT\", Path(\".\").resolve())\n",
    "INPUT_ = globals().get(\"INPUT\", {\n",
    "    \"TEST_IMAGES\": ROOT_ / \"test_images\",\n",
    "})\n",
    "DIRS_ = globals().get(\"DIRS\", {})\n",
    "RUN_NAME_ = globals().get(\"RUN_NAME\", \"run\")\n",
    "\n",
    "TEST_DIR = Path(INPUT_[\"TEST_IMAGES\"])\n",
    "SUB_DIR = Path(DIRS_[\"SUBMISSIONS\"])\n",
    "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BEST_PT = Path(DIRS_[\"CKPT\"]) / \"best.pt\"\n",
    "CACHE_DIR = Path(globals().get(\"CACHE_DIR\", DIRS_.get(\"CACHE\", ROOT_ / \"data\" / \"cache\" / \"merged\")))\n",
    "LABEL_MAP = CACHE_DIR / \"label_map_full.json\"\n",
    "\n",
    "lm = json.loads(LABEL_MAP.read_text(encoding=\"utf-8\"))\n",
    "idx2id = {int(k): int(v) for k, v in lm.get(\"idx2id\", {}).items()}\n",
    "if not idx2id:\n",
    "    idx2id = {int(i): int(cid) for i, cid in enumerate(lm.get(\"category_ids\", []))}\n",
    "\n",
    "def parse_image_id(p: Path) -> int:\n",
    "    return int(p.stem)\n",
    "\n",
    "TOPK = 4\n",
    "CONF = 0.02          # conf는 고정 (이미 검증한 값)\n",
    "MAX_DET = 200        # 충분히 크게 받고 TopK로 자름\n",
    "\n",
    "# sweep 후보 (가성비 조합)\n",
    "IOU_LIST = [0.4, 0.5, 0.6]\n",
    "IMGSZ_LIST = [768, 896, 1024]\n",
    "\n",
    "model = YOLO(str(BEST_PT))\n",
    "\n",
    "header = [\"annotation_id\", \"image_id\", \"category_id\", \"bbox_x\", \"bbox_y\", \"bbox_w\", \"bbox_h\", \"score\"]\n",
    "\n",
    "def run_one(iou, imgsz):\n",
    "    out_csv = SUB_DIR / f\"submission_{RUN_NAME_}_best_top4_conf{CONF:.3f}_iou{iou:.2f}_img{imgsz}.csv\"\n",
    "\n",
    "    ann_id = 1\n",
    "    per_img = Counter()\n",
    "    n_images = 0\n",
    "    n_rows = 0\n",
    "\n",
    "    with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow(header)\n",
    "\n",
    "        stream = model.predict(\n",
    "            source=str(TEST_DIR),\n",
    "            conf=CONF,\n",
    "            iou=float(iou),\n",
    "            imgsz=int(imgsz),\n",
    "            max_det=MAX_DET,\n",
    "            stream=True,\n",
    "            verbose=False,\n",
    "            device=0,\n",
    "        )\n",
    "\n",
    "        for r in stream:\n",
    "            img_path = Path(r.path)\n",
    "            image_id = parse_image_id(img_path)\n",
    "            n_images += 1\n",
    "\n",
    "            dets = []\n",
    "            boxes = r.boxes\n",
    "            if boxes is not None and len(boxes) > 0:\n",
    "                xyxy = boxes.xyxy.cpu().numpy()\n",
    "                confs = boxes.conf.cpu().numpy()\n",
    "                clss = boxes.cls.cpu().numpy()\n",
    "                for (x1, y1, x2, y2), sc, ci in zip(xyxy, confs, clss):\n",
    "                    ci = int(ci)\n",
    "                    cat_id = idx2id.get(ci, None)\n",
    "                    if cat_id is None:\n",
    "                        continue\n",
    "                    bw = max(0.0, float(x2) - float(x1))\n",
    "                    bh = max(0.0, float(y2) - float(y1))\n",
    "                    dets.append((float(sc), int(cat_id), float(x1), float(y1), bw, bh))\n",
    "\n",
    "            dets.sort(reverse=True, key=lambda x: x[0])\n",
    "            dets = dets[:TOPK]\n",
    "\n",
    "            for sc, cat_id, x, y, bw, bh in dets:\n",
    "                w.writerow([ann_id, image_id, cat_id, int(round(x)), int(round(y)), int(round(bw)), int(round(bh)), float(sc)])\n",
    "                ann_id += 1\n",
    "                n_rows += 1\n",
    "                per_img[image_id] += 1\n",
    "\n",
    "    too_many = sum(1 for _, c in per_img.items() if c > TOPK)\n",
    "    print(f\"- iou={iou:.2f} imgsz={imgsz} | rows={n_rows} | per-image>4={too_many} | {out_csv.name}\")\n",
    "    return out_csv\n",
    "\n",
    "print(\"[SWEEP START] (conf fixed at 0.02)\")\n",
    "made = []\n",
    "for iou in IOU_LIST:\n",
    "    for imgsz in IMGSZ_LIST:\n",
    "        made.append(run_one(iou, imgsz))\n",
    "\n",
    "print(\"\\n[DONE] 위에 생성된 submission_*.csv 중에서 하나씩 제출해보고 점수 비교하면 됩니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd70d3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TTA SUBMISSION READY]\n",
      "- images processed : 842\n",
      "- rows            : 3252\n",
      "- per-image >4: 0\n",
      "- saved           : C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\artifacts\\exp_20260202_230604\\submissions\\submission_exp_20260202_230604_best_top4_conf0.020_iou0.50_img1024_tta.csv\n",
      "[OK] recorded -> results.csv | submission_tta_img1024 (kaggle_submit)\n",
      "[OK] jsonl    -> results.jsonl\n",
      "[OK] summary  -> latest_summary.md\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 5-4. best.pt TTA(augment=True) inference -> top4 -> submission 생성\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "ROOT_ = globals().get(\"ROOT\", Path(\".\").resolve())\n",
    "INPUT_ = globals().get(\"INPUT\", {\n",
    "    \"TEST_IMAGES\": ROOT_ / \"test_images\",\n",
    "})\n",
    "DIRS_ = globals().get(\"DIRS\", {})\n",
    "RUN_NAME_ = globals().get(\"RUN_NAME\", \"run\")\n",
    "CFG_ = globals().get(\"CFG\", {})\n",
    "\n",
    "TEST_DIR = Path(INPUT_[\"TEST_IMAGES\"])\n",
    "SUB_DIR = Path(DIRS_[\"SUBMISSIONS\"])\n",
    "SUB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BEST_PT = Path(DIRS_[\"CKPT\"]) / \"best.pt\"\n",
    "if not BEST_PT.exists():\n",
    "    raise FileNotFoundError(f\"best.pt not found: {BEST_PT}\")\n",
    "\n",
    "CACHE_DIR = Path(globals().get(\"CACHE_DIR\", DIRS_.get(\"CACHE\", ROOT_ / \"data\" / \"cache\" / \"merged\")))\n",
    "LABEL_MAP = CACHE_DIR / \"label_map_full.json\"\n",
    "if not LABEL_MAP.exists():\n",
    "    cand = list(CACHE_DIR.rglob(\"label_map_full.json\"))\n",
    "    if cand:\n",
    "        LABEL_MAP = cand[0]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"label_map_full.json not found under {CACHE_DIR}\")\n",
    "\n",
    "lm = json.loads(LABEL_MAP.read_text(encoding=\"utf-8\"))\n",
    "idx2id = {int(k): int(v) for k, v in lm.get(\"idx2id\", {}).items()}\n",
    "if not idx2id:\n",
    "    idx2id = {int(i): int(cid) for i, cid in enumerate(lm.get(\"category_ids\", []))}\n",
    "\n",
    "def parse_image_id(p: Path) -> int:\n",
    "    return int(p.stem)\n",
    "\n",
    "# --- 설정 (baseline 유지 + TTA만 추가) ---\n",
    "CONF = 0.02\n",
    "IOU = 0.50\n",
    "TOPK = 4\n",
    "MAX_DET = 200\n",
    "IMGSZ = 1024  # 768로도 하나 더 만들고 싶으면 여기만 바꾸면 됨\n",
    "\n",
    "out_csv = SUB_DIR / f\"submission_{RUN_NAME_}_best_top{TOPK}_conf{CONF:.3f}_iou{IOU:.2f}_img{IMGSZ}_tta.csv\"\n",
    "header = [\"annotation_id\", \"image_id\", \"category_id\", \"bbox_x\", \"bbox_y\", \"bbox_w\", \"bbox_h\", \"score\"]\n",
    "\n",
    "from ultralytics import YOLO\n",
    "model = YOLO(str(BEST_PT))\n",
    "\n",
    "ann_id = 1\n",
    "n_images = 0\n",
    "n_rows = 0\n",
    "per_img = Counter()\n",
    "\n",
    "with open(out_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(header)\n",
    "\n",
    "    stream = model.predict(\n",
    "        source=str(TEST_DIR),\n",
    "        conf=CONF,\n",
    "        iou=IOU,\n",
    "        imgsz=IMGSZ,\n",
    "        max_det=MAX_DET,\n",
    "        augment=True,     # <-- TTA 핵심\n",
    "        stream=True,\n",
    "        verbose=False,\n",
    "        device=0,\n",
    "    )\n",
    "\n",
    "    for r in stream:\n",
    "        img_path = Path(r.path)\n",
    "        image_id = parse_image_id(img_path)\n",
    "        n_images += 1\n",
    "\n",
    "        dets = []\n",
    "        boxes = r.boxes\n",
    "        if boxes is not None and len(boxes) > 0:\n",
    "            xyxy = boxes.xyxy.cpu().numpy()\n",
    "            confs = boxes.conf.cpu().numpy()\n",
    "            clss = boxes.cls.cpu().numpy()\n",
    "\n",
    "            for (x1, y1, x2, y2), sc, ci in zip(xyxy, confs, clss):\n",
    "                ci = int(ci)\n",
    "                cat_id = idx2id.get(ci, None)\n",
    "                if cat_id is None:\n",
    "                    continue\n",
    "                bw = max(0.0, float(x2) - float(x1))\n",
    "                bh = max(0.0, float(y2) - float(y1))\n",
    "                dets.append((float(sc), int(cat_id), float(x1), float(y1), bw, bh))\n",
    "\n",
    "        dets.sort(reverse=True, key=lambda x: x[0])\n",
    "        dets = dets[:TOPK]\n",
    "\n",
    "        for sc, cat_id, x, y, bw, bh in dets:\n",
    "            w.writerow([ann_id, image_id, cat_id,\n",
    "                        int(round(x)), int(round(y)), int(round(bw)), int(round(bh)), float(sc)])\n",
    "            ann_id += 1\n",
    "            n_rows += 1\n",
    "            per_img[image_id] += 1\n",
    "\n",
    "too_many = sum(1 for _, c in per_img.items() if c > TOPK)\n",
    "\n",
    "print(\"[TTA SUBMISSION READY]\")\n",
    "print(f\"- images processed : {n_images}\")\n",
    "print(f\"- rows            : {n_rows}\")\n",
    "print(f\"- per-image >{TOPK}: {too_many}\")\n",
    "print(f\"- saved           : {out_csv}\")\n",
    "\n",
    "if \"record_result\" in globals():\n",
    "    record_result(\n",
    "        result_name=f\"submission_tta_img{IMGSZ}\",\n",
    "        stage=\"kaggle_submit\",\n",
    "        metrics={\"conf_thr\": CONF, \"nms_iou_thr\": IOU, \"topk\": TOPK, \"imgsz\": IMGSZ, \"tta\": True, \"rows\": n_rows},\n",
    "        notes=\"best.pt inference with augment=True (TTA)\",\n",
    "        submission_path=str(out_csv),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a291bc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] loaded pred cache: pred_test_raw_exp_20260202_230604.jsonl (images=842)\n",
      "[SAVED] visualizations -> C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\vis\\preds_best\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 5-6. exp_20260202_230604(best.pt) 예측 시각화(첨부 스타일) + 파일 저장\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "\n",
    "# --- 기본 경로 (이전 셀들에서 생성된 DIRS/INPUT/RUN_NAME/CACHE_DIR 사용) ---\n",
    "ROOT_ = globals().get(\"ROOT\", Path(\".\").resolve())\n",
    "DIRS_ = globals().get(\"DIRS\", {})\n",
    "INPUT_ = globals().get(\"INPUT\", {\n",
    "    \"TRAIN_IMAGES\": ROOT_ / \"train_images\",\n",
    "    \"TEST_IMAGES\": ROOT_ / \"test_images\",\n",
    "})\n",
    "RUN_NAME_ = globals().get(\"RUN_NAME\", \"run\")\n",
    "CACHE_DIR = Path(globals().get(\"CACHE_DIR\", DIRS_.get(\"CACHE\", ROOT_ / \"data\" / \"cache\" / \"merged\")))\n",
    "\n",
    "RUN_DIR = Path(DIRS_[\"RUN_DIR\"])\n",
    "CKPT_BEST = Path(DIRS_[\"CKPT\"]) / \"best.pt\"\n",
    "SUB_DIR = Path(DIRS_[\"SUBMISSIONS\"])\n",
    "\n",
    "TRAIN_DIR = Path(INPUT_[\"TRAIN_IMAGES\"])\n",
    "TEST_DIR = Path(INPUT_[\"TEST_IMAGES\"])\n",
    "\n",
    "assert CKPT_BEST.exists(), f\"best.pt not found: {CKPT_BEST}\"\n",
    "\n",
    "# --- 시각화 저장 폴더 ---\n",
    "VIS_DIR = RUN_DIR / \"vis\" / \"preds_best\"\n",
    "VIS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- 라벨 매핑 로드: class_index -> category_id, category_id -> name ---\n",
    "label_map_path = CACHE_DIR / \"label_map_full.json\"\n",
    "if not label_map_path.exists():\n",
    "    cands = list(CACHE_DIR.rglob(\"label_map_full.json\"))\n",
    "    if cands:\n",
    "        label_map_path = cands[0]\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"label_map_full.json not found under {CACHE_DIR}\")\n",
    "\n",
    "lm = json.loads(label_map_path.read_text(encoding=\"utf-8\"))\n",
    "idx2id = {int(k): int(v) for k, v in lm.get(\"idx2id\", {}).items()}\n",
    "if not idx2id:\n",
    "    # fallback\n",
    "    idx2id = {int(i): int(cid) for i, cid in enumerate(lm.get(\"category_ids\", []))}\n",
    "\n",
    "cat_name_path = CACHE_DIR / \"category_id_to_name.json\"\n",
    "if not cat_name_path.exists():\n",
    "    cands = list(CACHE_DIR.rglob(\"category_id_to_name.json\"))\n",
    "    if cands:\n",
    "        cat_name_path = cands[0]\n",
    "    else:\n",
    "        # 마지막 fallback: reports쪽 csv라도 있으면 거기서 만들 수 있는데, 일단 없으면 id만 표시\n",
    "        cat_name_path = None\n",
    "\n",
    "catid2name = {}\n",
    "if cat_name_path:\n",
    "    obj = json.loads(cat_name_path.read_text(encoding=\"utf-8\"))\n",
    "    # 저장 포맷이 dict(cat_id->name)라고 가정\n",
    "    catid2name = {int(k): str(v) for k, v in obj.items()}\n",
    "\n",
    "def cat_name(cat_id: int) -> str:\n",
    "    return catid2name.get(int(cat_id), f\"class_{cat_id}\")\n",
    "\n",
    "# --- (선택) 캐시(jsonl)에서 dets 읽기: 있으면 빠르게 시각화 가능 ---\n",
    "def find_pred_cache_jsonl():\n",
    "    # 우선순위: raw(top4) -> cache(full)\n",
    "    patterns = [\n",
    "        f\"pred_test_raw_{RUN_NAME_}.jsonl\",\n",
    "        f\"pred_test_cache_full_{RUN_NAME_}.jsonl\",\n",
    "        f\"pred_test_raw_*{RUN_NAME_}*.jsonl\",\n",
    "        f\"pred_test_cache_full_*{RUN_NAME_}*.jsonl\",\n",
    "    ]\n",
    "    for pat in patterns:\n",
    "        cands = sorted(SUB_DIR.glob(pat))\n",
    "        if cands:\n",
    "            return cands[-1]\n",
    "    # 폴더에 없으면 runs/artifacts 전체에서 마지막 하나라도\n",
    "    cands = sorted(RUN_DIR.rglob(f\"pred_test_*{RUN_NAME_}*.jsonl\"))\n",
    "    return cands[-1] if cands else None\n",
    "\n",
    "PRED_CACHE_JSONL = find_pred_cache_jsonl()\n",
    "\n",
    "cache_map = None\n",
    "if PRED_CACHE_JSONL and PRED_CACHE_JSONL.exists():\n",
    "    cache_map = {}\n",
    "    with open(PRED_CACHE_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            o = json.loads(line)\n",
    "            cache_map[int(o[\"image_id\"])] = o.get(\"dets\", [])\n",
    "    print(f\"[OK] loaded pred cache: {PRED_CACHE_JSONL.name} (images={len(cache_map)})\")\n",
    "else:\n",
    "    print(\"[INFO] pred cache jsonl not found -> will run inference for selected images.\")\n",
    "\n",
    "# --- 시각화 대상 이미지 id (원하는 대로 수정) ---\n",
    "IMAGE_IDS = [1143, 1265, 627]  # <- 여기 원하는 image_id로 바꿔도 됨\n",
    "\n",
    "# --- 추론 설정(첨부 이미지처럼 top4) ---\n",
    "CONF = 0.02\n",
    "IOU = 0.50\n",
    "IMGSZ = 768\n",
    "TOPK = 4\n",
    "MAX_DET = 200\n",
    "\n",
    "# --- 이미지 경로 찾기(우선 test_images, 없으면 train_images) ---\n",
    "def find_image_path(image_id: int) -> Path:\n",
    "    p_test = TEST_DIR / f\"{image_id}.png\"\n",
    "    if p_test.exists():\n",
    "        return p_test\n",
    "    p_train = TRAIN_DIR / f\"{image_id}.png\"\n",
    "    if p_train.exists():\n",
    "        return p_train\n",
    "    raise FileNotFoundError(f\"{image_id}.png not found in test/train dirs.\")\n",
    "\n",
    "# --- dets 확보: cache 있으면 cache 사용, 없으면 모델로 해당 이미지들만 추론 ---\n",
    "def get_dets_for_ids(image_ids):\n",
    "    dets_by_id = {}\n",
    "    missing = []\n",
    "    for iid in image_ids:\n",
    "        if cache_map is not None and iid in cache_map:\n",
    "            # cache det format: {category_id,bbox_x,bbox_y,bbox_w,bbox_h,score}\n",
    "            dets = sorted(cache_map[iid], key=lambda d: float(d[\"score\"]), reverse=True)[:TOPK]\n",
    "            dets_by_id[iid] = dets\n",
    "        else:\n",
    "            missing.append(iid)\n",
    "\n",
    "    if missing:\n",
    "        from ultralytics import YOLO\n",
    "        model = YOLO(str(CKPT_BEST))\n",
    "        paths = [str(find_image_path(i)) for i in missing]\n",
    "\n",
    "        # stream=False로 한 번에 받아서 iid 매칭\n",
    "        results = model.predict(\n",
    "            source=paths,\n",
    "            conf=CONF,\n",
    "            iou=IOU,\n",
    "            imgsz=IMGSZ,\n",
    "            max_det=MAX_DET,\n",
    "            verbose=False,\n",
    "            device=0,\n",
    "        )\n",
    "\n",
    "        for r in results:\n",
    "            img_path = Path(r.path)\n",
    "            iid = int(img_path.stem)\n",
    "            dets = []\n",
    "            boxes = r.boxes\n",
    "            if boxes is not None and len(boxes) > 0:\n",
    "                xyxy = boxes.xyxy.cpu().numpy()\n",
    "                confs = boxes.conf.cpu().numpy()\n",
    "                clss = boxes.cls.cpu().numpy()\n",
    "                for (x1, y1, x2, y2), sc, ci in zip(xyxy, confs, clss):\n",
    "                    ci = int(ci)\n",
    "                    cat_id = idx2id.get(ci, None)\n",
    "                    if cat_id is None:\n",
    "                        continue\n",
    "                    bw = max(0.0, float(x2) - float(x1))\n",
    "                    bh = max(0.0, float(y2) - float(y1))\n",
    "                    dets.append({\n",
    "                        \"category_id\": int(cat_id),\n",
    "                        \"bbox_x\": float(x1),\n",
    "                        \"bbox_y\": float(y1),\n",
    "                        \"bbox_w\": float(bw),\n",
    "                        \"bbox_h\": float(bh),\n",
    "                        \"score\": float(sc),\n",
    "                    })\n",
    "            dets = sorted(dets, key=lambda d: float(d[\"score\"]), reverse=True)[:TOPK]\n",
    "            dets_by_id[iid] = dets\n",
    "\n",
    "    return dets_by_id\n",
    "\n",
    "dets_by_id = get_dets_for_ids(IMAGE_IDS)\n",
    "\n",
    "# --- 그리기 ---\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 폰트(윈도우면 맑은고딕 시도, 없으면 기본)\n",
    "def load_font(size=18):\n",
    "    candidates = [\n",
    "        \"C:/Windows/Fonts/malgun.ttf\",\n",
    "        \"C:/Windows/Fonts/malgunbd.ttf\",\n",
    "        \"/usr/share/fonts/truetype/nanum/NanumGothic.ttf\",\n",
    "    ]\n",
    "    for fp in candidates:\n",
    "        try:\n",
    "            if Path(fp).exists():\n",
    "                return ImageFont.truetype(fp, size=size)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return ImageFont.load_default()\n",
    "\n",
    "font = load_font(18)\n",
    "\n",
    "def draw_one(image_path: Path, dets, out_path: Path):\n",
    "    im = Image.open(image_path).convert(\"RGB\")\n",
    "    W, H = im.size\n",
    "    draw = ImageDraw.Draw(im)\n",
    "\n",
    "    # title 느낌(첨부 이미지처럼 위에 정보)\n",
    "    title = f\"image_id={image_path.stem} | {image_path.name} | {W}x{H}\"\n",
    "\n",
    "    # 박스/라벨 스타일\n",
    "    box_color = (255, 0, 0)\n",
    "    text_color = (255, 255, 255)\n",
    "    fill_color = (255, 0, 0)\n",
    "\n",
    "    # 상단 타이틀 영역(깔끔하게)\n",
    "    pad = 6\n",
    "    tw, th = draw.textbbox((0, 0), title, font=font)[2:]\n",
    "    draw.rectangle([0, 0, tw + pad*2, th + pad*2], fill=(255, 255, 255))\n",
    "    draw.text((pad, pad), title, fill=(0, 0, 0), font=font)\n",
    "\n",
    "    for d in dets:\n",
    "        x = float(d[\"bbox_x\"])\n",
    "        y = float(d[\"bbox_y\"])\n",
    "        w = float(d[\"bbox_w\"])\n",
    "        h = float(d[\"bbox_h\"])\n",
    "        sc = float(d[\"score\"])\n",
    "        cid = int(d[\"category_id\"])\n",
    "        name = cat_name(cid)\n",
    "\n",
    "        x1, y1 = int(round(x)), int(round(y))\n",
    "        x2, y2 = int(round(x + w)), int(round(y + h))\n",
    "\n",
    "        # box\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=box_color, width=3)\n",
    "\n",
    "        label = f\"{name} ({cid}) {sc:.3f}\"\n",
    "        lb = draw.textbbox((0, 0), label, font=font)\n",
    "        lw, lh = lb[2]-lb[0], lb[3]-lb[1]\n",
    "\n",
    "        # label 위치: 박스 위쪽(넘치면 안쪽)\n",
    "        tx = x1\n",
    "        ty = y1 - (lh + 6)\n",
    "        if ty < 0:\n",
    "            ty = y1 + 2\n",
    "\n",
    "        # label background\n",
    "        draw.rectangle([tx, ty, tx + lw + 10, ty + lh + 6], fill=fill_color)\n",
    "        draw.text((tx + 5, ty + 3), label, fill=text_color, font=font)\n",
    "\n",
    "    im.save(out_path)\n",
    "    return im\n",
    "\n",
    "# 표시(그리드)\n",
    "cols = 2\n",
    "rows = math.ceil(len(IMAGE_IDS) / cols)\n",
    "plt.figure(figsize=(cols * 7, rows * 7))\n",
    "\n",
    "for i, iid in enumerate(IMAGE_IDS, 1):\n",
    "    img_path = find_image_path(iid)\n",
    "    dets = dets_by_id.get(iid, [])\n",
    "    out_path = VIS_DIR / f\"vis_{iid}.png\"\n",
    "    vis_im = draw_one(img_path, dets, out_path)\n",
    "\n",
    "    ax = plt.subplot(rows, cols, i)\n",
    "    ax.imshow(vis_im)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"saved: {out_path.name}\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "print(f\"[SAVED] visualizations -> {VIS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d6a5f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIG]\n",
      "- IOU_MATCH=0.5 | CONF=0.02 | NMS_IOU=0.5 | IMGSZ=768 | MAX_DET=200\n",
      "- TOP_N_FP=12 | TOP_N_FN=12\n",
      "- VIS_DIR=C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\vis\\failcases_val\n",
      "[OK] pred_by_img filled: 47 val images\n",
      "\n",
      "[SAMPLING]\n",
      "- FP candidates picked: 12\n",
      "- FN candidates picked: 3\n",
      "\n",
      "[VISUALIZE] FP Top-N\n",
      "[SAVED] fp visuals -> C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\vis\\failcases_val\n",
      "\n",
      "[VISUALIZE] FN Top-N\n",
      "[SAVED] fn visuals -> C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\vis\\failcases_val\n",
      "\n",
      "[REPORT]\n",
      "- saved -> C:\\Users\\amy\\Desktop\\sprint\\초급 프로젝트\\pjt-sprint_ai07_healthcare\\experiments\\DM2\\runs\\exp_20260202_230604\\vis\\failcases_val\\failcases_val_summary.json\n",
      "- fp_top image_ids: [4, 6, 5, 127, 144, 164, 157, 78, 181, 90, 15, 18]\n",
      "- fn_top image_ids: [4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# [Code Cell] 5-7. Val 실패 케이스(오검출/미검출) Top-N 자동 샘플링 + 시각화 저장\n",
    "\n",
    "from pathlib import Path\n",
    "import json, math\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Paths / load basics\n",
    "# -----------------------------\n",
    "ROOT_ = globals().get(\"ROOT\", Path(\".\").resolve())\n",
    "DIRS_ = globals().get(\"DIRS\", {})\n",
    "INPUT_ = globals().get(\"INPUT\", {\n",
    "    \"TRAIN_IMAGES\": ROOT_ / \"train_images\",\n",
    "    \"TEST_IMAGES\": ROOT_ / \"test_images\",\n",
    "})\n",
    "CACHE_DIR = Path(globals().get(\"CACHE_DIR\", DIRS_.get(\"CACHE\", ROOT_ / \"data\" / \"cache\" / \"merged\")))\n",
    "\n",
    "RUN_DIR = Path(DIRS_[\"RUN_DIR\"])\n",
    "CKPT_BEST = Path(DIRS_[\"CKPT\"]) / \"best.pt\"\n",
    "TRAIN_DIR = Path(INPUT_[\"TRAIN_IMAGES\"])\n",
    "\n",
    "assert CKPT_BEST.exists(), f\"best.pt not found: {CKPT_BEST}\"\n",
    "assert TRAIN_DIR.exists(), f\"train_images not found: {TRAIN_DIR}\"\n",
    "\n",
    "# merged coco / split / label map\n",
    "MERGED_COCO = CACHE_DIR / \"train_merged_coco.json\"\n",
    "SPLIT_JSON  = CACHE_DIR / \"splits\" / \"split_train_valid.json\"\n",
    "LABEL_MAP   = CACHE_DIR / \"label_map_full.json\"\n",
    "CATID2NAME  = CACHE_DIR / \"category_id_to_name.json\"\n",
    "\n",
    "if not MERGED_COCO.exists():\n",
    "    cands = list(CACHE_DIR.rglob(\"train_merged_coco.json\"))\n",
    "    if cands: MERGED_COCO = cands[0]\n",
    "    else: raise FileNotFoundError(f\"train_merged_coco.json not found under {CACHE_DIR}\")\n",
    "\n",
    "if not SPLIT_JSON.exists():\n",
    "    cands = list(CACHE_DIR.rglob(\"split_train_valid.json\"))\n",
    "    if cands: SPLIT_JSON = cands[0]\n",
    "    else: raise FileNotFoundError(f\"split_train_valid.json not found under {CACHE_DIR}\")\n",
    "\n",
    "if not LABEL_MAP.exists():\n",
    "    cands = list(CACHE_DIR.rglob(\"label_map_full.json\"))\n",
    "    if cands: LABEL_MAP = cands[0]\n",
    "    else: raise FileNotFoundError(f\"label_map_full.json not found under {CACHE_DIR}\")\n",
    "\n",
    "catid2name = {}\n",
    "if CATID2NAME.exists():\n",
    "    catid2name = {int(k): str(v) for k, v in json.loads(CATID2NAME.read_text(encoding=\"utf-8\")).items()}\n",
    "def cname(cid: int) -> str:\n",
    "    return catid2name.get(int(cid), f\"class_{cid}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Hyperparams (tweakable)\n",
    "# -----------------------------\n",
    "# 실패 판정용 매칭 IoU (보통 0.5로 시작)\n",
    "IOU_MATCH = 0.50\n",
    "\n",
    "# 모델 추론 파라미터 (baseline과 동일 추천)\n",
    "CONF = 0.02\n",
    "NMS_IOU = 0.50\n",
    "IMGSZ = 768\n",
    "MAX_DET = 200\n",
    "\n",
    "# Top-N 샘플링 개수\n",
    "TOP_N_FP = 12\n",
    "TOP_N_FN = 12\n",
    "\n",
    "# 시각화 저장 폴더\n",
    "VIS_DIR = RUN_DIR / \"vis\" / \"failcases_val\"\n",
    "VIS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"[CONFIG]\")\n",
    "print(f\"- IOU_MATCH={IOU_MATCH} | CONF={CONF} | NMS_IOU={NMS_IOU} | IMGSZ={IMGSZ} | MAX_DET={MAX_DET}\")\n",
    "print(f\"- TOP_N_FP={TOP_N_FP} | TOP_N_FN={TOP_N_FN}\")\n",
    "print(f\"- VIS_DIR={VIS_DIR}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Load COCO + split (val ids)\n",
    "# -----------------------------\n",
    "coco = json.loads(MERGED_COCO.read_text(encoding=\"utf-8\"))\n",
    "split = json.loads(SPLIT_JSON.read_text(encoding=\"utf-8\"))\n",
    "val_ids = [int(x) for x in split.get(\"valid_image_ids\", [])]\n",
    "val_set = set(val_ids)\n",
    "\n",
    "images = coco.get(\"images\", [])\n",
    "anns = coco.get(\"annotations\", [])\n",
    "\n",
    "img_by_id = {int(im[\"id\"]): im for im in images if isinstance(im, dict) and im.get(\"id\") is not None}\n",
    "\n",
    "gt_by_img = defaultdict(list)\n",
    "for a in anns:\n",
    "    if not isinstance(a, dict): \n",
    "        continue\n",
    "    iid = a.get(\"image_id\", None)\n",
    "    bbox = a.get(\"bbox\", None)\n",
    "    cid = a.get(\"category_id\", None)\n",
    "    if iid is None or bbox is None or cid is None:\n",
    "        continue\n",
    "    if not isinstance(bbox, list) or len(bbox) != 4:\n",
    "        continue\n",
    "    iid = int(iid)\n",
    "    if iid not in val_set:\n",
    "        continue\n",
    "    x, y, w, h = map(float, bbox)\n",
    "    if w <= 0 or h <= 0:\n",
    "        continue\n",
    "    gt_by_img[iid].append({\n",
    "        \"category_id\": int(cid),\n",
    "        \"bbox_x\": x,\n",
    "        \"bbox_y\": y,\n",
    "        \"bbox_w\": w,\n",
    "        \"bbox_h\": h\n",
    "    })\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Label map (yolo cls idx -> category_id)\n",
    "# -----------------------------\n",
    "lm = json.loads(LABEL_MAP.read_text(encoding=\"utf-8\"))\n",
    "idx2id = {int(k): int(v) for k, v in lm.get(\"idx2id\", {}).items()}\n",
    "if not idx2id:\n",
    "    idx2id = {int(i): int(cid) for i, cid in enumerate(lm.get(\"category_ids\", []))}\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Inference on val images\n",
    "# -----------------------------\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# coco images에서 file_name -> image_id 매핑 생성 (val에 해당하는 것만)\n",
    "fname2id = {}\n",
    "for _iid in val_ids:\n",
    "    im = img_by_id.get(int(_iid), None)\n",
    "    if im is None:\n",
    "        continue\n",
    "    fn = Path(str(im[\"file_name\"])).name\n",
    "    fname2id[fn] = int(_iid)\n",
    "\n",
    "# Inference 결과를 pred_by_img에 적재 (iid 추출 방식을 file_name 기반으로 변경)\n",
    "pred_by_img = defaultdict(list)\n",
    "\n",
    "for r in results:\n",
    "    f = Path(r.path).name  # 예: K-003351-..._200.png\n",
    "    iid = fname2id.get(f, None)\n",
    "\n",
    "    # 혹시라도 매핑이 안 되면(예외 케이스) 숫자 stem 시도\n",
    "    if iid is None:\n",
    "        try:\n",
    "            iid = int(Path(r.path).stem)\n",
    "        except Exception:\n",
    "            # 그래도 안 되면 skip\n",
    "            continue\n",
    "\n",
    "    boxes = r.boxes\n",
    "    if boxes is None or len(boxes) == 0:\n",
    "        continue\n",
    "\n",
    "    xyxy = boxes.xyxy.cpu().numpy()\n",
    "    confs = boxes.conf.cpu().numpy()\n",
    "    clss  = boxes.cls.cpu().numpy()\n",
    "\n",
    "    for (x1, y1, x2, y2), sc, ci in zip(xyxy, confs, clss):\n",
    "        ci = int(ci)\n",
    "        cat_id = idx2id.get(ci, None)\n",
    "        if cat_id is None:\n",
    "            continue\n",
    "\n",
    "        x1 = float(x1); y1 = float(y1); x2 = float(x2); y2 = float(y2)\n",
    "        w = max(0.0, x2 - x1)\n",
    "        h = max(0.0, y2 - y1)\n",
    "\n",
    "        pred_by_img[iid].append({\n",
    "            \"category_id\": int(cat_id),\n",
    "            \"bbox_x\": x1,\n",
    "            \"bbox_y\": y1,\n",
    "            \"bbox_w\": w,\n",
    "            \"bbox_h\": h,\n",
    "            \"score\": float(sc),\n",
    "        })\n",
    "\n",
    "print(f\"[OK] pred_by_img filled: {len(pred_by_img)} val images\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Matching (class-aware greedy)\n",
    "# -----------------------------\n",
    "def to_xyxy(b):\n",
    "    x1 = float(b[\"bbox_x\"])\n",
    "    y1 = float(b[\"bbox_y\"])\n",
    "    x2 = x1 + float(b[\"bbox_w\"])\n",
    "    y2 = y1 + float(b[\"bbox_h\"])\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def iou_xyxy(a, b):\n",
    "    ax1, ay1, ax2, ay2 = a\n",
    "    bx1, by1, bx2, by2 = b\n",
    "    ix1 = max(ax1, bx1)\n",
    "    iy1 = max(ay1, by1)\n",
    "    ix2 = min(ax2, bx2)\n",
    "    iy2 = min(ay2, by2)\n",
    "    iw = max(0.0, ix2 - ix1)\n",
    "    ih = max(0.0, iy2 - iy1)\n",
    "    inter = iw * ih\n",
    "    area_a = max(0.0, ax2 - ax1) * max(0.0, ay2 - ay1)\n",
    "    area_b = max(0.0, bx2 - bx1) * max(0.0, by2 - by1)\n",
    "    union = area_a + area_b - inter + 1e-9\n",
    "    return inter / union\n",
    "\n",
    "fail_stats = {}  # image_id -> dict\n",
    "fp_examples = []\n",
    "fn_examples = []\n",
    "\n",
    "for iid in val_ids:\n",
    "    gts = gt_by_img.get(iid, [])\n",
    "    preds = pred_by_img.get(iid, [])\n",
    "\n",
    "    # sort preds by score desc\n",
    "    preds = sorted(preds, key=lambda d: d[\"score\"], reverse=True)\n",
    "\n",
    "    gt_used = [False] * len(gts)\n",
    "    pred_used = [False] * len(preds)\n",
    "\n",
    "    tp_pairs = []  # (pred_idx, gt_idx, iou)\n",
    "\n",
    "    for pi, p in enumerate(preds):\n",
    "        best_j = -1\n",
    "        best_iou = -1.0\n",
    "        p_xyxy = to_xyxy(p)\n",
    "        for gi, g in enumerate(gts):\n",
    "            if gt_used[gi]:\n",
    "                continue\n",
    "            if int(g[\"category_id\"]) != int(p[\"category_id\"]):\n",
    "                continue\n",
    "            iou = iou_xyxy(p_xyxy, to_xyxy(g))\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_j = gi\n",
    "        if best_j >= 0 and best_iou >= IOU_MATCH:\n",
    "            gt_used[best_j] = True\n",
    "            pred_used[pi] = True\n",
    "            tp_pairs.append((pi, best_j, best_iou))\n",
    "\n",
    "    fps = [preds[i] for i, u in enumerate(pred_used) if not u]\n",
    "    fns = [gts[i] for i, u in enumerate(gt_used) if not u]\n",
    "\n",
    "    # fp/fn scoring for ranking\n",
    "    max_fp_score = max([d[\"score\"] for d in fps], default=0.0)\n",
    "    fn_area_sum = sum([float(d[\"bbox_w\"]) * float(d[\"bbox_h\"]) for d in fns]) if fns else 0.0\n",
    "\n",
    "    stat = {\n",
    "        \"image_id\": iid,\n",
    "        \"n_gt\": len(gts),\n",
    "        \"n_pred\": len(preds),\n",
    "        \"n_tp\": len(tp_pairs),\n",
    "        \"n_fp\": len(fps),\n",
    "        \"n_fn\": len(fns),\n",
    "        \"max_fp_score\": float(max_fp_score),\n",
    "        \"fn_area_sum\": float(fn_area_sum),\n",
    "        \"tp_pairs\": tp_pairs,\n",
    "        \"fps\": fps,\n",
    "        \"fns\": fns,\n",
    "        \"gts\": gts,\n",
    "        \"preds\": preds,\n",
    "    }\n",
    "    fail_stats[iid] = stat\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Top-N sampling\n",
    "# -----------------------------\n",
    "# FP Top: fp 개수 우선, 그 다음 max fp score\n",
    "fp_rank = sorted(\n",
    "    fail_stats.values(),\n",
    "    key=lambda s: (s[\"n_fp\"], s[\"max_fp_score\"]),\n",
    "    reverse=True\n",
    ")\n",
    "fp_pick = [s for s in fp_rank if s[\"n_fp\"] > 0][:TOP_N_FP]\n",
    "\n",
    "# FN Top: fn 개수 우선, 그 다음 fn_area_sum\n",
    "fn_rank = sorted(\n",
    "    fail_stats.values(),\n",
    "    key=lambda s: (s[\"n_fn\"], s[\"fn_area_sum\"]),\n",
    "    reverse=True\n",
    ")\n",
    "fn_pick = [s for s in fn_rank if s[\"n_fn\"] > 0][:TOP_N_FN]\n",
    "\n",
    "print(\"\\n[SAMPLING]\")\n",
    "print(f\"- FP candidates picked: {len(fp_pick)}\")\n",
    "print(f\"- FN candidates picked: {len(fn_pick)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Visualization (GT=green, Pred=red)\n",
    "# -----------------------------\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_font(size=18):\n",
    "    candidates = [\n",
    "        \"C:/Windows/Fonts/malgun.ttf\",\n",
    "        \"C:/Windows/Fonts/malgunbd.ttf\",\n",
    "        \"/usr/share/fonts/truetype/nanum/NanumGothic.ttf\",\n",
    "    ]\n",
    "    for fp in candidates:\n",
    "        try:\n",
    "            if Path(fp).exists():\n",
    "                return ImageFont.truetype(fp, size=size)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return ImageFont.load_default()\n",
    "\n",
    "font = load_font(18)\n",
    "\n",
    "def draw_label(draw, x, y, text, color, font):\n",
    "    pad = 4\n",
    "    bb = draw.textbbox((0, 0), text, font=font)\n",
    "    tw, th = bb[2]-bb[0], bb[3]-bb[1]\n",
    "    # 화면 밖 방지\n",
    "    y = max(0, y)\n",
    "    draw.rectangle([x, y, x + tw + pad*2, y + th + pad*2], fill=color)\n",
    "    draw.text((x + pad, y + pad), text, fill=(255, 255, 255), font=font)\n",
    "\n",
    "def render_one(iid: int, stat: dict, out_path: Path, mode: str):\n",
    "    \"\"\"\n",
    "    mode: \"fp\" or \"fn\" (표시 강조용. 둘 다 GT/Pred 모두 그림)\n",
    "    \"\"\"\n",
    "    img_path = find_img_path(iid)\n",
    "    im = Image.open(img_path).convert(\"RGB\")\n",
    "    W, H = im.size\n",
    "    draw = ImageDraw.Draw(im)\n",
    "\n",
    "    # title\n",
    "    title = f\"image_id={iid} | {img_path.name} | {W}x{H} | TP={stat['n_tp']} FP={stat['n_fp']} FN={stat['n_fn']}\"\n",
    "    draw.rectangle([0, 0, 10 + len(title)*10, 36], fill=(255, 255, 255))\n",
    "    draw.text((6, 6), title, fill=(0, 0, 0), font=font)\n",
    "\n",
    "    # GT boxes (green)\n",
    "    gt_color = (0, 180, 0)\n",
    "    for g in stat[\"gts\"]:\n",
    "        x1 = int(round(g[\"bbox_x\"]))\n",
    "        y1 = int(round(g[\"bbox_y\"]))\n",
    "        x2 = int(round(g[\"bbox_x\"] + g[\"bbox_w\"]))\n",
    "        y2 = int(round(g[\"bbox_y\"] + g[\"bbox_h\"]))\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=gt_color, width=3)\n",
    "        label = f\"GT {cname(g['category_id'])} ({int(g['category_id'])})\"\n",
    "        draw_label(draw, x1, y1-24, label, gt_color, font)\n",
    "\n",
    "    # Pred boxes (red)\n",
    "    pred_color = (255, 0, 0)\n",
    "    for p in stat[\"preds\"]:\n",
    "        x1 = int(round(p[\"bbox_x\"]))\n",
    "        y1 = int(round(p[\"bbox_y\"]))\n",
    "        x2 = int(round(p[\"bbox_x\"] + p[\"bbox_w\"]))\n",
    "        y2 = int(round(p[\"bbox_y\"] + p[\"bbox_h\"]))\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=pred_color, width=3)\n",
    "        label = f\"{cname(p['category_id'])} ({int(p['category_id'])}) {p['score']:.3f}\"\n",
    "        draw_label(draw, x1, y1-24, label, pred_color, font)\n",
    "\n",
    "    # 실패 강조(추가 표시): FP는 빨간 점수 높은 것, FN은 GT박스 두껍게\n",
    "    if mode == \"fp\" and stat[\"fps\"]:\n",
    "        # FP 박스만 테두리 두껍게(겹쳐보이면 강조)\n",
    "        for p in stat[\"fps\"]:\n",
    "            x1 = int(round(p[\"bbox_x\"]))\n",
    "            y1 = int(round(p[\"bbox_y\"]))\n",
    "            x2 = int(round(p[\"bbox_x\"] + p[\"bbox_w\"]))\n",
    "            y2 = int(round(p[\"bbox_y\"] + p[\"bbox_h\"]))\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=(255, 50, 50), width=5)\n",
    "    if mode == \"fn\" and stat[\"fns\"]:\n",
    "        for g in stat[\"fns\"]:\n",
    "            x1 = int(round(g[\"bbox_x\"]))\n",
    "            y1 = int(round(g[\"bbox_y\"]))\n",
    "            x2 = int(round(g[\"bbox_x\"] + g[\"bbox_w\"]))\n",
    "            y2 = int(round(g[\"bbox_y\"] + g[\"bbox_h\"]))\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=(0, 255, 80), width=6)\n",
    "\n",
    "    im.save(out_path)\n",
    "    return im\n",
    "\n",
    "# save + show grids\n",
    "def show_grid(picks, tag):\n",
    "    if not picks:\n",
    "        print(f\"[{tag}] no samples\")\n",
    "        return\n",
    "    cols = 2\n",
    "    rows = math.ceil(len(picks) / cols)\n",
    "    plt.figure(figsize=(cols * 7, rows * 7))\n",
    "    for i, s in enumerate(picks, 1):\n",
    "        iid = int(s[\"image_id\"])\n",
    "        out_path = VIS_DIR / f\"{tag}_{iid}.png\"\n",
    "        im = render_one(iid, s, out_path, mode=tag)\n",
    "\n",
    "        ax = plt.subplot(rows, cols, i)\n",
    "        ax.imshow(im)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(out_path.name, fontsize=10)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    print(f\"[SAVED] {tag} visuals -> {VIS_DIR}\")\n",
    "\n",
    "print(\"\\n[VISUALIZE] FP Top-N\")\n",
    "show_grid(fp_pick, \"fp\")\n",
    "\n",
    "print(\"\\n[VISUALIZE] FN Top-N\")\n",
    "show_grid(fn_pick, \"fn\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Save summary report\n",
    "# -----------------------------\n",
    "report = {\n",
    "    \"config\": {\n",
    "        \"IOU_MATCH\": IOU_MATCH,\n",
    "        \"CONF\": CONF,\n",
    "        \"NMS_IOU\": NMS_IOU,\n",
    "        \"IMGSZ\": IMGSZ,\n",
    "        \"MAX_DET\": MAX_DET,\n",
    "        \"TOP_N_FP\": TOP_N_FP,\n",
    "        \"TOP_N_FN\": TOP_N_FN,\n",
    "    },\n",
    "    \"fp_top\": [{\"image_id\": s[\"image_id\"], \"n_fp\": s[\"n_fp\"], \"max_fp_score\": s[\"max_fp_score\"]} for s in fp_pick],\n",
    "    \"fn_top\": [{\"image_id\": s[\"image_id\"], \"n_fn\": s[\"n_fn\"], \"fn_area_sum\": s[\"fn_area_sum\"]} for s in fn_pick],\n",
    "}\n",
    "\n",
    "out_json = VIS_DIR / \"failcases_val_summary.json\"\n",
    "out_json.write_text(json.dumps(report, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\n[REPORT]\")\n",
    "print(f\"- saved -> {out_json}\")\n",
    "print(\"- fp_top image_ids:\", [x[\"image_id\"] for x in report[\"fp_top\"]])\n",
    "print(\"- fn_top image_ids:\", [x[\"image_id\"] for x in report[\"fn_top\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6774b5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
